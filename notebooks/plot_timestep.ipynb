{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "#IMPORTS\n",
    "%reset\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import copy \n",
    "from sklearn import decomposition\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "from helpers import helper_functions\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name+'.pkl', 'rb') as f:\n",
    "        return pickle.load(f, encoding='latin1')\n",
    "\n",
    "# Gaussian normalization, return 0 if std is 0\n",
    "def normalize(obj, mean, std):\n",
    "    a = obj-mean\n",
    "    b=std\n",
    "    return np.divide(a, b, out=np.zeros_like(a), where=b!=0)\n",
    "\n",
    "# load conf file, 'config'\n",
    "def load_config(config_file):\n",
    "    with open(config_file) as f:\n",
    "        config = yaml.load(f)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir=\"/global/cscratch1/sd/al34/autoruns/data_50ms_trying_different_predictions/predicting_dens_smaller_archi\"\n",
    "\n",
    "input_dir = \"/global/cscratch1/sd/al34/data_50ms/pred_dens_half_data_1_inputs_roryjoe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in model and conf file\n",
    "model_filename='model.h5'\n",
    "\n",
    "\n",
    "config=load_config(os.path.join(root_dir,'conf.yaml'))['data_and_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'delay': 2,\n",
       " 'lookback': 4,\n",
       " 'sigs_0d': ['curr', 'pinj', 'ech'],\n",
       " 'sigs_1d': ['dens', 'temp'],\n",
       " 'sigs_predict': ['dens']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['curr', 'pinj', 'ech', 'dens', 'temp']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###############\n",
    "#LOAD DATA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # load raw data, 'raw_data'\n",
    "# with open(os.path.join(input_dir,'final_data.pkl'), 'rb') as f: \n",
    "#         raw_data=pickle.load(f, encoding='latin1')\n",
    "        \n",
    "\n",
    "# load processed train data, 'train_data'\n",
    "with open(os.path.join(input_dir,'train_data.pkl'), 'rb') as f: \n",
    "        train_data=pickle.load(f, encoding='latin1')\n",
    "        \n",
    "        \n",
    "# load processed val data, 'val_data'\n",
    "with open(os.path.join(input_dir,'val_data.pkl'), 'rb') as f: \n",
    "        val_data=pickle.load(f, encoding='latin1')\n",
    "\n",
    "# load processed train target, 'train_target'\n",
    "with open(os.path.join(input_dir,'train_target.pkl'), 'rb') as f: \n",
    "        train_target=pickle.load(f, encoding='latin1')\n",
    "\n",
    "# load processed val target, 'val_target'\n",
    "with open(os.path.join(input_dir,'val_target.pkl'), 'rb') as f: \n",
    "        val_target=pickle.load(f, encoding='latin1')\n",
    "        \n",
    "\n",
    "# load trainshots\n",
    "with open(os.path.join(input_dir,'train_shot.pkl'), 'rb') as f: \n",
    "        train_shot=pickle.load(f, encoding='latin1')\n",
    "\n",
    "# load val shots\n",
    "with open(os.path.join(input_dir,'val_shot.pkl'), 'rb') as f: \n",
    "        val_shot=pickle.load(f, encoding='latin1')\n",
    "\n",
    "        \n",
    "# load model, 'loaded_model'\n",
    "loaded_model=models.load_model(os.path.join(root_dir,model_filename))\n",
    "\n",
    "loaded_model.compile(optimizer=optimizers.RMSprop(lr=.001),\n",
    "                     metrics=['mae'], loss='mse')\n",
    "\n",
    "\n",
    "# load process true train times, 'train_real_times'\n",
    "with open(os.path.join(input_dir,'train_time.pkl'), 'rb') as f: \n",
    "        train_real_times=pickle.load(f, encoding='latin1')\n",
    "        \n",
    "# load process true val times, 'val_real_times'\n",
    "with open(os.path.join(input_dir,'val_time.pkl'), 'rb') as f: \n",
    "        val_real_times=pickle.load(f, encoding='latin1')\n",
    "        \n",
    "        \n",
    "# # load standard rho points, 'rho_points'\n",
    "# with open(os.path.join(input_dir,'rho_standard.pkl'), 'rb') as f: \n",
    "#         rho_points=pickle.load(f, encoding='latin1')\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# list of 0D and 1D signals, 'sig_keys_0d' and 'sig_keys_1d'\n",
    "sig_keys_0d = config['sigs_0d'] #['curr_target', 'pinj']\n",
    "sig_keys_1d = config['sigs_1d'] #['e_temp']\n",
    "sig_keys_predict = config['sigs_predict']\n",
    "\n",
    "all_sigs = sig_keys_0d+sig_keys_1d\n",
    "\n",
    "print(all_sigs)\n",
    "\n",
    "\n",
    "# delay and lookback values\n",
    "delay = config['delay']\n",
    "lookback = config['lookback']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_points=np.linspace(0,1,65)\n",
    "# new_data = val_data[:, :, :2]\n",
    "# new_data = np.append(new_data, val_data[:, :, 2:67:4], axis = 2)\n",
    "# new_data = np.append(new_data, val_data[:, :, 67::4], axis = 2)\n",
    "# input_data = new_data\n",
    "# output_data=val_target[:, ::4]\n",
    "\n",
    "input_data = val_data\n",
    "output_data = val_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "means = helper_functions.load_obj(os.path.join(input_dir,'means'))\n",
    "stds = helper_functions.load_obj(os.path.join(input_dir,'stds'))\n",
    "\n",
    "\n",
    "# means['thomson_dens'] = means['thomson_dens'][::4]\n",
    "# means['thomson_temp'] = means['thomson_temp'][::4]\n",
    "# means['temp'] = means['temp'][::4]\n",
    "\n",
    "\n",
    "# stds['thomson_dens'] = stds['thomson_dens'][::4]\n",
    "# stds['thomson_temp'] = stds['thomson_temp'][::4]\n",
    "# stds['temp'] = stds['temp'][::4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_train_dict = {}\n",
    "generated_target_dict = {}\n",
    "\n",
    "\n",
    "# TRAINING WORK***************************************************************************************************\n",
    "\n",
    "# combine the actuators together    \n",
    "future_actuators = np.concatenate(([self.data[\"future_actuators\"][sig][inds, :, np.newaxis] for sig in self.sigs_0d]), axis =2)\n",
    "previous_actuators = np.concatenate(([self.data[\"previous_actuators\"][sig][inds, :, np.newaxis] for sig in self.sigs_0d]), axis =2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         # if you want to lump actuators together (Rory's models) and have profile sigs separate. \n",
    "#         #########################layer names: ----1d sig names, \"all_actuators\"\n",
    "\n",
    "#         generated_train_dict[\"all_actuators\"] = np.concatenate((previous_actuators, future_actuators), axis = 1)\n",
    "#         # get previous profiles data\n",
    "#         for index, sig in enumerate(self.sigs_1d):\n",
    "#             generated_train_dict[sig] = self.data[\"previous_profiles\"][sig][inds]\n",
    "\n",
    "#         ##########################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if you want to split based on time (ie previous vs future) where you mash together all the sigs \n",
    "# Joe's model#####################################\n",
    "# layer names:\n",
    "# ---\"all_previous_sigs\", \"future_actuators\"\n",
    "\n",
    "# combine all profiles\n",
    "prev_profiles = np.concatenate(([self.data[\"previous_profiles\"][sig][inds] for sig in self.sigs_1d]), axis = 2)\n",
    "prev_profiles = np.concatenate((prev_profiles, previous_actuators), axis = 2)\n",
    "\n",
    "# read data into the output dictionary\n",
    "generated_train_dict[\"all_previous_sigs\"] = prev_profiles\n",
    "\n",
    "generated_train_dict[\"future_actuators\"] = future_actuators\n",
    "\n",
    "###################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TARGET WORK ####################################\n",
    "for index, sig in enumerate(self.sigs_predict):\n",
    "generated_target_dict[\"target_{}\".format(sig)] = self.target[sig][inds]\n",
    "\n",
    "\n",
    "\n",
    "return generated_train_dict, generated_target_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ML model performance for full\n",
    "val_mae = abs(loaded_model.predict(input_data)-output_data)\n",
    "avg_val_mae=np.mean(val_mae, axis=0)\n",
    "std_val_mae=np.std(val_mae, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for full\n",
    "baseline_mae=abs(output_data)\n",
    "avg_baseline_mae=np.mean(baseline_mae, axis=0)\n",
    "std_baseline_mae=np.std(baseline_mae, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following must have already been run: \n",
    "# shot_indices_dictionary = helper_functions.load_obj(input_dir+'shot_indices')\n",
    "# train_shots = shot_indices_dictionary['train_shot_names']\n",
    "# val_shots = shot_indices_dictionary['val_shot_names']\n",
    "# train_shot_inds = shot_indices_dictionary['train_shot_indices']\n",
    "# val_shot_inds = shot_indices_dictionary['val_shot_indices']\n",
    "\n",
    "def plot_timestep(timestep, train=False, normalized=True, \n",
    "                  sig_to_perturb=None):\n",
    "    \n",
    "    delay = config['delay']\n",
    "    titlesize=20\n",
    "    othersize=20\n",
    "    legendsize=15\n",
    "    linewidth=7\n",
    "    smalllinewidth=3\n",
    "    \n",
    "    if(train):\n",
    "        data=train_data\n",
    "        target=train_target\n",
    "        shot_nums=train_shot\n",
    "        #shot_inds=train_shot_inds\n",
    "        true_times=train_real_times\n",
    "    else:\n",
    "        data=val_data\n",
    "        target=val_target\n",
    "        shot_nums=val_shot\n",
    "        #shot_inds=val_shot_inds\n",
    "        true_times=val_real_times\n",
    "        \n",
    "#     new_data = data[:, :, :2]\n",
    "#     new_data = np.append(new_data, data[:, :, 2:67:4], axis = 2)\n",
    "#     new_data = np.append(new_data, data[:, :, 67::4], axis = 2)\n",
    "#     data = new_data\n",
    "\n",
    "#     # get every 4th rho point for sig predict\n",
    "#     new_target = target[:, ::4]\n",
    "#     target = new_target\n",
    "    \n",
    "    \n",
    "\n",
    "    shot_num=shot_nums[timestep]\n",
    "    \n",
    "    shot_range = np.where(shot_nums == shot_num)[0]\n",
    "    times = list(range(shot_range[0],shot_range[-1]+1))\n",
    "#     times=list(range(shot_inds[index],shot_inds[index+1]))\n",
    "    \n",
    "    input_data=np.array(data)[timestep:timestep+1]\n",
    "    #input_data[:,:,:3]=0\n",
    "    \n",
    "    pred=np.ndarray.flatten(loaded_model.predict(input_data))                      \n",
    "    \n",
    "    true=target[timestep]\n",
    "    # note that timestep 0 will now throw an error \n",
    "    prev=target[timestep-1] #data[timestep][-1][num_sigs:]\n",
    "    \n",
    "    if normalized:\n",
    "        pred_real=pred\n",
    "        true_real=true\n",
    "        prev_real=prev\n",
    "    else:\n",
    "        pred_real = np.multiply(pred,stds['temp'])+means['temp']\n",
    "        true_real = np.multiply(true,stds['temp'])+means['temp']\n",
    "        #prev_real = (np.multiply(prev,stds['thomson_temp'])+means['thomson_temp'])/1000.\n",
    "        prev_real = (np.multiply(prev,stds['temp'])+means['temp'])\n",
    "    \n",
    "    sigs=data[timestep][-1][:len(sig_keys_0d)] #pinj, tinj, curr\n",
    "    sigs_all_times=np.array(data)[times,-delay-1,:].T\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    baseline_error = abs(true) #abs(true-prev)\n",
    "    pred_error = abs(true-pred)\n",
    "    \n",
    "    ax = fig.add_subplot('222')\n",
    "    ax.set_title('Electron temperature predictions by the ML model', fontsize=titlesize)\n",
    "    ax.plot(rho_points,pred_real,label='Predicted', linewidth=smalllinewidth)\n",
    "    \n",
    "    \n",
    "    #### FOR SENSITIVITY ANALYSIS ####\n",
    "    if sig_to_perturb is not None:\n",
    "        if sig_to_perturb in sig_keys_0d:\n",
    "            sig_index=sig_keys_0d.index(sig_to_perturb)\n",
    "        else:\n",
    "            sig_index=range(len(sig_keys_0d),input_data.shape[2])\n",
    "\n",
    "        input_data_perturbed=input_data.copy()\n",
    "\n",
    "\n",
    "        # Flatline at the previous value\n",
    "        input_data_perturbed[:,:,sig_index]=input_data_perturbed[:,0,sig_index]\n",
    "        # Increase linearly\n",
    "        #input_data_perturbed[:,:,sig_index]*=[1+i*.1 for i in range(input_data_perturbed.shape[1])]\n",
    "        # add 10%\n",
    "        #input_data_perturbed[:,:,sig_index] *= 1.1\n",
    "\n",
    "\n",
    "\n",
    "        pred_perturbed=np.ndarray.flatten(loaded_model.predict(input_data_perturbed))\n",
    "        if normalized:\n",
    "            pred_perturbed_real=pred_perturbed\n",
    "        else: \n",
    "            if config['n_components']!='None':\n",
    "                # do the processing with pca, which was already loaded\n",
    "                pred_perturbed = pca.inverse_transform(pred_perturbed)\n",
    "            pred_perturbed_real = np.multiply(pred_perturbed,stds['e_temp'])+means['e_temp']\n",
    "        ax.plot(rho_points,pred_perturbed_real,\n",
    "                label='Predicted with perturbation to {}'.format(sig_to_perturb), \n",
    "                linewidth=smalllinewidth)\n",
    "    ##################################\n",
    "    \n",
    "    \n",
    "    ax.plot(rho_points,true_real,label='True', linewidth=smalllinewidth)\n",
    "    #for differences\n",
    "    if normalized:\n",
    "        ax.axhline(0,label='Baseline',linewidth=smalllinewidth, color='k')\n",
    "    else:\n",
    "        ax.plot(rho_points,prev_real,label='Previous', linewidth=smalllinewidth)\n",
    "    \n",
    "    ax.set_ylabel('Normalized e_temp', fontsize=othersize)\n",
    "    ax.set_xlim(0,1)\n",
    "    #ax.set_xlabel('Normalized rho', fontsize=othersize)\n",
    "    ax.legend(loc='best', fontsize=legendsize)\n",
    "    \n",
    "    ax = fig.add_subplot('224')\n",
    "    ax.set_title('Error in e_temp predictions', fontsize=titlesize)\n",
    "    ax.plot(rho_points, pred_error, color='red', label='Model error for one example', linewidth=linewidth)\n",
    "    ax.plot(rho_points, avg_val_mae, color='red', linestyle='--', label='Average prediction error', linewidth=smalllinewidth)\n",
    "    ax.fill_between(rho_points,avg_val_mae-std_val_mae, avg_val_mae+std_val_mae, color='red', alpha=.2)\n",
    "    ax.axhline(0,color='k')\n",
    "    ax.plot(rho_points, baseline_error, color='b', label='Baseline error for one example', linewidth=linewidth)\n",
    "    ax.plot(rho_points, avg_baseline_mae, color='b', linestyle='--', label='Average baseline error', linewidth=smalllinewidth)\n",
    "    ax.fill_between(rho_points,avg_baseline_mae-std_baseline_mae, avg_baseline_mae+std_baseline_mae, color='blue', alpha=.2)\n",
    "    ax.set_ylim(0,.25)\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.legend(loc='best', fontsize=legendsize)\n",
    "    ax.set_ylabel('Error (abs val)', fontsize=othersize)\n",
    "    ax.set_xlabel('Normalized rho', fontsize=othersize)\n",
    "        \n",
    "        #textstr='Baseline error: %.2f \\nModel error: %.3f'%(np.mean(baseline_error),np.mean(pred_error))\n",
    "        #ax.text(0.05,0.1,textstr, transform=ax.transAxes)\n",
    "    \n",
    "    #plt.subplots_adjust(hspace=.5)\n",
    "\n",
    "    for i in range(len(sig_keys_0d)):\n",
    "        which_plot='{}2{}'.format(len(sig_keys_0d)+len(sig_keys_1d),2*i+1) \n",
    "        ax=fig.add_subplot(which_plot) #axes[i+1]\n",
    "        #ax.plot(times,sigs_all_times[i])\n",
    "        if normalized:\n",
    "            ax.plot(true_times[times],sigs_all_times[i])\n",
    "            if sig_to_perturb is not None:\n",
    "                if (i==sig_index):\n",
    "                    ax.plot(true_times[timestep-config['lookback']:timestep+config['delay']+1],\n",
    "                        input_data_perturbed[0,:,i],\n",
    "                        linewidth=10)\n",
    "        else:\n",
    "            normalized_sig=(sigs_all_times[i]*stds[sig_keys_0d[i]])+means[sig_keys_0d[i]]\n",
    "            ax.plot(true_times[times],normalized_sig)\n",
    "        ax.set_ylabel(sig_keys_0d[i], fontsize=titlesize)\n",
    "        ax.axvline(true_times[timestep],color='r')\n",
    "        ax.axvline(true_times[timestep+delay],color='b')\n",
    "            \n",
    "    for i in range(len(sig_keys_1d)):\n",
    "        which_plot='{}2{}'.format(len(sig_keys_0d)+len(sig_keys_1d),2*(len(sig_keys_0d)+i)+1) \n",
    "        ax=fig.add_subplot(which_plot) #axes[i+1]\n",
    "        #ax.plot(times,sigs_all_times[i])\n",
    "        rho_len_in = int((data.shape[2]-len(sig_keys_0d))/len(sig_keys_1d))\n",
    "        if normalized:\n",
    "            sig_trace_1d = np.mean(sigs_all_times[len(sig_keys_0d)+i*rho_len_in:len(sig_keys_0d)+(i+1)*rho_len_in],axis=0)\n",
    "            ax.plot(true_times[times],sig_trace_1d)\n",
    "        else:\n",
    "            tmp = sigs_all_times[len(sig_keys_0d)+i*rho_len_in:len(sig_keys_0d)+(i+1)*rho_len_in]\n",
    "            if config['n_components']!='None':\n",
    "                # do the processing with pca, which was already loaded\n",
    "                tmp = pca.inverse_transform(tmp)\n",
    "            # convert back by multiplying by std, adding mean\n",
    "            sig_trace_1d = (stds[sig_keys_1d[i]].dot(tmp)+sum(means[sig_keys_1d[i]]))/rho_len_in\n",
    "            ax.plot(true_times[times],sig_trace_1d)\n",
    "        ax.set_ylabel('mean {}'.format(sig_keys_1d[i]), fontsize=titlesize)\n",
    "        ax.axvline(true_times[timestep],color='r')\n",
    "        ax.axvline(true_times[timestep+delay],color='b')\n",
    "        if i==len(sig_keys_0d)+len(sig_keys_1d)-1:\n",
    "            ax.set_xlabel('Time (ms)', fontsize=othersize)\n",
    "    \n",
    "    fig.suptitle('Shot {} (timestep {}) for 100ms delay, 50ms intervals. ALL RHO POINTS'.format(shot_num,timestep))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 15]\n",
    "font={'size': 20, 'weight': 'heavy'}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "timestep=random.randint(0,len(val_data))\n",
    "# timestep=np.random.choice(val_shot_inds[:-1])+np.random.randint(10)\n",
    "\n",
    "#random.randint(0,len(train_data))\n",
    "#timestep=np.random.choice(train_shot_inds)+np.random.randint(10)\n",
    "\n",
    "#i+=2\n",
    "#timestep=val_shot_inds[val_shots.index(163174)]+i\n",
    "#timestep=val_shot_inds[42]+20\n",
    "#timestep=56410\n",
    "\n",
    "\n",
    "shot_nums = val_shot\n",
    "shot_num=shot_nums[timestep]\n",
    "    \n",
    "shot_range = np.where(shot_nums == shot_num)[0]\n",
    "timestep = shot_range[0] + 1\n",
    "\n",
    "\n",
    "\n",
    "plot_timestep(timestep, train=True, normalized=True, sig_to_perturb=None)\n",
    "\n",
    "\n",
    "#14030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
