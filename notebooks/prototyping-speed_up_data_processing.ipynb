{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "efit_type = 'EFIT02'\n",
    "\n",
    "scenario = {'actuator_names': ['pinj', 'curr', 'tinj'],\n",
    "                    'profile_names': ['thomson_temp_{}'.format(efit_type),\n",
    "                                      'thomson_dens_{}'.format(efit_type),\n",
    "                                      'ffprime_{}'.format(efit_type),\n",
    "                                      'press_{}'.format(efit_type),\n",
    "                                      'q_{}'.format(efit_type)],\n",
    "                    'scalar_names': [],\n",
    "                    'profile_downsample': 2,\n",
    "                    'state_encoder_type': 'dense',\n",
    "                    'state_decoder_type': 'dense',\n",
    "                    'control_encoder_type': 'dense',\n",
    "                    'control_decoder_type': 'dense',\n",
    "                    'state_encoder_kwargs': {'num_layers': 6,\n",
    "                                             'layer_scale': 2,\n",
    "                                             'std_activation':'relu'},\n",
    "                    'state_decoder_kwargs': {'num_layers': 6,\n",
    "                                             'layer_scale': 2,\n",
    "                                             'std_activation':'relu'},\n",
    "                    'control_encoder_kwargs': {'num_layers': 10,\n",
    "                                               'layer_scale': 2,\n",
    "                                               'std_activation':'relu'},\n",
    "                    'control_decoder_kwargs': {'num_layers': 10,\n",
    "                                               'layer_scale': 2,\n",
    "                                               'std_activation':'relu'},\n",
    "                    'state_latent_dim':50,\n",
    "                    'control_latent_dim':5,\n",
    "                    'x_weight':1,\n",
    "                    'u_weight':1,\n",
    "                    'discount_factor':1,\n",
    "                    'batch_size': 128,\n",
    "                    'epochs': 100,\n",
    "                    'flattop_only': True,\n",
    "                    'raw_data_path': '/scratch/gpfs/jabbate/mixed_data/final_data.pkl',\n",
    "                    'process_data': True,\n",
    "                    'processed_filename_base': '/scratch/gpfs/jabbate/data_60_ms_randomized_',\n",
    "                    'optimizer': 'adagrad',\n",
    "                    'optimizer_kwargs': {},\n",
    "                    'shuffle_generators': True,\n",
    "                    'pruning_functions': ['remove_nan', 'remove_dudtrip', 'remove_I_coil'],\n",
    "                    'normalization_method': 'RobustScaler',\n",
    "                    'window_length': 1,\n",
    "                    'window_overlap': 0,\n",
    "                    'lookback': 0,\n",
    "                    'lookahead': 3,\n",
    "                    'sample_step': 1,\n",
    "                    'uniform_normalization': True,\n",
    "                    'train_frac': 0.8,\n",
    "                    'val_frac': 0.2,\n",
    "                    'nshots': 12000,\n",
    "                    'excluded_shots': ['topology_TOP', 'topology_OUT', 'topology_MAR', 'topology_IN', 'topology_DN', 'topology_BOT']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import TensorBoard\n",
    "from helpers.normalization import normalize\n",
    "from helpers.pruning_functions import remove_dudtrip, remove_I_coil, remove_ECH, remove_gas, remove_nan\n",
    "from tqdm import tqdm\n",
    "from helpers import exclude_shots\n",
    "import numba\n",
    "\n",
    "def process_data(rawdata, sig_names, normalization_method, window_length=1,\n",
    "                 window_overlap=0, lookbacks={}, lookahead=3, sample_step=5,\n",
    "                 uniform_normalization=True, train_frac=0.7, val_frac=0.2,\n",
    "                 nshots=None,\n",
    "                 verbose=1, flattop_only=True, randomize=True, **kwargs):\n",
    "    \"\"\"Organize data into correct format for training\n",
    "\n",
    "    Gathers raw data into bins, group into training sequences, normalize,\n",
    "    and split into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        rawdata (dict): Nested dictionary of raw signal data, or path to pickle.\n",
    "            Should be of the form rawdata[shot][signal_name] = signal_data.\n",
    "        sig_names (list): List of signal names as strings.\n",
    "        normalization_method (str): One of `StandardScaler`, `MinMax`, `MaxAbs`,\n",
    "            `RobustScaler`, `PowerTransform`.\n",
    "        window_length (int): Number of samples to average over in each bin/window.\n",
    "        window_overlap (int): How many timesteps to overlap windows.\n",
    "        lookbacks (dict of int): How many window lengths for lookback for each sig.\n",
    "        lookahead (int): How many window lengths to predict into the future.\n",
    "        sample_step (int): How much to offset sequential training sequences.\n",
    "            Step of 1 means sample[i] and sample[i+1] will be offset by 1, with\n",
    "            the rest overlapping.\n",
    "        uniform_normalization (bool): 'True' uses the same normalization\n",
    "            parameters over a whole profile, 'False' normalizes each spatial\n",
    "            point separately.\n",
    "        val_frac (float): Fraction of samples to use for validation.\n",
    "        nshots (int): How many shots to use. If None, all available will be used.\n",
    "        verbose (int): verbosity level. 0 is no CL output, 1 shows progress, 2 is abbreviated.\n",
    "        flattop_only (bool): Whether to only include data from flattop.\n",
    "\n",
    "    Returns:\n",
    "        traindata (dict): Dictionary of numpy arrays, one entry for each signal.\n",
    "            Each array has shape [nsamples,lookback+lookahead,signal_shape]\n",
    "        valdata (dict): Dictionary of numpy arrays, one entry for each signal.\n",
    "            Each array has shape [nsamples,lookback+lookahead,signal_shape]\n",
    "        param_dict (dict): Dictionary of parameters used during normalization,\n",
    "            to be used for denormalizing later. Eg, mean, stddev, method, etc.\n",
    "    \"\"\"\n",
    "    ##############################\n",
    "    # Load data\n",
    "    ##############################\n",
    "    if type(rawdata) is not dict:\n",
    "        if verbose:\n",
    "            print('Loading')\n",
    "        abs_path = Path(rawdata).resolve()\n",
    "        if abs_path.exists():\n",
    "            with open(abs_path, 'rb') as f:\n",
    "                rawdata = pickle.load(f, encoding='latin1')\n",
    "        else:\n",
    "            print(abs_path)\n",
    "            raise IOError(\"No such path to data file\")\n",
    "            \n",
    "    ##############################\n",
    "    # get pruning functions\n",
    "    ##############################\n",
    "    pruning_functions = kwargs.get('pruning_functions', [])\n",
    "    if 'ech' not in sig_names:\n",
    "        pruning_functions.append('remove_ECH')\n",
    "    if not {'gasB', 'gasC', 'gasD', 'gasE'}.issubset(set(sig_names)):\n",
    "        pruning_functions.append('remove_gas')\n",
    "    prun_dict = {'remove_nan': remove_nan,\n",
    "                 'remove_ECH': remove_ECH,\n",
    "                 'remove_I_coil': remove_I_coil,\n",
    "                 'remove_gas': remove_gas,\n",
    "                 'remove_dudtrip': remove_dudtrip}\n",
    "    for i, elem in enumerate(pruning_functions):\n",
    "        if isinstance(elem, str):\n",
    "            pruning_functions[i] = prun_dict[elem]\n",
    "\n",
    "    ##############################\n",
    "    # get excluded shots\n",
    "    ##############################\n",
    "    excluded_shots = kwargs.get('excluded_shots', [])\n",
    "    exclude_dict = {'topology_TOP': exclude_shots.topology_TOP,\n",
    "                    'topology_SNT': exclude_shots.topology_SNT,\n",
    "                    'topology_SNB': exclude_shots.topology_SNB,\n",
    "                    'topology_OUT': exclude_shots.topology_OUT,\n",
    "                    'topology_MAR': exclude_shots.topology_MAR,\n",
    "                    'topology_IN': exclude_shots.topology_IN,\n",
    "                    'topology_DN': exclude_shots.topology_DN,\n",
    "                    'topology_BOT': exclude_shots.topology_BOT}\n",
    "    for i, elem in enumerate(excluded_shots):\n",
    "        if isinstance(elem, str):\n",
    "            excluded_shots[i] = exclude_dict[elem]\n",
    "        if not isinstance(elem, list):\n",
    "            excluded_shots[i] = [elem]\n",
    "    excluded_shots = [item for sublist in excluded_shots for item in sublist]\n",
    "\n",
    "    ##############################\n",
    "    # get sig names\n",
    "    ##############################\n",
    "    extra_sigs = ['time', 'shotnum']\n",
    "    if remove_dudtrip in pruning_functions:\n",
    "        extra_sigs += ['dud_trip']\n",
    "    if remove_I_coil in pruning_functions:\n",
    "        extra_sigs += ['bt', 'curr', 'C_coil_method', 'I_coil_method']\n",
    "    if remove_gas in pruning_functions:\n",
    "        extra_sigs += ['gasB', 'gasC', 'gasD', 'gasE', 'pfx1', 'pfx2']\n",
    "    if remove_ECH in pruning_functions:\n",
    "        extra_sigs += ['ech']\n",
    "    sig_names = list(np.unique(sig_names))\n",
    "    sigsplustime = list(np.unique(sig_names + extra_sigs))\n",
    "    if verbose:\n",
    "        print('Signals: ' + ', '.join(sig_names))\n",
    "\n",
    "    ##############################\n",
    "    # figure out lookbacks\n",
    "    ##############################\n",
    "    if isinstance(lookbacks, int):\n",
    "        max_lookback = lookbacks\n",
    "        lookbacks = {sig: max_lookback for sig in sigsplustime}\n",
    "    else:\n",
    "        max_lookback = 0\n",
    "        for val in lookbacks.values():\n",
    "            if val > max_lookback:\n",
    "                max_lookback = val\n",
    "        for sig in sigsplustime:\n",
    "            if sig not in lookbacks.keys():\n",
    "                lookbacks[sig] = max_lookback\n",
    "            \n",
    "    ##############################\n",
    "    # find which shots have all the signals needed\n",
    "    ##############################\n",
    "    usabledata = []\n",
    "    all_shots = sorted(list(rawdata.keys()))\n",
    "    for shot in all_shots:\n",
    "        rawdata[shot]['shotnum'] = np.ones(rawdata[shot]['time'].shape[0])*shot\n",
    "        if set(sigsplustime).issubset(set(rawdata[shot].keys())) \\\n",
    "           and rawdata[shot]['time'].size > (max_lookback+lookahead) \\\n",
    "           and shot not in excluded_shots:\n",
    "            usabledata.append(rawdata[shot])\n",
    "    usabledata = np.array(usabledata)\n",
    "    del rawdata\n",
    "    gc.collect()\n",
    "    if nshots is not None:\n",
    "        nshots = np.minimum(nshots, len(usabledata))\n",
    "        usabledata = usabledata[:nshots]\n",
    "    else:\n",
    "        nshots = len(usabledata)\n",
    "    if verbose:\n",
    "        print('Number of useable shots: ', str(len(usabledata)))\n",
    "        print('Number of shots used: ', str(nshots))\n",
    "        sys.stdout.flush()\n",
    "    if verbose:\n",
    "        t = 0\n",
    "        for shot in usabledata:\n",
    "            t += shot['time'].size\n",
    "        print('Total number of timesteps: ', str(t))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    ##############################\n",
    "    # some helper functions\n",
    "    ##############################          \n",
    "    def moving_average(a, n):\n",
    "        \"\"\"moving average of array a with window size n\"\"\"\n",
    "        ret = np.nancumsum(a, axis=0)\n",
    "        ret[n:] = ret[n:] - ret[:-n]\n",
    "        return ret[n - 1:] / n\n",
    "\n",
    "    def is_valid(shot):\n",
    "        \"\"\"checks if a shot is completely NaN or if it never reached flattop\"\"\"\n",
    "        for sig in sigsplustime:\n",
    "            if np.isnan(shot[sig]).all():  # or np.isinf(shot[sig]).any():\n",
    "                return False\n",
    "        if (flattop_only):\n",
    "            if (shot['t_ip_flat'] == None or shot['ip_flat_duration'] == None):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def get_non_nan_inds(arr):\n",
    "        \"\"\"gets indices of array where value is not NaN\"\"\"\n",
    "        if len(arr.shape) == 1:\n",
    "            return np.where(~np.isnan(arr))[0]\n",
    "        else:\n",
    "            return np.where(np.any(~np.isnan(arr), axis=1))[0]\n",
    "\n",
    "    def get_first_index(shot):\n",
    "        \"\"\"gets index of first valid timeslice for a shot\"\"\"\n",
    "        input_max = max([get_non_nan_inds(shot[sig])[0] +\n",
    "                         lookbacks[sig] for sig in sig_names])\n",
    "        output_max = max([get_non_nan_inds(shot[sig])[0] -\n",
    "                          lookahead for sig in sig_names])\n",
    "        if (flattop_only) and (shot['t_ip_flat'] != None):\n",
    "            current_max = np.searchsorted(\n",
    "                shot['time'], shot['t_ip_flat'], side='left')\n",
    "            return np.ceil(max(input_max, output_max, current_max)).astype(int)\n",
    "        else:\n",
    "            return np.ceil(max(input_max, output_max)).astype(int)\n",
    "\n",
    "    def get_last_index(shot):\n",
    "        \"\"\"gets index of last valid timeslice for a shot\"\"\"\n",
    "        partial_min = min([get_non_nan_inds(shot[sig])[-1]\n",
    "                           for sig in sig_names])\n",
    "        full_min = min([get_non_nan_inds(shot[sig])[-1] -\n",
    "                        lookahead for sig in sig_names])\n",
    "        if (flattop_only) and (shot['t_ip_flat'] != None) and (shot['ip_flat_duration'] != None):\n",
    "            current_min = np.searchsorted(\n",
    "                shot['time'], shot['t_ip_flat']+shot['ip_flat_duration'], side='right')\n",
    "            return np.floor(min(full_min, partial_min, current_min)).astype(int)\n",
    "        else:\n",
    "            return np.floor(min(full_min, partial_min)).astype(int)\n",
    "    \n",
    "    @numba.njit\n",
    "    def group_data(array,first,last,sample_step,lookback, lookahead):\n",
    "        \"\"\"groups shot data into i/o chunks\"\"\"\n",
    "        data = []\n",
    "        for i in range(first,last,sample_step):\n",
    "            data.append(array[i-lookback:i+lookahead+1])\n",
    "        return data\n",
    "    \n",
    "    ##############################\n",
    "    # loop through shots and do stuff\n",
    "    ##############################\n",
    "    alldata = {}\n",
    "    shots_with_complete_nan = []\n",
    "    for sig in sigsplustime:\n",
    "        alldata[sig] = []  # initalize empty lists\n",
    "    for shot in tqdm(usabledata, desc='Gathering', ascii=True, dynamic_ncols=True,\n",
    "                     disable=not verbose == 1):\n",
    "        ##############################\n",
    "        # take moving average of data and bin it\n",
    "        ##############################\n",
    "        binned_shot = {}\n",
    "        for sig in sigsplustime:\n",
    "            if np.any(np.isinf(shot[sig])):\n",
    "                shot[sig][np.isinf(shot[sig])] = np.nan            \n",
    "            binned_shot[sig] = moving_average(shot[sig],window_length)[::window_length-window_overlap]\n",
    "        binned_shot['t_ip_flat'] = shot['t_ip_flat']\n",
    "        binned_shot['ip_flat_duration'] = shot['ip_flat_duration']\n",
    "        if not is_valid(binned_shot):\n",
    "            shots_with_complete_nan.append(np.unique(shot[\"shotnum\"]))\n",
    "            continue\n",
    "\n",
    "        ##############################\n",
    "        # group into arrays of input/output pairs\n",
    "        ##############################\n",
    "        first = get_first_index(binned_shot)\n",
    "        last = get_last_index(binned_shot)\n",
    "        for sig in sigsplustime:\n",
    "            alldata[sig] += group_data(binned_shot[sig],first,last,sample_step,lookbacks[sig],lookahead)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Shots with Complete NaN: \" + ', '.join(str(e)\n",
    "                                                      for e in shots_with_complete_nan))\n",
    "    sys.stdout.flush()\n",
    "    del usabledata\n",
    "    gc.collect()\n",
    "    \n",
    "    ##############################\n",
    "    # stack data from all shots together\n",
    "    ##############################    \n",
    "    for sig in tqdm(sigsplustime, desc='Stacking', ascii=True, dynamic_ncols=True,\n",
    "                    disable=not verbose == 1):\n",
    "        alldata[sig] = np.stack(alldata[sig])\n",
    "    print(\"{} samples total\".format(len(alldata['time'])))\n",
    "    sys.stdout.flush()\n",
    "    ##############################\n",
    "    # apply pruning functions\n",
    "    ##############################\n",
    "    # call fns in the right order to speed things up\n",
    "    if remove_ECH in pruning_functions:\n",
    "        alldata = remove_ECH(alldata,verbose)\n",
    "    if remove_gas in pruning_functions:\n",
    "        alldata = remove_gas(alldata,verbose)\n",
    "    if remove_I_coil in pruning_functions:\n",
    "        alldata = remove_I_coil(alldata,verbose)\n",
    "    if remove_nan in pruning_functions:\n",
    "        alldata = remove_nan(alldata,verbose)\n",
    "    if remove_dudtrip in pruning_functions:\n",
    "        alldata = remove_dudtrip(alldata,verbose)\n",
    "\n",
    "    print(\"{} samples remaining after pruning\".format(len(alldata['time'])))\n",
    "    sys.stdout.flush()\n",
    "    ##############################\n",
    "    # normalize data\n",
    "    ##############################    \n",
    "    alldata, normalization_params = normalize(\n",
    "        alldata, normalization_method, uniform_normalization, verbose)\n",
    "    \n",
    "    ##############################\n",
    "    # split into train and validation sets\n",
    "    ##############################    \n",
    "    nsamples = alldata['time'].shape[0]\n",
    "    inds = np.random.permutation(nsamples) if randomize else np.arange(nsamples)\n",
    "    traininds = inds[:int(nsamples*train_frac)]\n",
    "    valinds = inds[int(nsamples*train_frac)\n",
    "                       :int(nsamples*(val_frac+train_frac))]\n",
    "    traindata = {}\n",
    "    valdata = {}\n",
    "    for sig in tqdm(sigsplustime, desc='Splitting', ascii=True, dynamic_ncols=True,\n",
    "                    disable=not verbose == 1):\n",
    "        traindata[sig] = alldata[sig][traininds]\n",
    "        valdata[sig] = alldata[sig][valinds]\n",
    "    time.sleep(0.1)\n",
    "    if verbose:\n",
    "        print('Total number of samples: ', str(nsamples))\n",
    "        print('Number of training samples: ', str(traininds.size))\n",
    "        print('Number of validation samples: ', str(valinds.size))\n",
    "    return traindata, valdata, normalization_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def prune_loop(inds,shotnumarr,timearr):\n",
    "    remove_inds = set()\n",
    "    for ind in inds:\n",
    "        shot = shotnumarr[ind]\n",
    "        time = timearr[ind]\n",
    "        i = ind\n",
    "        while np.any(shotnumarr[i] == shot) and np.any(timearr[i] >= time):\n",
    "            remove_inds.add(i)\n",
    "            i += 1\n",
    "            if i>=len(timearr):\n",
    "                break\n",
    "    return remove_inds\n",
    "\n",
    "\n",
    "def remove_dudtrip(data, verbose):\n",
    "    if verbose:\n",
    "        print('Removing dudtrip')\n",
    "    dud_trip_inds = np.nonzero(data['dud_trip'])[0]\n",
    "    if len(dud_trip_inds)==0:\n",
    "        return data\n",
    "    remove_inds = prune_loop(dud_trip_inds,data['shotnum'],data['time'])\n",
    "    if verbose:\n",
    "        print(\"Removed {} samples\".format(len(remove_inds)))\n",
    "    keep_inds = set(range(len(data['time']))).difference(remove_inds)\n",
    "    if verbose:\n",
    "        print(\"{} samples remaining\".format(len(keep_inds)))\n",
    "    for sig in data.keys():\n",
    "        data[sig] = data[sig][list(keep_inds)]\n",
    "    return data\n",
    "\n",
    "\n",
    "def remove_I_coil(data, verbose):\n",
    "    if verbose:\n",
    "        print('Removing weird I-coils')\n",
    "    \n",
    "    @numba.njit\n",
    "    def find_Icoil_inds(n,bt,curr,C_coil_method,I_coil_method):\n",
    "        c_coil = list()\n",
    "        i_coil = list()\n",
    "        EFC = list()\n",
    "        for i in range(n):\n",
    "            if np.mean(bt[i]*curr[i]) < 0:\n",
    "                # left-handed\n",
    "                if not set(np.unique(C_coil_method[i])).issubset({5, 0, -1}):\n",
    "                    c_coil.append(i)\n",
    "                if not set(np.unique(I_coil_method[i])).issubset({5, 0, -1}):\n",
    "                    i_coil.append(i)\n",
    "                if not np.all(np.logical_xor(C_coil_method[i] == 5, I_coil_method[i] == 5)):\n",
    "                    EFC.append(i)\n",
    "            else:\n",
    "                # right-handed\n",
    "                if not set(np.unique(C_coil_method[i])).issubset({6, 0, -1}):\n",
    "                    c_coil.append(i)\n",
    "                if not set(np.unique(I_coil_method[i])).issubset({7, 0, -1}):\n",
    "                    i_coil.append(i)\n",
    "                if not np.any(np.logical_or(np.logical_and(C_coil_method[i] == 6, \n",
    "                                                           I_coil_method[i] != 7), \n",
    "                                            np.logical_and(C_coil_method[i] != 6, \n",
    "                                                           I_coil_method[i] == 7))):\n",
    "                    EFC.append(i)\n",
    "                    \n",
    "        coil_inds = c_coil + i_coil + EFC\n",
    "        return coil_inds\n",
    "\n",
    "    coil_inds = np.unique(find_Icoil_inds(len(data['time']),\n",
    "                                          data['bt'],\n",
    "                                          data['curr'],\n",
    "                                          data['C_coil_method'].astype(int),\n",
    "                                          data['I_coil_method'].astype(int)))\n",
    "    if len(coil_inds)==0:\n",
    "        return data\n",
    "    remove_inds = prune_loop(coil_inds,data['shotnum'],data['time'])\n",
    "    if verbose:\n",
    "        print(\"Removed {} samples\".format(len(remove_inds)))\n",
    "    keep_inds = set(range(len(data['time']))).difference(remove_inds)\n",
    "    if verbose:\n",
    "        print(\"{} samples remaining\".format(len(keep_inds)))\n",
    "    for sig in data.keys():\n",
    "        data[sig] = data[sig][list(keep_inds)]\n",
    "    return data\n",
    "\n",
    "\n",
    "def remove_gas(data, verbose):\n",
    "    if verbose:\n",
    "        print('Removing weird gas')\n",
    "    from functools import reduce\n",
    "    threshold=2\n",
    "    gasB_inds = np.nonzero(np.any(data['gasB'] > threshold, axis=1))[0]\n",
    "    gasC_inds = np.nonzero(np.any(data['gasC'] > threshold, axis=1))[0]\n",
    "    gasD_inds = np.nonzero(np.any(data['gasD'] > threshold, axis=1))[0]\n",
    "    gasE_inds = np.nonzero(np.any(data['gasE'] > threshold, axis=1))[0]\n",
    "    pfx1_inds = np.nonzero(np.any(data['pfx1'] > threshold, axis=1))[0]\n",
    "    pfx2_inds = np.nonzero(np.any(data['pfx2'] > threshold, axis=1))[0]\n",
    "    gas_inds = reduce(np.union1d, (gasB_inds, gasC_inds,\n",
    "                                   gasD_inds, gasE_inds, pfx1_inds, pfx2_inds))\n",
    "    if len(gas_inds)==0:\n",
    "        return data\n",
    "    remove_inds = prune_loop(gas_inds,data['shotnum'],data['time'])\n",
    "    if verbose:\n",
    "        print(\"Removed {} samples\".format(len(remove_inds)))\n",
    "    keep_inds = set(range(len(data['time']))).difference(remove_inds)\n",
    "    if verbose:\n",
    "        print(\"{} samples remaining\".format(len(keep_inds)))\n",
    "    for sig in data.keys():\n",
    "        data[sig] = data[sig][list(keep_inds)]\n",
    "    return data\n",
    "\n",
    "\n",
    "def remove_ECH(data, verbose):\n",
    "    if verbose:\n",
    "        print('Removing ECH')\n",
    "    ech_inds = np.nonzero(np.any(data['ech'] > .5, axis=1))[0]\n",
    "    if len(ech_inds)==0:\n",
    "        return data\n",
    "    remove_inds = prune_loop(ech_inds,data['shotnum'],data['time'])\n",
    "    if verbose:\n",
    "        print(\"Removed {} samples\".format(len(remove_inds)))\n",
    "    keep_inds = set(range(len(data['time']))).difference(remove_inds)\n",
    "    if verbose:\n",
    "        print(\"{} samples remaining\".format(len(keep_inds)))\n",
    "    for sig in data.keys():\n",
    "        data[sig] = data[sig][list(keep_inds)]\n",
    "    return data\n",
    "\n",
    "\n",
    "def remove_nan(data, verbose):\n",
    "    if verbose:\n",
    "        print('Removing NaN')\n",
    "    remove_inds = []\n",
    "    for sig in data.keys():\n",
    "        if data[sig].ndim==1:\n",
    "            remove_inds += np.where(np.isnan(data[sig]))[0].tolist()\n",
    "        else:\n",
    "            ax = tuple(np.arange(1,data[sig].ndim).astype(int))\n",
    "            remove_inds += np.where(np.any(np.isnan(data[sig]),axis=ax))[0].tolist()\n",
    "    remove_inds = np.unique(remove_inds)\n",
    "    if verbose:\n",
    "        print(\"Removed {} samples\".format(len(remove_inds)))\n",
    "    keep_inds = set(range(len(data['time']))).difference(remove_inds)\n",
    "    if verbose:\n",
    "        print(\"{} samples remaining\".format(len(keep_inds)))\n",
    "    for sig in data.keys():\n",
    "        data[sig] = data[sig][list(keep_inds)]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from tqdm import tqdm\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def normalize_arr(data, method, uniform_over_profile=True):\n",
    "    \"\"\"Normalizes data before training\n",
    "\n",
    "    Args:\n",
    "        data: Numpy array. Array.shape[0] = samples\n",
    "        method (str): One of `StandardScaler`, `MinMax`, `MaxAbs`,\n",
    "            `RobustScaler`, `PowerTransform`.\n",
    "        uniform_over_profile (bool): 'True' uses the same normalization\n",
    "            parameters over a whole profile, 'False' normalizes each spatial\n",
    "            point separately.\n",
    "\n",
    "    Returns:\n",
    "        data: Numpy array of normalized data.\n",
    "        param_dict (dict): Dictionary of parameters used during normalization,\n",
    "            to be used for denormalizing later. Eg, mean, stddev, method, etc.\n",
    "    \"\"\"\n",
    "    param_dict = {}\n",
    "    # first replace all infs and nans with mean value\n",
    "    data[np.isinf(data)] = np.nan\n",
    "    nanmean = np.nanmean(data, axis=(0, 1))\n",
    "    param_dict['nanmean'] = nanmean\n",
    "    if data.ndim > 2:\n",
    "        for i in range(data.shape[2]):\n",
    "            data[np.isnan(data[:, :, i]), i] = nanmean[i]\n",
    "    else:\n",
    "        data[np.isnan(data)] = nanmean\n",
    "    # then normalize\n",
    "    if method == 'StandardScaler':\n",
    "        if uniform_over_profile or data.ndim < 3:\n",
    "            mean = np.mean(data)\n",
    "            std = np.std(data)\n",
    "        else:\n",
    "            mean = np.mean(data, axis=(0, 1), keepdims=True)\n",
    "            std = np.std(data, axis=(0, 1), keepdims=True)\n",
    "        param_dict.update({'method': method,\n",
    "                           'mean': mean,\n",
    "                           'std': std})\n",
    "        return (data-mean)/np.maximum(std, np.finfo(np.float32).eps), param_dict\n",
    "\n",
    "    elif method == 'MinMax':\n",
    "        if uniform_over_profile or data.ndim < 3:\n",
    "            armin = np.amin(data)\n",
    "            armax = np.amax(data)\n",
    "        else:\n",
    "            armin = np.amin(data, axis=(0, 1), keepdims=True)\n",
    "            armax = np.amax(data, axis=(0, 1), keepdims=True)\n",
    "        param_dict.update({'method': method,\n",
    "                           'armin': armin,\n",
    "                           'armax': armax})\n",
    "        return (data-armin)/np.maximum((armax-armin), np.finfo(np.float32).eps), param_dict\n",
    "\n",
    "    elif method == 'MaxAbs':\n",
    "        if uniform_over_profile or data.ndim < 3:\n",
    "            maxabs = np.amax(np.abs(data))\n",
    "        else:\n",
    "            maxabs = np.amax(np.abs(data), axis=(0, 1), keepdims=True)\n",
    "        param_dict.update({'method': method,\n",
    "                           'maxabs': maxabs})\n",
    "        return data/np.maximum(maxabs, np.finfo(np.float32).eps), param_dict\n",
    "\n",
    "    elif method == 'RobustScaler':\n",
    "        if uniform_over_profile or data.ndim < 3:\n",
    "            median = np.median(data)\n",
    "            iqr = np.subtract(*np.percentile(data, [75, 25]))\n",
    "        else:\n",
    "            median = np.median(data, axis=0)\n",
    "            iqr = np.subtract(*np.percentile(data, [75, 25], axis=(0, 1)))\n",
    "        param_dict.update({'method': method,\n",
    "                           'median': median,\n",
    "                           'iqr': iqr})\n",
    "        return (data-median)/np.maximum(iqr, np.finfo(np.float32).eps), param_dict\n",
    "\n",
    "    elif method == 'PowerTransform':\n",
    "        def yeo_johnson_transform(x, lmbda):\n",
    "            \"\"\"Return transformed input x following Yeo-Johnson transform with\n",
    "            parameter lambda.\n",
    "            \"\"\"\n",
    "            out = np.zeros_like(x)\n",
    "            pos = x >= 0  # binary mask\n",
    "            # when x >= 0\n",
    "            if abs(lmbda) < np.finfo(np.float32).eps:\n",
    "                out[pos] = np.log1p(x[pos])\n",
    "            else:  # lmbda != 0\n",
    "                out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda\n",
    "            # when x < 0\n",
    "            if abs(lmbda - 2) > np.finfo(np.float32).eps:\n",
    "                out[~pos] = - \\\n",
    "                    (np.power(-x[~pos] + 1, 2 - lmbda) - 1) / (2 - lmbda)\n",
    "            else:  # lmbda == 2\n",
    "                out[~pos] = -np.log1p(-x[~pos])\n",
    "            return out\n",
    "\n",
    "        def yeo_johnson_optimize(x):\n",
    "            \"\"\"Find and return optimal lambda parameter of the Yeo-Johnson\n",
    "            transform by MLE, for observed data x.\n",
    "            Like for Box-Cox, MLE is done via the brent optimizer. From Scipy\n",
    "            \"\"\"\n",
    "            def _neg_log_likelihood(lmbda):\n",
    "                \"\"\"Return the negative log likelihood of the observed data x as a\n",
    "                function of lambda. From Scipy\"\"\"\n",
    "                x_trans = yeo_johnson_transform(x, lmbda)\n",
    "                n_samples = x.shape[0]\n",
    "                loglike = -n_samples / 2 * np.log(x_trans.var())\n",
    "                loglike += (lmbda - 1) * (np.sign(x) *\n",
    "                                          np.log1p(np.abs(x))).sum()\n",
    "                return -loglike\n",
    "            # choosing bracket -2, 2 like for boxcox\n",
    "            return optimize.brent(_neg_log_likelihood, brack=(-2, 2))\n",
    "        if uniform_over_profile or data.ndim < 3:\n",
    "            lmbda = yeo_johnson_optimize(data.flatten())\n",
    "            y = yeo_johnson_transform(\n",
    "                data.flatten(), lmbda).reshape(data.shape)\n",
    "            mean = np.mean(y)\n",
    "            std = np.std(y)\n",
    "        else:\n",
    "            y = np.zeros_like(data)\n",
    "            lmbda = np.array([yeo_johnson_optimize(data[:, :, i])\n",
    "                              for i in range(data.shape[2])])\n",
    "            for i, l in enumerate(lmbda):\n",
    "                y[:, :, i] = yeo_johnson_transform(data[:, :, i], l)\n",
    "            mean = np.mean(y, axis=(0, 1))\n",
    "            std = np.std(y, axis=(0, 1))\n",
    "        param_dict.update({'method': method,\n",
    "                           'lambda': lmbda,\n",
    "                           'mean': mean,\n",
    "                           'std': std})\n",
    "        return (y-mean)/np.maximum(std, np.finfo(np.float32).eps), param_dict\n",
    "    elif method is None or method == 'None':\n",
    "        param_dict.update({'method': method})\n",
    "        return data, param_dict\n",
    "    else:\n",
    "        raise ValueError(\"Unknown normalization method\")\n",
    "\n",
    "\n",
    "def normalize(data, method, uniform_over_profile=True, verbose=1):\n",
    "    \"\"\"Normalizes data before training\n",
    "\n",
    "    Args:\n",
    "        data: Numpy array or dictionary of numpy arrays. If a dictionary, all\n",
    "            arrays are normalized using the same method, but each array with\n",
    "            respect to itself. Array.shape[0] = batches\n",
    "        method (str): One of `StandardScaler`, `MinMax`, `MaxAbs`,\n",
    "            `RobustScaler`, `PowerTransform`.\n",
    "        uniform_over_profile (bool): 'True' uses the same normalization\n",
    "            parameters over a whole profile, 'False' normalizes each spatial\n",
    "            point separately.\n",
    "        verbose (int): verbosity level. 0 is no CL output, 1 shows progress, 2 abbreviates.\n",
    "\n",
    "    Returns:\n",
    "        data: Numpy array or dictionary of numpy arrays. Normalized data.\n",
    "        param_dict (dict): Dictionary of parameters used during normalization,\n",
    "            to be used for denormalizing later. Eg, mean, stddev, method, etc.\n",
    "    \"\"\"\n",
    "    if type(data) is dict:\n",
    "        param_dict = {}\n",
    "        for key in tqdm(data.keys(), desc='Normalizing', ascii=True, dynamic_ncols=True,\n",
    "                        disable=not verbose==1):\n",
    "            if key not in ['time', 'shotnum']:\n",
    "                data[key], p = normalize_arr(\n",
    "                    data[key], method, uniform_over_profile)\n",
    "                param_dict[key] = p\n",
    "        return data, param_dict\n",
    "    else:\n",
    "        return normalize_arr(data, method, uniform_over_profile)\n",
    "\n",
    "\n",
    "def denormalize_arr(data, param_dict):\n",
    "    \"\"\"Denormalizes data after training\n",
    "\n",
    "    Args:\n",
    "        data: Numpy array of data to denorm.\n",
    "        param_dict (dict): Dictionary of parameters used during normalization,\n",
    "            to be used for denormalizing. Eg, mean, stddev, method, etc.\n",
    "\n",
    "    Returns:\n",
    "        data: Numpy array of denormalized data.\n",
    "    \"\"\"\n",
    "    eps = np.finfo('float32').eps\n",
    "    #for key, val in param_dict.items():\n",
    "    #    if K.is_tensor(val):\n",
    "    #        val = np.array(K.eval(val))\n",
    "    if param_dict['method'] == 'StandardScaler':\n",
    "        return data*np.maximum(param_dict['std'], eps) + param_dict['mean']\n",
    "    elif param_dict['method'] == 'MinMax':\n",
    "        return data*np.maximum((param_dict['armax']-param_dict['armin']), eps)\n",
    "        + param_dict['armin']\n",
    "    elif param_dict['method'] == 'MaxAbs':\n",
    "        return data*np.maximum(param_dict['maxabs'], eps)\n",
    "    elif param_dict['method'] == 'RobustScaler':\n",
    "        return data*np.maximum(param_dict['iqr'], eps) + param_dict['median']\n",
    "    elif param_dict['method'] == 'PowerTransform':\n",
    "        y = data*np.maximum(param_dict['std'], eps) + param_dict['mean']\n",
    "\n",
    "        def np_yeo_johnson_inverse_transform(x, lmbda):\n",
    "            \"\"\"Return inverse-transformed input x following Yeo-Johnson inverse\n",
    "            transform with parameter lambda. From Scipy\n",
    "            \"\"\"\n",
    "            x_inv = np.zeros_like(x)\n",
    "            pos = x >= 0\n",
    "            # when x >= 0\n",
    "            if np.abs(lmbda) < np.finfo(np.float32).eps:\n",
    "                x_inv[pos] = np.exp(x[pos]) - 1\n",
    "            else:  # lmbda != 0\n",
    "                x_inv[pos] = np.power(x[pos] * lmbda + 1, 1 / lmbda) - 1\n",
    "            # when x < 0\n",
    "            if np.abs(lmbda - 2) > np.finfo(np.float32).eps:\n",
    "                x_inv[~pos] = 1 - np.power(-(2 - lmbda) * x[~pos] + 1,\n",
    "                                           1 / (2 - lmbda))\n",
    "            else:  # lmbda == 2\n",
    "                x_inv[~pos] = 1 - np.exp(-x[~pos])\n",
    "            return x_inv\n",
    "        if param_dict['lambda'].size > 1:\n",
    "            for i, l in enumerate(param_dict['lambda']):\n",
    "                y[:, i] = np_yeo_johnson_inverse_transform(y[:, i], l)\n",
    "        else:\n",
    "            y = np_yeo_johnson_inverse_transform(\n",
    "                y.flatten(), param_dict['lambda']).reshape(y.shape)\n",
    "        return y\n",
    "    elif param_dict['method'] is None or param_dict['method'] == 'None':\n",
    "        return data\n",
    "    else:\n",
    "        raise ValueError(\"Unknown normalization method\")\n",
    "\n",
    "\n",
    "def denormalize(data, param_dict, verbose=1):\n",
    "    \"\"\"Denormalizes data after training\n",
    "\n",
    "    Args:\n",
    "        data: Numpy array or dictionary of numpy arrays.\n",
    "        param_dict (dict): Dictionary of parameters used during normalization,\n",
    "            to be used for denormalizing. Eg, mean, stddev, method, etc.\n",
    "        verbose (int): verbosity level. 0 is no CL output, 1 shows progress, 2 abbreviates.\n",
    "\n",
    "    Returns:\n",
    "        data: Numpy array or dictionary of numpy arrays. Denormalized data.\n",
    "    \"\"\"\n",
    "    if type(data) is dict:\n",
    "        data=data.copy() # don't make changes in place\n",
    "        for key in tqdm(data.keys(), desc='Denormalizing', ascii=True, dynamic_ncols=True,\n",
    "                        disable=not verbose==1):\n",
    "            if key not in ['time', 'shotnum']:\n",
    "                data[key] = denormalize_arr(data[key], param_dict[key])\n",
    "        return data\n",
    "    else:\n",
    "        return denormalize_arr(data, param_dict)\n",
    "\n",
    "\n",
    "def renormalize(data, param_dict, verbose=1):\n",
    "    \"\"\"Normalizes data using already determined parameters\n",
    "\n",
    "    Args:\n",
    "        data: Numpy array or dictionary of numpy arrays of raw data.\n",
    "        param_dict (dict): Dictionary of parameters used during normalization,\n",
    "            Eg, mean, stddev, method, etc.\n",
    "        verbose (int): verbosity level. 0 is no CL output, 1 shows progress, 2 abbreviates\n",
    "\n",
    "    Returns:\n",
    "        data: Numpy array or dictionary of numpy arrays. Normalized data.\n",
    "    \"\"\"\n",
    "    if type(data) is dict:\n",
    "        for key in tqdm(data.keys(), desc='Normalizing', ascii=True, dynamic_ncols=True,\n",
    "                        disable=not verbose==1):\n",
    "            if key not in ['time', 'shotnum']:\n",
    "                data[key] = renormalize(data[key], param_dict[key])\n",
    "        return data\n",
    "    else:\n",
    "        # first remove all inf/nan\n",
    "        data[np.isinf(data)] = np.nan\n",
    "        if data.ndim > 2:\n",
    "            for i in range(data.shape[2]):\n",
    "                data[np.isnan(data[:, :, i]), i] = param_dict['nanmean'][i]\n",
    "        else:\n",
    "            data[np.isnan(data)] = param_dict['nanmean']\n",
    "        # then normalize\n",
    "        if param_dict['method'] == 'StandardScaler':\n",
    "            return (data - param_dict['mean'])/np.maximum(\n",
    "                param_dict['std'], np.finfo(np.float32).eps)\n",
    "        elif param_dict['method'] == 'MinMax':\n",
    "            return (data - param_dict['armin'])/np.maximum(\n",
    "                (param_dict['armax']-param_dict['armin']), np.finfo(np.float32).eps)\n",
    "        elif param_dict['method'] == 'MaxAbs':\n",
    "            return data/np.maximum(param_dict['maxabs'], np.finfo(np.float32).eps)\n",
    "        elif param_dict['method'] == 'RobustScaler':\n",
    "            return (data - param_dict['median'])/np.maximum(\n",
    "                param_dict['iqr'], np.finfo(np.float32).eps)\n",
    "        elif param_dict['method'] == 'PowerTransform':\n",
    "            def yeo_johnson_transform(x, lmbda):\n",
    "                \"\"\"Return transformed input x following Yeo-Johnson transform with\n",
    "                parameter lambda.\n",
    "                \"\"\"\n",
    "                out = np.zeros_like(x)\n",
    "                pos = x >= 0  # binary mask\n",
    "                # when x >= 0\n",
    "                if abs(lmbda) < np.finfo(np.float32).eps:\n",
    "                    out[pos] = np.log1p(x[pos])\n",
    "                else:  # lmbda != 0\n",
    "                    out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda\n",
    "                # when x < 0\n",
    "                if abs(lmbda - 2) > np.finfo(np.float32).eps:\n",
    "                    out[~pos] = - \\\n",
    "                        (np.power(-x[~pos] + 1, 2 - lmbda) - 1) / (2 - lmbda)\n",
    "                else:  # lmbda == 2\n",
    "                    out[~pos] = -np.log1p(-x[~pos])\n",
    "                return out\n",
    "            y = data\n",
    "            if param_dict['lambda'].size > 1:\n",
    "                for i, l in enumerate(param_dict['lambda']):\n",
    "                    y[:, i] = yeo_johnson_transform(y[:, i], l)\n",
    "            else:\n",
    "                y = yeo_johnson_transform(\n",
    "                    y.flatten(), param_dict['lambda']).reshape(y.shape)\n",
    "            y = (y - param_dict['mean'])/np.maximum(\n",
    "                param_dict['std'], np.finfo(np.float32).eps)\n",
    "            return y\n",
    "        elif param_dict['method'] is None or param_dict['method'] == 'None':\n",
    "            return data\n",
    "        else:\n",
    "            raise ValueError(\"Unknown normalization method\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext heat\n",
    "%load_ext memory_profiler\n",
    "scenario['sig_names'] = scenario['profile_names'] + scenario['scalar_names'] + scenario['actuator_names']\n",
    "verbose=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/wconlin/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/wconlin/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/wconlin/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/wconlin/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/wconlin/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/wconlin/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from helpers.data_generator import process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Signals: curr, ffprime_EFIT02, pinj, press_EFIT02, q_EFIT02, thomson_dens_EFIT02, thomson_temp_EFIT02, tinj\n",
      "Number of useable shots:  36\n",
      "Number of shots used:  36\n",
      "Total number of timesteps:  11009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering: 100%|##########| 36/36 [00:00<00:00, 59.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shots with Complete NaN: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacking: 100%|##########| 21/21 [00:00<00:00, 21.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7769 samples total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing ECH\n",
      "Removed 5751 samples\n",
      "2018 samples remaining\n",
      "Removing weird gas\n",
      "Removing weird I-coils\n",
      "Removing NaN\n",
      "Removed 0 samples\n",
      "2018 samples remaining\n",
      "Removing dudtrip\n",
      "2018 samples remaining after pruning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing: 100%|##########| 21/21 [00:00<00:00, 223.63it/s]\n",
      "Splitting: 100%|##########| 21/21 [00:00<00:00, 7203.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples:  2018\n",
      "Number of training samples:  1614\n",
      "Number of validation samples:  404\n"
     ]
    }
   ],
   "source": [
    "traindata, valdata, normdict = process_data(scenario['raw_data_path'],scenario['sig_names'],scenario['normalization_method'],scenario['window_length'],scenario['window_overlap'],scenario['lookback'],scenario['lookahead'],scenario['sample_step'],scenario['uniform_normalization'],scenario['train_frac'],scenario['val_frac'],scenario['nshots'],verbose,scenario['flattop_only'],pruning_functions=scenario['pruning_functions'],excluded_shots=scenario['excluded_shots'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Signals: curr, ffprime_EFIT02, pinj, press_EFIT02, q_EFIT02, thomson_dens_EFIT02, thomson_temp_EFIT02, tinj\n",
      "Number of useable shots:  5740\n",
      "Number of shots used:  5740\n",
      "Total number of timesteps:  1517720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering: 100%|##########| 5740/5740 [00:28<00:00, 198.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shots with Complete NaN: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacking: 100%|##########| 21/21 [00:54<00:00,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1052920 samples total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing ECH\n",
      "Removed 399160 samples\n",
      "653760 samples remaining\n",
      "Removing weird gas\n",
      "Removed 36421 samples\n",
      "617339 samples remaining\n",
      "Removing weird I-coils\n",
      "Removed 118866 samples\n",
      "498473 samples remaining\n",
      "Removing NaN\n",
      "Removed 0 samples\n",
      "498473 samples remaining\n",
      "Removing dudtrip\n",
      "Removed 16602 samples\n",
      "481871 samples remaining\n",
      "481871 samples remaining after pruning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing: 100%|##########| 21/21 [00:38<00:00,  1.81s/it]\n",
      "Splitting: 100%|##########| 21/21 [00:01<00:00, 11.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples:  481871\n",
      "Number of training samples:  385496\n",
      "Number of validation samples:  96375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "traindata, valdata, normdict = process_data(scenario['raw_data_path'],scenario['sig_names'],scenario['normalization_method'],scenario['window_length'],scenario['window_overlap'],scenario['lookback'],scenario['lookahead'],scenario['sample_step'],scenario['uniform_normalization'],scenario['train_frac'],scenario['val_frac'],scenario['nshots'],verbose,scenario['flattop_only'],pruning_functions=scenario['pruning_functions'],excluded_shots=scenario['excluded_shots'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96375, 4)\n",
      "float32\n",
      "(96375, 4)\n",
      "float32\n",
      "(96375, 4)\n",
      "float32\n",
      "(96375, 4)\n",
      "float32\n",
      "(96375, 4)\n",
      "float32\n",
      "(96375, 4)\n",
      "float32\n",
      "(96375, 4, 65)\n",
      "float32\n",
      "(96375, 4)\n",
      "float32\n",
      "(96375, 4)\n",
      "float32\n",
      "(96375, 4)\n",
      "float32\n",
      "(96375, 4)\n",
      "float32\n",
      "(96375, 4)\n",
      "float32\n",
      "(96375, 4)\n",
      "float32\n",
      "(96375, 4)\n",
      "float32\n",
      "(96375, 4, 65)\n",
      "float32\n",
      "(96375, 4, 65)\n",
      "float32\n",
      "(96375, 4)\n",
      "float32\n",
      "(96375, 4, 65)\n",
      "float32\n",
      "(96375, 4, 65)\n",
      "float32\n",
      "(96375, 4)\n",
      "float32\n",
      "(96375, 4)\n",
      "float32\n",
      "525.822\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for v in valdata.values():\n",
    "    s += v.size\n",
    "    print(v.shape)\n",
    "    print(v.dtype)\n",
    "print(s*4/10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.       , 1.3333334, 1.6666666], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([3,4,5],dtype='float32')/3).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Signals: curr, ffprime_EFIT02, pinj, press_EFIT02, q_EFIT02, thomson_dens_EFIT02, thomson_temp_EFIT02, tinj\n",
      "Number of useable shots:  5740\n",
      "Number of shots used:  5740\n",
      "Total number of timesteps:  1517720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gathering: 100%|##########| 5740/5740 [05:02<00:00, 18.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shots with Complete NaN: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacking: 100%|##########| 21/21 [03:20<00:00,  9.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1052920 samples total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing ECH\n",
      "Removed 399160 samples\n",
      "653760 samples remaining\n",
      "Removing weird gas\n",
      "Removed 36421 samples\n",
      "617339 samples remaining\n",
      "Removing weird I-coils\n",
      "Removed 118866 samples\n",
      "498473 samples remaining\n",
      "Removing NaN\n",
      "Removed 0 samples\n",
      "498473 samples remaining\n",
      "Removing dudtrip\n",
      "Removed 16602 samples\n",
      "481871 samples remaining\n",
      "481871 samples remaining after pruning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing: 100%|##########| 21/21 [00:36<00:00,  1.72s/it]\n",
      "Splitting: 100%|##########| 21/21 [00:01<00:00, 11.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples:  481871\n",
      "Number of training samples:  385496\n",
      "Number of validation samples:  96375\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Filename: /home/wconlin/plasma-profile-predictor/helpers/data_generator.py\n",
       "\n",
       "Line #    Mem usage    Increment   Line Contents\n",
       "================================================\n",
       "   333    305.8 MiB    305.8 MiB   def process_data(rawdata, sig_names, normalization_method, window_length=1,\n",
       "   334                                              window_overlap=0, lookbacks={}, lookahead=3, sample_step=5,\n",
       "   335                                              uniform_normalization=True, train_frac=0.7, val_frac=0.2,\n",
       "   336                                              nshots=None,\n",
       "   337                                              verbose=1, flattop_only=True, randomize=True, **kwargs):\n",
       "   338                                 \"\"\"Organize data into correct format for training\n",
       "   339                             \n",
       "   340                                 Gathers raw data into bins, group into training sequences, normalize,\n",
       "   341                                 and split into training and validation sets.\n",
       "   342                             \n",
       "   343                                 Args:\n",
       "   344                                     rawdata (dict): Nested dictionary of raw signal data, or path to pickle.\n",
       "   345                                         Should be of the form rawdata[shot][signal_name] = signal_data.\n",
       "   346                                     sig_names (list): List of signal names as strings.\n",
       "   347                                     normalization_method (str): One of `StandardScaler`, `MinMax`, `MaxAbs`,\n",
       "   348                                         `RobustScaler`, `PowerTransform`.\n",
       "   349                                     window_length (int): Number of samples to average over in each bin/window.\n",
       "   350                                     window_overlap (int): How many timesteps to overlap windows.\n",
       "   351                                     lookbacks (dict of int): How many window lengths for lookback for each sig.\n",
       "   352                                     lookahead (int): How many window lengths to predict into the future.\n",
       "   353                                     sample_step (int): How much to offset sequential training sequences.\n",
       "   354                                         Step of 1 means sample[i] and sample[i+1] will be offset by 1, with\n",
       "   355                                         the rest overlapping.\n",
       "   356                                     uniform_normalization (bool): 'True' uses the same normalization\n",
       "   357                                         parameters over a whole profile, 'False' normalizes each spatial\n",
       "   358                                         point separately.\n",
       "   359                                     val_frac (float): Fraction of samples to use for validation.\n",
       "   360                                     nshots (int): How many shots to use. If None, all available will be used.\n",
       "   361                                     verbose (int): verbosity level. 0 is no CL output, 1 shows progress, 2 is abbreviated.\n",
       "   362                                     flattop_only (bool): Whether to only include data from flattop.\n",
       "   363                             \n",
       "   364                                 Returns:\n",
       "   365                                     traindata (dict): Dictionary of numpy arrays, one entry for each signal.\n",
       "   366                                         Each array has shape [nsamples,lookback+lookahead,signal_shape]\n",
       "   367                                     valdata (dict): Dictionary of numpy arrays, one entry for each signal.\n",
       "   368                                         Each array has shape [nsamples,lookback+lookahead,signal_shape]\n",
       "   369                                     param_dict (dict): Dictionary of parameters used during normalization,\n",
       "   370                                         to be used for denormalizing later. Eg, mean, stddev, method, etc.\n",
       "   371                                 \"\"\"\n",
       "   372                                 ##############################\n",
       "   373                                 # Load data\n",
       "   374                                 ##############################\n",
       "   375    305.8 MiB      0.0 MiB       if type(rawdata) is not dict:\n",
       "   376    305.8 MiB      0.0 MiB           if verbose:\n",
       "   377    305.8 MiB      0.0 MiB               print('Loading')\n",
       "   378    305.8 MiB      0.0 MiB           abs_path = Path(rawdata).resolve()\n",
       "   379    305.8 MiB      0.0 MiB           if abs_path.exists():\n",
       "   380    305.8 MiB      0.0 MiB               with open(abs_path, 'rb') as f:\n",
       "   381  30371.9 MiB  30066.2 MiB                   rawdata = pickle.load(f, encoding='latin1')\n",
       "   382                                     else:\n",
       "   383                                         print(abs_path)\n",
       "   384                                         raise IOError(\"No such path to data file\")\n",
       "   385                                         \n",
       "   386                                 ##############################\n",
       "   387                                 # get pruning functions\n",
       "   388                                 ##############################\n",
       "   389  30371.9 MiB      0.0 MiB       pruning_functions = copy.copy(kwargs.get('pruning_functions', []))\n",
       "   390  30371.9 MiB      0.0 MiB       if 'ech' not in sig_names:\n",
       "   391  30371.9 MiB      0.0 MiB           pruning_functions.append('remove_ECH')\n",
       "   392  30371.9 MiB      0.0 MiB       if not {'gasB', 'gasC', 'gasD', 'gasE'}.issubset(set(sig_names)):\n",
       "   393  30371.9 MiB      0.0 MiB           pruning_functions.append('remove_gas')\n",
       "   394  30371.9 MiB      0.0 MiB       prun_dict = {'remove_nan': remove_nan,\n",
       "   395  30371.9 MiB      0.0 MiB                    'remove_ECH': remove_ECH,\n",
       "   396  30371.9 MiB      0.0 MiB                    'remove_I_coil': remove_I_coil,\n",
       "   397  30371.9 MiB      0.0 MiB                    'remove_gas': remove_gas,\n",
       "   398  30371.9 MiB      0.0 MiB                    'remove_dudtrip': remove_dudtrip}\n",
       "   399  30371.9 MiB      0.0 MiB       for i, elem in enumerate(pruning_functions):\n",
       "   400  30371.9 MiB      0.0 MiB           if isinstance(elem, str):\n",
       "   401  30371.9 MiB      0.0 MiB               pruning_functions[i] = prun_dict[elem]\n",
       "   402                             \n",
       "   403                                 ##############################\n",
       "   404                                 # get excluded shots\n",
       "   405                                 ##############################\n",
       "   406  30371.9 MiB      0.0 MiB       excluded_shots = copy.copy(kwargs.get('excluded_shots', []))\n",
       "   407  30371.9 MiB      0.0 MiB       exclude_dict = {'topology_TOP': exclude_shots.topology_TOP,\n",
       "   408  30371.9 MiB      0.0 MiB                       'topology_SNT': exclude_shots.topology_SNT,\n",
       "   409  30371.9 MiB      0.0 MiB                       'topology_SNB': exclude_shots.topology_SNB,\n",
       "   410  30371.9 MiB      0.0 MiB                       'topology_OUT': exclude_shots.topology_OUT,\n",
       "   411  30371.9 MiB      0.0 MiB                       'topology_MAR': exclude_shots.topology_MAR,\n",
       "   412  30371.9 MiB      0.0 MiB                       'topology_IN': exclude_shots.topology_IN,\n",
       "   413  30371.9 MiB      0.0 MiB                       'topology_DN': exclude_shots.topology_DN,\n",
       "   414  30371.9 MiB      0.0 MiB                       'topology_BOT': exclude_shots.topology_BOT}\n",
       "   415  30371.9 MiB      0.0 MiB       for i, elem in enumerate(excluded_shots):\n",
       "   416  30371.9 MiB      0.0 MiB           if isinstance(elem, str):\n",
       "   417  30371.9 MiB      0.0 MiB               excluded_shots[i] = exclude_dict[elem]\n",
       "   418  30371.9 MiB      0.0 MiB           if not isinstance(elem, list):\n",
       "   419  30371.9 MiB      0.0 MiB               excluded_shots[i] = [elem]\n",
       "   420  30371.9 MiB      0.0 MiB       excluded_shots = [item for sublist in excluded_shots for item in sublist]\n",
       "   421                             \n",
       "   422                                 ##############################\n",
       "   423                                 # get sig names\n",
       "   424                                 ##############################\n",
       "   425  30371.9 MiB      0.0 MiB       extra_sigs = ['time', 'shotnum']\n",
       "   426  30371.9 MiB      0.0 MiB       if remove_dudtrip in pruning_functions:\n",
       "   427  30371.9 MiB      0.0 MiB           extra_sigs += ['dud_trip']\n",
       "   428  30371.9 MiB      0.0 MiB       if remove_I_coil in pruning_functions:\n",
       "   429  30371.9 MiB      0.0 MiB           extra_sigs += ['bt', 'curr', 'C_coil_method', 'I_coil_method']\n",
       "   430  30371.9 MiB      0.0 MiB       if remove_gas in pruning_functions:\n",
       "   431  30371.9 MiB      0.0 MiB           extra_sigs += ['gasB', 'gasC', 'gasD', 'gasE', 'pfx1', 'pfx2']\n",
       "   432  30371.9 MiB      0.0 MiB       if remove_ECH in pruning_functions:\n",
       "   433  30371.9 MiB      0.0 MiB           extra_sigs += ['ech']\n",
       "   434  30371.9 MiB      0.0 MiB       sig_names = list(np.unique(sig_names))\n",
       "   435  30371.9 MiB      0.0 MiB       sigsplustime = list(np.unique(sig_names + extra_sigs))\n",
       "   436  30371.9 MiB      0.0 MiB       if verbose:\n",
       "   437  30371.9 MiB      0.0 MiB           print('Signals: ' + ', '.join(sig_names))\n",
       "   438                             \n",
       "   439                                 ##############################\n",
       "   440                                 # figure out lookbacks\n",
       "   441                                 ##############################\n",
       "   442  30371.9 MiB      0.0 MiB       if isinstance(lookbacks, int):\n",
       "   443  30371.9 MiB      0.0 MiB           max_lookback = lookbacks\n",
       "   444  30371.9 MiB      0.0 MiB           lookbacks = {sig: max_lookback for sig in sigsplustime}\n",
       "   445                                 else:\n",
       "   446                                     max_lookback = 0\n",
       "   447                                     for val in lookbacks.values():\n",
       "   448                                         if val > max_lookback:\n",
       "   449                                             max_lookback = val\n",
       "   450                                     for sig in sigsplustime:\n",
       "   451                                         if sig not in lookbacks.keys():\n",
       "   452                                             lookbacks[sig] = max_lookback\n",
       "   453                                         \n",
       "   454                                 ##############################\n",
       "   455                                 # find which shots have all the signals needed\n",
       "   456                                 ##############################\n",
       "   457  30371.9 MiB      0.0 MiB       usabledata = []\n",
       "   458  30371.9 MiB      0.0 MiB       shots_without_sigs = []\n",
       "   459  30371.9 MiB      0.0 MiB       shots_too_short = []\n",
       "   460  30371.9 MiB      0.0 MiB       shots_excluded = []\n",
       "   461  30371.9 MiB      0.0 MiB       all_shots = sorted(list(rawdata.keys()))\n",
       "   462  30388.5 MiB      0.0 MiB       for shot in all_shots:\n",
       "   463  30388.5 MiB      4.1 MiB           rawdata[shot]['shotnum'] = np.ones(rawdata[shot]['time'].shape[0])*shot\n",
       "   464  30388.5 MiB      1.2 MiB           if set(sigsplustime).issubset(set(rawdata[shot].keys())) \\\n",
       "   465  30388.5 MiB      0.0 MiB              and rawdata[shot]['time'].size > (max_lookback+lookahead)*(window_length-window_overlap) \\\n",
       "   466  30388.5 MiB      0.0 MiB              and shot not in excluded_shots:\n",
       "   467  30388.5 MiB      0.0 MiB               usabledata.append(rawdata[shot])\n",
       "   468  30388.5 MiB      0.0 MiB           if set(sigsplustime).issubset(set(rawdata[shot].keys())) \\\n",
       "   469  30388.5 MiB      0.0 MiB              and shot not in excluded_shots:\n",
       "   470                                         # shot is too short\n",
       "   471  30388.5 MiB      0.0 MiB               shots_too_short.append(shot)\n",
       "   472  30388.5 MiB      3.3 MiB           if rawdata[shot]['time'].size > (max_lookback+lookahead)*(window_length-window_overlap) \\\n",
       "   473  30388.5 MiB      0.0 MiB              and shot not in excluded_shots:\n",
       "   474                                         # shot missing sigs\n",
       "   475  30388.5 MiB      0.0 MiB               shots_without_sigs.append(shot)\n",
       "   476  30388.5 MiB      0.0 MiB           if set(sigsplustime).issubset(set(rawdata[shot].keys())) \\\n",
       "   477  30388.5 MiB      0.0 MiB              and rawdata[shot]['time'].size > (max_lookback+lookahead)*(window_length-window_overlap):\n",
       "   478  30388.5 MiB      0.0 MiB               shots_excluded.append(shot)\n",
       "   479                                     \n",
       "   480                                         \n",
       "   481  30388.6 MiB      0.1 MiB       usabledata = np.array(usabledata)\n",
       "   482  30388.6 MiB      0.0 MiB       if len(usabledata) == 0:\n",
       "   483                                     s = 'No valid shots \\n'\n",
       "   484                                     s += 'Num Total: {}\\n'.format(len(all_shots))\n",
       "   485                                     s += 'Num without sigs: {}\\n'.format(len(shots_without_sigs))\n",
       "   486                                     s += 'Num too short: {}\\n'.format(len(shots_too_short))\n",
       "   487                                     s += 'Num excluded: {}\\n'.format(len(shots_excluded))   \n",
       "   488                                     if len(shots_without_sigs) == len(all_shots):\n",
       "   489                                         missing_sigs = []\n",
       "   490                                         for shot in all_shots:\n",
       "   491                                             missing_sigs.append(set(sigsplustime).difference(set(rawdata[shot].keys())))\n",
       "   492                                         missing_sigs = missing_sigs[0].intersection(*missing_sigs[1:])\n",
       "   493                                         s += 'Missing sigs: ' + str(missing_sigs)\n",
       "   494                                     raise ValueError(s)\n",
       "   495                                     \n",
       "   496  30372.6 MiB      0.0 MiB       del rawdata\n",
       "   497  30372.4 MiB      0.0 MiB       gc.collect()\n",
       "   498  30372.4 MiB      0.0 MiB       if nshots is not None:\n",
       "   499  30372.4 MiB      0.0 MiB           nshots = np.minimum(nshots, len(usabledata))\n",
       "   500  30372.4 MiB      0.0 MiB           usabledata = usabledata[:nshots]\n",
       "   501                                 else:\n",
       "   502                                     nshots = len(usabledata)\n",
       "   503  30372.4 MiB      0.0 MiB       if verbose:\n",
       "   504  30372.4 MiB      0.0 MiB           print('Number of useable shots: ', str(len(usabledata)))\n",
       "   505  30372.4 MiB      0.0 MiB           print('Number of shots used: ', str(nshots))\n",
       "   506  30372.4 MiB      0.0 MiB           t = 0\n",
       "   507  30372.4 MiB      0.0 MiB           for shot in usabledata:\n",
       "   508  30372.4 MiB      0.1 MiB               t += shot['time'].size\n",
       "   509  30372.4 MiB      0.0 MiB           print('Total number of timesteps: ', str(t))\n",
       "   510  30372.4 MiB      0.0 MiB           sys.stdout.flush()\n",
       "   511                                     \n",
       "   512                                 ##############################\n",
       "   513                                 # some helper functions\n",
       "   514                                 ##############################          \n",
       "   515  32071.5 MiB      0.0 MiB       def moving_average(a, n):\n",
       "   516                                     \"\"\"moving average of array a with window size n\"\"\"\n",
       "   517  32071.5 MiB      1.4 MiB           ret = np.nancumsum(a, axis=0)\n",
       "   518  32071.5 MiB      1.0 MiB           ret[n:] = ret[n:] - ret[:-n]\n",
       "   519  32071.5 MiB      1.2 MiB           return (ret[n - 1:] / n).astype('float32')\n",
       "   520                             \n",
       "   521  32071.5 MiB      0.0 MiB       def is_valid(shot):\n",
       "   522                                     \"\"\"checks if a shot is completely NaN or if it never reached flattop\"\"\"\n",
       "   523  32071.5 MiB      0.0 MiB           for sig in sigsplustime:\n",
       "   524  32071.5 MiB      0.4 MiB               if np.isnan(shot[sig]).all():  # or np.isinf(shot[sig]).any():\n",
       "   525                                             return False\n",
       "   526  32071.5 MiB      0.0 MiB           if (flattop_only):\n",
       "   527  32071.5 MiB      0.8 MiB               if (shot['t_ip_flat'] == None or shot['ip_flat_duration'] == None):\n",
       "   528                                             return False\n",
       "   529  32071.5 MiB      0.0 MiB           return True\n",
       "   530                             \n",
       "   531  32071.5 MiB      0.6 MiB       def get_non_nan_inds(arr):\n",
       "   532                                     \"\"\"gets indices of array where value is not NaN\"\"\"\n",
       "   533  32071.5 MiB      0.0 MiB           if len(arr.shape) == 1:\n",
       "   534  32071.5 MiB      0.8 MiB               return np.where(~np.isnan(arr))[0]\n",
       "   535                                     else:\n",
       "   536  32071.5 MiB      0.2 MiB               return np.where(np.any(~np.isnan(arr), axis=1))[0]\n",
       "   537                             \n",
       "   538  32071.5 MiB      0.0 MiB       def get_first_index(shot):\n",
       "   539                                     \"\"\"gets index of first valid timeslice for a shot\"\"\"\n",
       "   540  32071.5 MiB      0.0 MiB           input_max = max([get_non_nan_inds(shot[sig])[0] +\n",
       "   541  32071.5 MiB      0.5 MiB                            lookbacks[sig] for sig in sig_names])\n",
       "   542  32071.5 MiB      0.0 MiB           output_max = max([get_non_nan_inds(shot[sig])[0] -\n",
       "   543  32071.5 MiB      0.0 MiB                             lookahead for sig in sig_names])\n",
       "   544  32071.5 MiB      0.0 MiB           if (flattop_only) and (shot['t_ip_flat'] != None):\n",
       "   545  32071.5 MiB      0.0 MiB               current_max = np.searchsorted(\n",
       "   546  32071.5 MiB      0.0 MiB                   shot['time'], shot['t_ip_flat'], side='left')\n",
       "   547  32071.5 MiB      0.0 MiB               return np.ceil(max(input_max, output_max, current_max)).astype(int)\n",
       "   548                                     else:\n",
       "   549                                         return np.ceil(max(input_max, output_max)).astype(int)\n",
       "   550                             \n",
       "   551  32071.5 MiB      0.0 MiB       def get_last_index(shot):\n",
       "   552                                     \"\"\"gets index of last valid timeslice for a shot\"\"\"\n",
       "   553  32071.5 MiB      0.0 MiB           partial_min = min([get_non_nan_inds(shot[sig])[-1]\n",
       "   554  32071.5 MiB      0.0 MiB                              for sig in sig_names])\n",
       "   555  32071.5 MiB      0.0 MiB           full_min = min([get_non_nan_inds(shot[sig])[-1] -\n",
       "   556  32071.5 MiB      0.4 MiB                           lookahead for sig in sig_names])\n",
       "   557  32071.5 MiB      0.0 MiB           if (flattop_only) and (shot['t_ip_flat'] != None) and (shot['ip_flat_duration'] != None):\n",
       "   558  32071.5 MiB      0.0 MiB               current_min = np.searchsorted(\n",
       "   559  32071.5 MiB      0.0 MiB                   shot['time'], shot['t_ip_flat']+shot['ip_flat_duration'], side='right')\n",
       "   560  32071.5 MiB      0.0 MiB               return np.floor(min(full_min, partial_min, current_min)).astype(int)\n",
       "   561                                     else:\n",
       "   562                                         return np.floor(min(full_min, partial_min)).astype(int)\n",
       "   563                                 \n",
       "   564  30372.4 MiB      0.0 MiB       @numba.njit\n",
       "   565                                 def group_data(array,first,last,sample_step,lookback, lookahead):\n",
       "   566                                     \"\"\"groups shot data into i/o chunks\"\"\"\n",
       "   567                                     data = []\n",
       "   568                                     for i in range(first,last,sample_step):\n",
       "   569                                         data.append(array[i-lookback:i+lookahead+1])\n",
       "   570                                     return data\n",
       "   571                                 \n",
       "   572                                 ##############################\n",
       "   573                                 # loop through shots and do stuff\n",
       "   574                                 ##############################\n",
       "   575  30372.4 MiB      0.0 MiB       alldata = {}\n",
       "   576  30372.4 MiB      0.0 MiB       shots_with_complete_nan = []\n",
       "   577  30372.4 MiB      0.0 MiB       for sig in sigsplustime:\n",
       "   578  30372.4 MiB      0.0 MiB           alldata[sig] = []  # initalize empty lists\n",
       "   579  30372.4 MiB      0.0 MiB       for shot in tqdm(usabledata, desc='Gathering', ascii=True, dynamic_ncols=True,\n",
       "   580  32071.6 MiB      1.1 MiB                        disable=not verbose == 1):\n",
       "   581                                     ##############################\n",
       "   582                                     # take moving average of data and bin it\n",
       "   583                                     ##############################\n",
       "   584  32070.6 MiB      0.1 MiB           binned_shot = {}\n",
       "   585  32071.5 MiB      0.4 MiB           for sig in sigsplustime:\n",
       "   586  32071.5 MiB      1.8 MiB               if np.any(np.isinf(shot[sig])):\n",
       "   587                                             shot[sig][np.isinf(shot[sig])] = np.nan            \n",
       "   588  32071.5 MiB      0.9 MiB               binned_shot[sig] = moving_average(shot[sig],window_length)[::window_length-window_overlap]\n",
       "   589  32071.5 MiB      0.0 MiB           binned_shot['t_ip_flat'] = shot['t_ip_flat'].astype('float32')\n",
       "   590  32071.5 MiB      0.0 MiB           binned_shot['ip_flat_duration'] = shot['ip_flat_duration'].astype('float32')\n",
       "   591  32071.5 MiB      0.4 MiB           if not is_valid(binned_shot):\n",
       "   592                                         shots_with_complete_nan.append(np.unique(shot[\"shotnum\"]))\n",
       "   593                                         continue\n",
       "   594                             \n",
       "   595                                     ##############################\n",
       "   596                                     # group into arrays of input/output pairs\n",
       "   597                                     ##############################\n",
       "   598  32071.5 MiB      0.0 MiB           first = get_first_index(binned_shot)\n",
       "   599  32071.5 MiB      0.0 MiB           last = get_last_index(binned_shot)\n",
       "   600  32071.5 MiB      1.1 MiB           for sig in sigsplustime:\n",
       "   601  32071.5 MiB      1.5 MiB               alldata[sig] += group_data(binned_shot[sig],first,last,sample_step,lookbacks[sig],lookahead)\n",
       "   602                             \n",
       "   603  32071.6 MiB      0.0 MiB       if verbose:\n",
       "   604  32071.6 MiB      0.0 MiB           print(\"Shots with Complete NaN: \" + ', '.join(str(e)\n",
       "   605  32071.6 MiB      0.0 MiB                                                         for e in shots_with_complete_nan))\n",
       "   606  32071.6 MiB      0.0 MiB       sys.stdout.flush()\n",
       "   607  32072.0 MiB      0.4 MiB       del usabledata\n",
       "   608  32072.0 MiB      0.0 MiB       gc.collect()\n",
       "   609                                 \n",
       "   610                                 ##############################\n",
       "   611                                 # stack data from all shots together\n",
       "   612                                 ##############################    \n",
       "   613  32072.0 MiB      0.0 MiB       for sig in tqdm(sigsplustime, desc='Stacking', ascii=True, dynamic_ncols=True,\n",
       "   614  36736.1 MiB      0.0 MiB                       disable=not verbose == 1):\n",
       "   615  36736.1 MiB   1043.6 MiB           alldata[sig] = np.stack(alldata[sig]).astype('float32')\n",
       "   616  36736.1 MiB      0.0 MiB           gc.collect()\n",
       "   617                             \n",
       "   618  35508.1 MiB      0.0 MiB       if verbose:\n",
       "   619  35508.1 MiB      0.0 MiB           print(\"{} samples total\".format(len(alldata['time'])))\n",
       "   620  35508.1 MiB      0.0 MiB       sys.stdout.flush()\n",
       "   621  35508.1 MiB      0.0 MiB       gc.collect()\n",
       "   622                             \n",
       "   623                                 ##############################\n",
       "   624                                 # apply pruning functions\n",
       "   625                                 ##############################\n",
       "   626                                 # call fns in the right order to speed things up\n",
       "   627  35508.1 MiB      0.0 MiB       if remove_ECH in pruning_functions:\n",
       "   628  33528.7 MiB      0.0 MiB           alldata = remove_ECH(alldata,verbose)\n",
       "   629  33528.7 MiB      0.0 MiB       if remove_gas in pruning_functions:\n",
       "   630  33348.1 MiB      0.0 MiB           alldata = remove_gas(alldata,verbose)\n",
       "   631  33348.1 MiB      0.0 MiB       if remove_I_coil in pruning_functions:\n",
       "   632  32761.8 MiB      0.0 MiB           alldata = remove_I_coil(alldata,verbose)\n",
       "   633  32761.8 MiB      0.0 MiB       if remove_nan in pruning_functions:\n",
       "   634  32761.8 MiB      0.0 MiB           alldata = remove_nan(alldata,verbose)\n",
       "   635  32761.8 MiB      0.0 MiB       if remove_dudtrip in pruning_functions:\n",
       "   636  32679.3 MiB      0.0 MiB           alldata = remove_dudtrip(alldata,verbose)\n",
       "   637                                 \n",
       "   638  32679.3 MiB      0.0 MiB       if verbose:\n",
       "   639  32679.3 MiB      0.0 MiB           print(\"{} samples remaining after pruning\".format(len(alldata['time'])))\n",
       "   640  32679.3 MiB      0.0 MiB       sys.stdout.flush()\n",
       "   641  32679.3 MiB      0.0 MiB       gc.collect()\n",
       "   642                             \n",
       "   643                                 ##############################\n",
       "   644                                 # normalize data\n",
       "   645                                 ##############################    \n",
       "   646  32679.3 MiB      0.0 MiB       alldata, normalization_params = normalize(\n",
       "   647  32679.4 MiB      0.1 MiB           alldata, normalization_method, uniform_normalization, verbose)\n",
       "   648  32679.4 MiB      0.0 MiB       gc.collect()\n",
       "   649                                 \n",
       "   650                                 ##############################\n",
       "   651                                 # split into train and validation sets\n",
       "   652                                 ##############################    \n",
       "   653  32679.4 MiB      0.0 MiB       nsamples = alldata['time'].shape[0]\n",
       "   654  32679.5 MiB      0.1 MiB       inds = np.random.permutation(nsamples) if randomize else np.arange(nsamples)\n",
       "   655  32679.5 MiB      0.0 MiB       traininds = inds[:int(nsamples*train_frac)]\n",
       "   656  32679.5 MiB      0.0 MiB       valinds = inds[int(nsamples*train_frac)\n",
       "   657  32679.5 MiB      0.0 MiB                          :int(nsamples*(val_frac+train_frac))]\n",
       "   658  32679.5 MiB      0.0 MiB       traindata = {}\n",
       "   659  32679.5 MiB      0.0 MiB       valdata = {}\n",
       "   660  32679.5 MiB      0.0 MiB       for sig in tqdm(sigsplustime, desc='Splitting', ascii=True, dynamic_ncols=True,\n",
       "   661  34591.4 MiB      0.0 MiB                       disable=not verbose == 1):\n",
       "   662  34591.4 MiB    382.2 MiB           traindata[sig] = alldata[sig][traininds]\n",
       "   663  34591.4 MiB      2.8 MiB           valdata[sig] = alldata[sig][valinds]\n",
       "   664  34591.4 MiB      0.0 MiB       time.sleep(0.1)\n",
       "   665  34591.4 MiB      0.0 MiB       if verbose:\n",
       "   666  34591.4 MiB      0.0 MiB           print('Total number of samples: ', str(nsamples))\n",
       "   667  34591.4 MiB      0.0 MiB           print('Number of training samples: ', str(traininds.size))\n",
       "   668  34591.4 MiB      0.0 MiB           print('Number of validation samples: ', str(valinds.size))\n",
       "   669  34591.4 MiB      0.0 MiB       return traindata, valdata, normalization_params"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%mprun -f process_data process_data(scenario['raw_data_path'],scenario['sig_names'],scenario['normalization_method'],scenario['window_length'],scenario['window_overlap'],scenario['lookback'],scenario['lookahead'],scenario['sample_step'],scenario['uniform_normalization'],scenario['train_frac'],scenario['val_frac'],scenario['nshots'],verbose,scenario['flattop_only'],pruning_functions=scenario['pruning_functions'],excluded_shots=scenario['excluded_shots'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(5)\n",
    "b = [False,True,True,False,False]\n",
    "a[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan,  1.,  2., nan])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [np.inf,1,2,np.nan]\n",
    "a = np.array(a)\n",
    "a[np.isinf(a)] = np.nan\n",
    "a[np.isinf(a)] = np.nan\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.random.random((5,6,7))>.5,axis=tuple(np.arange(1,3).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(np.arange(1,2).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
