{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "#IMPORTS\n",
    "%reset\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import copy \n",
    "from sklearn import decomposition\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "from helpers import helper_functions\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name+'.pkl', 'rb') as f:\n",
    "        return pickle.load(f, encoding='latin1')\n",
    "\n",
    "# Gaussian normalization, return 0 if std is 0\n",
    "def normalize(obj, mean, std):\n",
    "    a = obj-mean\n",
    "    b=std\n",
    "    return np.divide(a, b, out=np.zeros_like(a), where=b!=0)\n",
    "\n",
    "# load conf file, 'config'\n",
    "def load_config(config_file):\n",
    "    with open(config_file) as f:\n",
    "        config = yaml.load(f)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to load in the necessary data, first creating the data files with the necessary delay times\n",
    "def data_loader(data_dir):\n",
    "    \n",
    "#     print(\"Loading the model/conf file in from: \" + results_dir)\n",
    "    \n",
    "#     ################\n",
    "#     # Load in the conf file\n",
    "#     model_filename='model.h5'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     config=load_config(os.path.join(results_dir,'conf.yaml'))['data']\n",
    "#     config_entire = load_config(os.path.join(results_dir,'conf.yaml'))\n",
    "\n",
    "\n",
    "#     ###############\n",
    "#     #LOAD the model in\n",
    "\n",
    "    \n",
    "#     # load model, 'loaded_model'\n",
    "#     loaded_model=models.load_model(os.path.join(results_dir,model_filename))\n",
    "\n",
    "#     loaded_model.compile(optimizer=optimizers.RMSprop(lr=.001),\n",
    "#                          metrics=['mae'], loss='mse')\n",
    "#     print(\"This is what the conf file you gave me looks like: \")\n",
    "#     print(config)\n",
    "\n",
    "\n",
    "    #############\n",
    "#     # preprocess the data and save it based on the conf file\n",
    "#     helper_functions.preprocess_data(dirname=data_dir, \n",
    "#                                     sigs_0d=config['sigs_0d'],\n",
    "#                                     sigs_1d=config['sigs_1d'],\n",
    "#                                     sigs_predict=config['sigs_predict'],\n",
    "#                                     n_components=config['n_components'],\n",
    "#                                     avg_window=config['avg_window'],\n",
    "#                                     lookback=config['lookback'],\n",
    "#                                     delay=config['delay'],\n",
    "#                                     train_frac=config['train_frac'],\n",
    "#                                     val_frac=config['val_frac'],\n",
    "#                                     save_data=True, noised_signal = None)    \n",
    "    \n",
    "\n",
    "    print(\"Reading the newly processed data from: \" + data_dir)\n",
    "    \n",
    "    # load raw data, 'raw_data'\n",
    "    with open(os.path.join(data_dir,'final_data.pkl'), 'rb') as f: \n",
    "            raw_data=pickle.load(f, encoding='latin1')\n",
    "\n",
    "\n",
    "    # load processed train data, 'train_data'\n",
    "    with open(os.path.join(data_dir,'train_data.pkl'), 'rb') as f: \n",
    "            train_data=pickle.load(f, encoding='latin1')\n",
    "\n",
    "\n",
    "    # load processed val data, 'val_data'\n",
    "    with open(os.path.join(data_dir,'val_data.pkl'), 'rb') as f: \n",
    "            val_data=pickle.load(f, encoding='latin1')\n",
    "\n",
    "    # load processed train target, 'train_target'\n",
    "    with open(os.path.join(data_dir,'train_target.pkl'), 'rb') as f: \n",
    "            train_target=pickle.load(f, encoding='latin1')\n",
    "\n",
    "    # load processed val target, 'val_target'\n",
    "    with open(os.path.join(data_dir,'val_target.pkl'), 'rb') as f: \n",
    "            val_target=pickle.load(f, encoding='latin1')\n",
    "\n",
    "\n",
    "    # load processed shot indices, a dictionary, 'shot_dict'\n",
    "    with open(os.path.join(data_dir,'shot_indices.pkl'), 'rb') as f: \n",
    "            shot_dict=pickle.load(f, encoding='latin1')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # load process true train times, 'train_real_times'\n",
    "    with open(os.path.join(data_dir,'train_time.pkl'), 'rb') as f: \n",
    "            train_real_times=pickle.load(f, encoding='latin1')\n",
    "\n",
    "    # load process true val times, 'val_real_times'\n",
    "    with open(os.path.join(data_dir,'val_time.pkl'), 'rb') as f: \n",
    "            val_real_times=pickle.load(f, encoding='latin1')\n",
    "\n",
    "\n",
    "    # load standard rho points, 'rho_points'\n",
    "    with open(os.path.join(data_dir,'rho_standard.pkl'), 'rb') as f: \n",
    "            rho_points=pickle.load(f, encoding='latin1')\n",
    "\n",
    "\n",
    "    # load means and standard deviations of data\n",
    "    means = helper_functions.load_obj(data_dir+'/means')\n",
    "    stds = helper_functions.load_obj(data_dir+'/stds')\n",
    "\n",
    "\n",
    "    #################\n",
    "    #Data organization\n",
    "\n",
    "    #list of all the shots in sorted (depending) order based on number, 'train_shot_nums'\n",
    "    train_shot_nums = shot_dict['train_shot_names']\n",
    "\n",
    "    #list of all the shots in sorted (depending) order based on number\n",
    "\n",
    "    val_shot_nums = shot_dict['val_shot_names']\n",
    "\n",
    "    # list of the indices where each shot begins in the data, 'train_shot_indices'\n",
    "    train_shot_indices = shot_dict['train_shot_indices']\n",
    "\n",
    "    # list of the indices where each shot begins in the data, 'val_shot_indices'\n",
    "    val_shot_indices = shot_dict['val_shot_indices']\n",
    "\n",
    "\n",
    "    # list of 0D and 1D signals, 'sig_keys_0d' and 'sig_keys_1d'\n",
    "    sig_keys_0d = config['sigs_0d'] #['curr_target', 'pinj']\n",
    "    sig_keys_1d = config['sigs_1d'] #['e_temp']\n",
    "\n",
    "    #print(sig_keys_0d)\n",
    "    all_sigs = sig_keys_0d+sig_keys_1d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # delay and lookback values\n",
    "#     delay = config['delay']\n",
    "#     lookback = config['lookback']\n",
    "    \n",
    "    \n",
    "    \n",
    "    #compute baseline MAE for val\n",
    "    val_baseline_mae=abs(val_target)\n",
    "    val_avg_baseline_mae=np.mean(val_baseline_mae, axis=0)\n",
    "    val_std_baseline_mae=np.std(val_baseline_mae, axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #compute baseline MSE for val\n",
    "    val_baseline_mse=np.square(val_target)\n",
    "    val_avg_baseline_mse=np.mean(val_baseline_mse, axis=0)\n",
    "    \n",
    "    \n",
    "    #compute baseline MAE for train\n",
    "    train_baseline_mae=abs(train_target)\n",
    "    train_avg_baseline_mae=np.mean(train_baseline_mae, axis=0)\n",
    "    train_std_baseline_mae=np.std(train_baseline_mae, axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #compute baseline MSE for train\n",
    "    train_baseline_mse=np.square(train_target)\n",
    "    train_avg_baseline_mse=np.mean(train_baseline_mse, axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    output_dict = {\n",
    "    'raw_data' : raw_data,\n",
    "    'train_data' : train_data,\n",
    "    'val_data' : val_data,\n",
    "    'train_target' : train_target,\n",
    "    'val_target' : val_target,\n",
    "    'shot_dict' : shot_dict,\n",
    "    'loaded_model' : loaded_model,\n",
    "    'train_real_times' : train_real_times,\n",
    "    'val_real_times' : val_real_times,\n",
    "    'rho_points' : rho_points,\n",
    "    'means' : means,\n",
    "    'stds' : stds,\n",
    "    'train_shot_nums' : train_shot_nums,\n",
    "    'val_shot_nums' : val_shot_nums,\n",
    "    'train_shot_indices' : train_shot_indices,\n",
    "    'val_shot_indices' : val_shot_indices,\n",
    "    'sig_keys_0d' : sig_keys_0d,\n",
    "    'sig_keys_1d' : sig_keys_1d,\n",
    "    'all_sigs' : all_sigs,\n",
    "    'delay' : delay,\n",
    "    'lookback' : lookback,\n",
    "    'baselines' : {\n",
    "        'val_avg_baseline_mae': val_avg_baseline_mae,\n",
    "        'val_avg_baseline_mse': val_avg_baseline_mse,\n",
    "        'train_avg_baseline_mae': train_avg_baseline_mae,\n",
    "        'train_avg_baseline_mse': train_avg_baseline_mse,  \n",
    "        }\n",
    "  \n",
    "    \n",
    "    \n",
    "    }\n",
    "    \n",
    "    #sanity check that the data matches the delay:\n",
    "    print(\"What the timestep window should be: \" + str(delay+lookback+1))\n",
    "    print(\"What the timestep window is: \" + str(train_data.shape[1]))\n",
    "    if(delay+lookback+1 != train_data.shape[1]):\n",
    "        print(\"ABORT PROBLEM WITH THE DELAYS NOT MATCHING+++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "     #check how many timesteps there are   \n",
    "    print(train_target.shape[0])   \n",
    "    print(\"DONE=============================================================\")\n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='/global/homes/a/al34/new_data_files/'\n",
    "# load processed train data, 'train_data'\n",
    "with open(os.path.join(data_dir,'val_data.pkl'), 'rb') as f: \n",
    "        val_data=pickle.load(f, encoding='latin1')\n",
    "        \n",
    "        \n",
    "with open(os.path.join(data_dir,'val_target.pkl'), 'rb') as f: \n",
    "        val_target=pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 16, 67)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 65)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_target[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-3.39101466, -1.94930836, -1.23826798, ..., -0.90812404,\n",
       "         -0.5339706 , -0.18310244],\n",
       "        [-3.32364165, -1.25203185, -1.2524483 , ..., -0.80530204,\n",
       "         -0.30525659, -0.02134681],\n",
       "        [-3.26157671, -1.94930836, -1.06886462, ..., -1.01361335,\n",
       "         -0.47764129, -0.22642321],\n",
       "        ..., \n",
       "        [-2.504397  , -1.94930836,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-2.443075  , -1.94930836,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [-2.3750976 , -1.94930836,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.20840765,  0.20843489,  0.20848162,  0.20850593,  0.20846738,\n",
       "         0.20832822,  0.20804722,  0.20760371,  0.20694196,  0.20606234,\n",
       "         0.20490275,  0.20346545,  0.20170656,  0.19960787,  0.19715438,\n",
       "         0.19430371,  0.19107213,  0.18739457,  0.18330397,  0.17875601,\n",
       "         0.17376233,  0.16831048,  0.16239823,  0.15603843,  0.14922933,\n",
       "         0.14199533,  0.13435532,  0.12633575,  0.11797613,  0.10931933,\n",
       "         0.10040661,  0.09132636,  0.0820916 ,  0.07286793,  0.06364411,\n",
       "         0.05459198,  0.04576255,  0.03728091,  0.02929574,  0.02184746,\n",
       "         0.01521761,  0.00935695,  0.00460781,  0.00093816, -0.00140342,\n",
       "        -0.00232609, -0.00176093,  0.00048043,  0.00423972,  0.0097068 ,\n",
       "         0.0165194 ,  0.02454485,  0.03330231,  0.04224135,  0.05049569,\n",
       "         0.05725584,  0.06100049,  0.06118098,  0.05643725,  0.04723103,\n",
       "         0.03299822,  0.01284984, -0.01742873, -0.06424224, -0.05943314]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_target[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(data, size, stepsize=1, padded=False, axis=-1, copy=True):\n",
    "    \"\"\"\n",
    "    Calculate a sliding window over a signal\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy array\n",
    "        The array to be slided over.\n",
    "    size : int\n",
    "        The sliding window size\n",
    "    stepsize : int\n",
    "        The sliding window stepsize. Defaults to 1.\n",
    "    axis : int\n",
    "        The axis to slide over. Defaults to the last axis.\n",
    "    copy : bool\n",
    "        Return strided array as copy to avoid sideffects when manipulating the\n",
    "        output array.\n",
    "    Returns\n",
    "    -------\n",
    "    data : numpy array\n",
    "        A matrix where row in last dimension consists of one instance\n",
    "        of the sliding window.\n",
    "    Notes\n",
    "    -----\n",
    "    - Be wary of setting `copy` to `False` as undesired sideffects with the\n",
    "      output values may occurr.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> a = numpy.array([1, 2, 3, 4, 5])\n",
    "    >>> sliding_window(a, size=3)\n",
    "    array([[1, 2, 3],\n",
    "           [2, 3, 4],\n",
    "           [3, 4, 5]])\n",
    "    >>> sliding_window(a, size=3, stepsize=2)\n",
    "    array([[1, 2, 3],\n",
    "           [3, 4, 5]])\n",
    "    See Also\n",
    "    --------\n",
    "    pieces : Calculate number of pieces available by sliding\n",
    "    \"\"\"\n",
    "    if axis >= data.ndim:\n",
    "        raise ValueError(\n",
    "            \"Axis value out of range\"\n",
    "        )\n",
    "\n",
    "    if stepsize < 1:\n",
    "        raise ValueError(\n",
    "            \"Stepsize may not be zero or negative\"\n",
    "        )\n",
    "\n",
    "    if size > data.shape[axis]:\n",
    "        raise ValueError(\n",
    "            \"Sliding window size may not exceed size of selected axis\"\n",
    "        )\n",
    "\n",
    "    shape = list(data.shape)\n",
    "    shape[axis] = np.floor(data.shape[axis] / stepsize - size / stepsize + 1).astype(int)\n",
    "    shape.append(size)\n",
    "\n",
    "    strides = list(data.strides)\n",
    "    strides[axis] *= stepsize\n",
    "    strides.append(data.strides[axis])\n",
    "\n",
    "    strided = np.lib.stride_tricks.as_strided(\n",
    "        data, shape=shape, strides=strides\n",
    "    )\n",
    "\n",
    "    if copy:\n",
    "        return strided.copy()\n",
    "    else:\n",
    "        return strided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_version():\n",
    "    \n",
    "    a = np.arange(10)\n",
    "    return sliding_window(a, 3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [1, 2, 3],\n",
       "       [2, 3, 4],\n",
       "       [3, 4, 5],\n",
       "       [4, 5, 6],\n",
       "       [5, 6, 7],\n",
       "       [6, 7, 8],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def py_version():\n",
    "    a = np.arange(1, 9)\n",
    "    final = []\n",
    "    for i in a:\n",
    "        final.append([])\n",
    "        for x in range(i-1, i+2):\n",
    "            \n",
    "            final[-1].append(x)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2],\n",
       " [1, 2, 3],\n",
       " [2, 3, 4],\n",
       " [3, 4, 5],\n",
       " [4, 5, 6],\n",
       " [5, 6, 7],\n",
       " [6, 7, 8],\n",
       " [7, 8, 9]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
