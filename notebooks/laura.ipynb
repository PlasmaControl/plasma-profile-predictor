{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/lzfang/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lzfang/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lzfang/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lzfang/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lzfang/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lzfang/.conda/envs/tfgpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "#from keras import backend as K\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import helpers\n",
    "import helpers.data_generator\n",
    "from helpers.data_generator import process_data, DataGenerator\n",
    "from helpers.custom_losses import denorm_loss, hinge_mse_loss\n",
    "from helpers.custom_losses import percent_correct_sign, baseline_MAE\n",
    "# from models.LSTMConv2D import get_model_lstm_conv2d, get_model_simple_lstm\n",
    "# from models.LSTMConv2D import get_model_linear_systems, get_model_conv2d\n",
    "#from utils.callbacks import CyclicLR, TensorBoardWrapper\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from time import strftime, localtime\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline\n",
    "from helpers.normalization import normalize, denormalize, renormalize\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Signals: dens, idens, itemp, press_EFIT01, press_EFIT02, q_EFIT01, q_EFIT02, rotation, temp\n",
      "Number of useable shots:  4406\n",
      "Number of shots used:  4406\n",
      "Total number of timesteps:  470664\n",
      "Shots with Complete NaN: \n",
      "320950 samples total\n",
      "Removing ECH\n",
      "Removed 131668 samples\n",
      "189282 samples remaining\n",
      "Removing timesteps WITHOUT gas feedback\n",
      "Removed 35361 samples\n",
      "153921 samples remaining\n",
      "Removing weird I-coils\n",
      "Removed 33051 samples\n",
      "120870 samples remaining\n",
      "Removing NaN\n",
      "Removed 0 samples\n",
      "120870 samples remaining\n",
      "Removing dudtrip\n",
      "Removed 5539 samples\n",
      "115331 samples remaining\n",
      "Removing jagged q profiles\n",
      "Removed 82 samples\n",
      "115249 samples remaining\n",
      "Removing zero q profiles\n",
      "Removed 8126 samples\n",
      "107123 samples remaining\n",
      "Removing outlier pressure profiles\n",
      "Removed 46 samples\n",
      "107077 samples remaining\n",
      "Removing negative pressure profiles\n",
      "Removed 10909 samples\n",
      "96168 samples remaining\n",
      "Removing negative pressure profiles\n",
      "Removed 2551 samples\n",
      "93617 samples remaining\n",
      "93617 samples remaining after pruning\n",
      "Total number of samples:  93617\n",
      "Number of training samples:  84268\n",
      "Number of validation samples:  9349\n"
     ]
    }
   ],
   "source": [
    "# full_data_path = '/scratch/gpfs/jabbate/old_stuff/new_data/final_data.pkl'\n",
    "full_data_path = '/scratch/gpfs/jabbate/full_data/train_data.pkl'\n",
    "\n",
    "test_data_path = '/scratch/gpfs/jabbate/full_data/test_data.pkl' \n",
    "# profiles = ['thomson_temp_EFITRT1','thomson_dens_EFITRT1','temp','dens']\n",
    "profiles = ['temp','dens','rotation','itemp','idens','press_EFIT01','q_EFIT01','press_EFIT02','q_EFIT02',]\n",
    "# for profile in tqdm(profiles):\n",
    "traindata, valdata, normalization_dict = helpers.data_generator.process_data(full_data_path,\n",
    "                                                      profiles,\n",
    "                                                      normalization_method='RobustScaler',\n",
    "                                                      window_length=1,\n",
    "                                                      window_overlap=0,\n",
    "                                                      lookbacks=0,\n",
    "                                                      lookahead=4,\n",
    "                                                      sample_step=1,\n",
    "                                                      uniform_normalization=True,\n",
    "                                                      train_frac=1,\n",
    "                                                      val_frac=0,\n",
    "                                                      nshots=10000,\n",
    "                                                      verbose=2,\n",
    "                                                      flattop_only=True,\n",
    "                                                      randomize=False,\n",
    "                                                      pruning_functions=['remove_nan',\n",
    "                                                                         'remove_dudtrip',\n",
    "                                                                         'remove_I_coil',\n",
    "                                                                         'remove_non_gas_feedback',\n",
    "                                                                         'remove_ECH','remove_outliers'],\n",
    "                                                      excluded_shots=['topology_TOP', \n",
    "                                                                      'topology_OUT',\n",
    "                                                                      'topology_MAR',\n",
    "                                                                      'topology_IN',\n",
    "                                                                      'topology_DN',\n",
    "                                                                      'topology_BOT'],\n",
    "                                                      delta_sigs=[],\n",
    "                                                      invert_q=True,\n",
    "                                                      val_idx = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Denormalizing: 100%|##########| 18/18 [00:00<00:00, 32.05it/s]\n"
     ]
    }
   ],
   "source": [
    "traindata = denormalize(traindata,normalization_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(traindata,\n",
    "                                1,\n",
    "                                profiles,\n",
    "                                [],\n",
    "                                profiles,\n",
    "                                [],\n",
    "                                {profile:0 for profile in profiles},\n",
    "                                4,\n",
    "                                True,\n",
    "                                2,\n",
    "                                False,\n",
    "                                sample_weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Signals: curr_target, dens, density_estimate, kappa_EFIT01, li_EFIT01, pinj, press_EFIT01, q_EFIT01, rotation, target_density, temp, tinj, triangularity_bot_EFIT01, triangularity_top_EFIT01, volume_EFIT01\n",
      "Number of useable shots:  1116\n",
      "Number of shots used:  1116\n",
      "Total number of timesteps:  124182\n",
      "Shots with Complete NaN: \n",
      "85701 samples total\n",
      "Removing ECH\n",
      "Removed 46375 samples\n",
      "39326 samples remaining\n",
      "Removing timesteps WITHOUT gas feedback\n",
      "Removed 5176 samples\n",
      "34150 samples remaining\n",
      "Removing weird I-coils\n",
      "Removed 7495 samples\n",
      "26655 samples remaining\n",
      "Removing NaN\n",
      "Removed 0 samples\n",
      "26655 samples remaining\n",
      "Removing dudtrip\n",
      "Removed 620 samples\n",
      "26035 samples remaining\n",
      "26035 samples remaining after pruning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Denormalizing: 100%|##########| 24/24 [00:00<00:00, 277.67it/s]\n",
      "Normalizing:  46%|####5     | 11/24 [00:00<00:00, 109.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples:  26035\n",
      "Number of training samples:  23469\n",
      "Number of validation samples:  2566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing: 100%|##########| 24/24 [00:00<00:00, 53.25it/s] \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model: model-conv2d_profiles-dens-temp-q_EFIT01-rotation-press_EFIT01_act-target_density-pinj-tinj-curr_target_15Mar20-03-48_Scenario-265.h5\n",
      "loaded dict: model-conv2d_profiles-dens-temp-q_EFIT01-rotation-press_EFIT01_act-target_density-pinj-tinj-curr_target_15Mar20-03-48_Scenario-265_params.pkl\n",
      "with parameters: dict_keys(['model_type', 'model_kwargs', 'input_profile_names', 'target_profile_names', 'sample_weightin', 'scalar_input_names', 'std_activation', 'batch_size', 'process_data', 'predict_deltas', 'epochs', 'loss_function', 'actuator_names', 'profile_downsample', 'sample_weighting', 'loss_function_kwargs', 'flattop_only', 'raw_data_path', 'invert_q', 'processed_filename_base', 'optimizer', 'optimizer_kwargs', 'shuffle_generators', 'pruning_functions', 'normalization_method', 'window_length', 'window_overlap', 'profile_lookback', 'actuator_lookback', 'lookahead', 'sample_step', 'uniform_normalization', 'train_frac', 'val_frac', 'val_idx', 'nshots', 'excluded_shots', 'lookbacks', 'sig_names', 'dt', 'normalization_dict', 'profile_length', 'date', 'runname', 'steps_per_epoch', 'val_steps', 'model_path', 'history', 'history_params', 'evaluation_metrics'])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"THIS IS THE ONE FROM THE PAPER AS OF 3/5\"\"\"\n",
    "file_path='/home/jabbate/run_results_12_11/model-conv2d_profiles-dens-temp-q_EFIT01-rotation-press_EFIT01_act-curr_target-pinj-tinj-target_density-bt_targ-dens-temp-q_EFIT01-rotation-press_EFIT01_profLB-0_actLB-6_ftop-True_11Dec19-18-30_Scenario-12.h5'\n",
    "\n",
    "base_path = os.path.expanduser('/projects/EKOLEMEN/profile_predictor/run_results_03_10/')\n",
    "\"\"\"good but overfit\"\"\"\n",
    "#model_path = 'model-conv2d_profiles-dens-temp-q_EFIT01-rotation-press_EFIT01_act-target_density-pinj-tinj-curr_target_01Mar20-19-50_Scenario-93.h5'\n",
    "\"\"\"good val but looks bad?\"\"\"\n",
    "# model_path = 'model-conv2d_profiles-dens-temp-q_EFIT01-rotation-press_EFIT01_act-target_density-pinj-tinj-curr_target_01Mar20-02-31_Scenario-71.h5'\n",
    "\n",
    "model_path = 'model-conv2d_profiles-dens-temp-q_EFIT01-rotation-press_EFIT01_act-target_density-pinj-tinj-curr_target_15Mar20-03-48_Scenario-265.h5'\n",
    "file_path = base_path + model_path\n",
    "\n",
    "\n",
    "model = keras.models.load_model(file_path, compile=False)\n",
    "print('loaded model: ' + file_path.split('/')[-1])\n",
    "file_path = file_path[:-3] + '_params.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    scenario = pickle.load(f, encoding='latin1')\n",
    "print('loaded dict: ' + file_path.split('/')[-1])\n",
    "print('with parameters: ' + str(scenario.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Signals: curr_target, dens, density_estimate, kappa_EFIT01, li_EFIT01, pinj, press_EFIT01, q_EFIT01, rotation, target_density, temp, tinj, triangularity_bot_EFIT01, triangularity_top_EFIT01, volume_EFIT01\n",
      "Number of useable shots:  1116\n",
      "Number of shots used:  1116\n",
      "Total number of timesteps:  124182\n",
      "Shots with Complete NaN: \n",
      "85701 samples total\n",
      "Removing ECH\n",
      "Removed 46375 samples\n",
      "39326 samples remaining\n",
      "Removing timesteps WITHOUT gas feedback\n",
      "Removed 5176 samples\n",
      "34150 samples remaining\n",
      "Removing weird I-coils\n",
      "Removed 7495 samples\n",
      "26655 samples remaining\n",
      "Removing NaN\n",
      "Removed 0 samples\n",
      "26655 samples remaining\n",
      "Removing dudtrip\n",
      "Removed 620 samples\n",
      "26035 samples remaining\n",
      "26035 samples remaining after pruning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Denormalizing: 100%|##########| 24/24 [00:00<00:00, 279.53it/s]\n",
      "Normalizing:   0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples:  26035\n",
      "Number of training samples:  23469\n",
      "Number of validation samples:  2566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing: 100%|##########| 24/24 [00:00<00:00, 54.81it/s]\n"
     ]
    }
   ],
   "source": [
    "orig_data_path = '/scratch/gpfs/jabbate/full_data/train_data_full.pkl'\n",
    "test_data_path = '/scratch/gpfs/jabbate/full_data_with_error/test_data.pkl' \n",
    "testdata, valdata, test_normalization_dict = helpers.data_generator.process_data(test_data_path,\n",
    "                                                      scenario['sig_names'],\n",
    "                                                      scenario['normalization_method'],\n",
    "                                                      scenario['window_length'],\n",
    "                                                      scenario['window_overlap'],\n",
    "                                                      scenario['lookbacks'],\n",
    "                                                      scenario['lookahead'],\n",
    "                                                      scenario['sample_step'],\n",
    "                                                      scenario['uniform_normalization'],\n",
    "                                                      1, #scenario['train_frac'],\n",
    "                                                      0, #scenario['val_frac'],\n",
    "                                                      scenario['nshots'],\n",
    "                                                      2, #scenario['verbose']\n",
    "                                                      scenario['flattop_only'],\n",
    "                                                      randomize=False,\n",
    "                                                      pruning_functions=scenario['pruning_functions'],\n",
    "                                                      excluded_shots = scenario['excluded_shots'],\n",
    "                                                      delta_sigs = [],\n",
    "                                                      invert_q=scenario.setdefault('invert_q',False),\n",
    "                                                      val_idx = 1)\n",
    "\n",
    "testdata = helpers.normalization.renormalize(helpers.normalization.denormalize(testdata.copy(),test_normalization_dict),scenario['normalization_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Signals: curr_target, dens, density_estimate, kappa_EFIT01, li_EFIT01, pinj, press_EFIT01, q_EFIT01, rotation, target_density, temp, tinj, triangularity_bot_EFIT01, triangularity_top_EFIT01, volume_EFIT01\n",
      "Number of useable shots:  1116\n",
      "Number of shots used:  1116\n",
      "Total number of timesteps:  124182\n",
      "Shots with Complete NaN: \n",
      "85701 samples total\n",
      "Removing ECH\n",
      "Removed 46375 samples\n",
      "39326 samples remaining\n",
      "Removing timesteps WITHOUT gas feedback\n",
      "Removed 5176 samples\n",
      "34150 samples remaining\n",
      "Removing weird I-coils\n",
      "Removed 7495 samples\n",
      "26655 samples remaining\n",
      "Removing NaN\n",
      "Removed 0 samples\n",
      "26655 samples remaining\n",
      "Removing dudtrip\n",
      "Removed 620 samples\n",
      "26035 samples remaining\n",
      "26035 samples remaining after pruning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Denormalizing: 100%|##########| 24/24 [00:00<00:00, 277.98it/s]\n",
      "Normalizing:   0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples:  26035\n",
      "Number of training samples:  23469\n",
      "Number of validation samples:  2566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing: 100%|##########| 24/24 [00:00<00:00, 56.49it/s]\n"
     ]
    }
   ],
   "source": [
    "orig_data_path = '/scratch/gpfs/jabbate/full_data/train_data_full.pkl'\n",
    "test_data_path = '/scratch/gpfs/jabbate/full_data_with_error/test_data.pkl' \n",
    "testdata, valdata, test_normalization_dict = helpers.data_generator.process_data(test_data_path,\n",
    "                                                      scenario['sig_names'],\n",
    "                                                      scenario['normalization_method'],\n",
    "                                                      scenario['window_length'],\n",
    "                                                      scenario['window_overlap'],\n",
    "                                                      scenario['lookbacks'],\n",
    "                                                      scenario['lookahead'],\n",
    "                                                      scenario['sample_step'],\n",
    "                                                      scenario['uniform_normalization'],\n",
    "                                                      1, #scenario['train_frac'],\n",
    "                                                      0, #scenario['val_frac'],\n",
    "                                                      scenario['nshots'],\n",
    "                                                      2, #scenario['verbose']\n",
    "                                                      scenario['flattop_only'],\n",
    "                                                      randomize=False,\n",
    "                                                      pruning_functions=scenario['pruning_functions'],\n",
    "                                                      excluded_shots = scenario['excluded_shots'],\n",
    "                                                      delta_sigs = [],\n",
    "                                                      invert_q=scenario.setdefault('invert_q',False),\n",
    "                                                      val_idx = 1)\n",
    "\n",
    "testdata = helpers.normalization.renormalize(helpers.normalization.denormalize(testdata.copy(),test_normalization_dict),scenario['normalization_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindatafull = copy.deepcopy(traindata)\n",
    "testdata = copy.deepcopy(traindata)\n",
    "testdatafull = copy.deepcopy(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindeltas = {sig:traindatafull[sig][:,-1,::2] - traindatafull[sig][:,0,::2] for sig in scenario['target_profile_names']}\n",
    "traindata = {sig:traindatafull[sig][:,-1,::2] for sig in scenario['target_profile_names']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdeltas = {sig:testdatafull[sig][:,-1,::2] - testdatafull[sig][:,0,::2] for sig in scenario['target_profile_names']}\n",
    "testdata = {sig:testdatafull[sig][:,-1,::2] for sig in scenario['target_profile_names']}\n",
    "nbins=4\n",
    "n = int(len(traindata['temp'])/nbins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata = [{sig:traindata[sig][i*n:(i+1)*n] for sig in scenario['target_profile_names']} for i in range(nbins)]\n",
    "subdata.insert(0,{sig:testdata[sig] for sig in scenario['target_profile_names']})\n",
    "\n",
    "subdelta = [{sig:traindeltas[sig][i*n:(i+1)*n] for sig in scenario['target_profile_names']} for i in range(nbins)]\n",
    "subdelta.insert(0,{sig:testdeltas[sig] for sig in scenario['target_profile_names']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalarize_median(arr, **kwargs):\n",
    "    return np.median(arr,axis=-1)\n",
    "\n",
    "def scalarize_iqr(arr, **kwargs):\n",
    "    return np.percentile(arr,75,axis=-1) - np.percentile(arr,25,axis=-1)\n",
    "\n",
    "def scalarize_mean(arr, **kwargs):\n",
    "    return np.mean(arr,axis=-1)\n",
    "\n",
    "def scalarize_std(arr, **kwargs):\n",
    "    return np.std(arr,axis=-1)\n",
    "\n",
    "def scalarize_pca_1(arr, **kwargs):\n",
    "    fitter = kwargs.get('fitter')\n",
    "    ret = fitter.transform(arr).squeeze()[:,0]\n",
    "    return ret\n",
    "\n",
    "def scalarize_pca_2(arr, **kwargs):\n",
    "    fitter = kwargs.get('fitter')\n",
    "    ret = fitter.transform(arr).squeeze()[:,1]\n",
    "    return ret\n",
    "\n",
    "def scalarize_pca_3(arr, **kwargs):\n",
    "    fitter = kwargs.get('fitter')\n",
    "    ret = fitter.transform(arr).squeeze()[:,2]\n",
    "    return ret\n",
    "\n",
    "def scalarize_pca_4(arr, **kwargs):\n",
    "    fitter = kwargs.get('fitter')\n",
    "    ret = fitter.transform(arr).squeeze()[:,3]\n",
    "    return ret\n",
    "\n",
    "def scalarize_pca_5(arr, **kwargs):\n",
    "    fitter = kwargs.get('fitter')\n",
    "    ret = fitter.transform(arr).squeeze()[:,4]\n",
    "    return ret\n",
    "\n",
    "def scalarize_pca_6(arr, **kwargs):\n",
    "    fitter = kwargs.get('fitter')\n",
    "    ret = fitter.transform(arr).squeeze()[:,5]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/lzfang/plasma-profile-predictor/pca_fitters.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3ee8a0e00859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/plasma-profile-predictor/pca_fitters.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mfull_pca_fitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/plasma-profile-predictor/pca_kernels.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mkernels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/lzfang/plasma-profile-predictor/pca_fitters.pkl'"
     ]
    }
   ],
   "source": [
    "with open(os.path.expanduser('~/plasma-profile-predictor/pca_fitters.pkl'),'rb') as f:\n",
    "    full_pca_fitter = pickle.load(f, encoding='latin1')\n",
    "with open(os.path.expanduser('~/plasma-profile-predictor/pca_kernels.pkl'),'rb') as f:\n",
    "    kernels = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_pca_fitter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e62e6ec57ae0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m scalarizeddata = [{sig:{name: scalarize_fun(subdata[i][sig], fitter=full_pca_fitter[sig]) for name, scalarize_fun in zip(scalarize_function_names, scalarize_functions)}\n\u001b[0;32m----> 7\u001b[0;31m                for sig in profiles} for i in range(len(subdata))]\n\u001b[0m\u001b[1;32m      8\u001b[0m scalarizeddelta = [{sig:{name: scalarize_fun(subdelta[i][sig], fitter=full_pca_fitter[sig]) for name, scalarize_fun in zip(scalarize_function_names, scalarize_functions)}\n\u001b[1;32m      9\u001b[0m                for sig in profiles} for i in range(len(subdelta))]\n",
      "\u001b[0;32m<ipython-input-22-e62e6ec57ae0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m scalarizeddata = [{sig:{name: scalarize_fun(subdata[i][sig], fitter=full_pca_fitter[sig]) for name, scalarize_fun in zip(scalarize_function_names, scalarize_functions)}\n\u001b[0;32m----> 7\u001b[0;31m                for sig in profiles} for i in range(len(subdata))]\n\u001b[0m\u001b[1;32m      8\u001b[0m scalarizeddelta = [{sig:{name: scalarize_fun(subdelta[i][sig], fitter=full_pca_fitter[sig]) for name, scalarize_fun in zip(scalarize_function_names, scalarize_functions)}\n\u001b[1;32m      9\u001b[0m                for sig in profiles} for i in range(len(subdelta))]\n",
      "\u001b[0;32m<ipython-input-22-e62e6ec57ae0>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m scalarizeddata = [{sig:{name: scalarize_fun(subdata[i][sig], fitter=full_pca_fitter[sig]) for name, scalarize_fun in zip(scalarize_function_names, scalarize_functions)}\n\u001b[0;32m----> 7\u001b[0;31m                for sig in profiles} for i in range(len(subdata))]\n\u001b[0m\u001b[1;32m      8\u001b[0m scalarizeddelta = [{sig:{name: scalarize_fun(subdelta[i][sig], fitter=full_pca_fitter[sig]) for name, scalarize_fun in zip(scalarize_function_names, scalarize_functions)}\n\u001b[1;32m      9\u001b[0m                for sig in profiles} for i in range(len(subdelta))]\n",
      "\u001b[0;32m<ipython-input-22-e62e6ec57ae0>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m scalarizeddata = [{sig:{name: scalarize_fun(subdata[i][sig], fitter=full_pca_fitter[sig]) for name, scalarize_fun in zip(scalarize_function_names, scalarize_functions)}\n\u001b[0m\u001b[1;32m      7\u001b[0m                for sig in profiles} for i in range(len(subdata))]\n\u001b[1;32m      8\u001b[0m scalarizeddelta = [{sig:{name: scalarize_fun(subdelta[i][sig], fitter=full_pca_fitter[sig]) for name, scalarize_fun in zip(scalarize_function_names, scalarize_functions)}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_pca_fitter' is not defined"
     ]
    }
   ],
   "source": [
    "scalarize_functions=[scalarize_mean, scalarize_std, scalarize_pca_1,scalarize_pca_2, scalarize_median, scalarize_iqr]\n",
    "scalarize_function_names=['mean','std','pca 1','pca 2','median','iqr']\n",
    "profiles = ['dens','temp','rotation','q_EFIT01','press_EFIT01']\n",
    "\n",
    "\n",
    "scalarizeddata = [{sig:{name: scalarize_fun(subdata[i][sig], fitter=full_pca_fitter[sig]) for name, scalarize_fun in zip(scalarize_function_names, scalarize_functions)}\n",
    "               for sig in profiles} for i in range(len(subdata))]\n",
    "scalarizeddelta = [{sig:{name: scalarize_fun(subdelta[i][sig], fitter=full_pca_fitter[sig]) for name, scalarize_fun in zip(scalarize_function_names, scalarize_functions)}\n",
    "               for sig in profiles} for i in range(len(subdelta))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.argwhere(scalarizeddelta[0]['dens']['iqr']>5)\n",
    "normalization_dict['dens']\n",
    "scalarize_iqr(subdelta[0]['dens'][1494])\n",
    "testdatafull['shotnum'][inds][:,0,-1]\n",
    "with open(orig_data_path,'rb') as f:\n",
    "    orig_data = pickle.load(f,encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = ['temp','dens','itemp','press_EFIT01','q_EFIT01']\n",
    "psi = np.linspace(0,1,33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(traindata,\n",
    "                                1, #scenario['batch_size'],\n",
    "                                scenario['input_profile_names'],\n",
    "                                scenario['actuator_names'],\n",
    "                                scenario['target_profile_names'],\n",
    "                                scenario['scalar_input_names'],\n",
    "                                scenario['lookbacks'],\n",
    "                                scenario['lookahead'],\n",
    "                                scenario['predict_deltas'],\n",
    "                                scenario['profile_downsample'],\n",
    "                                False,\n",
    "                                sample_weights = 'std') #scenario['shuffle_generators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_batch = DataGenerator(traindatafull,\n",
    "                                128, #scenario['batch_size'],\n",
    "                                scenario['input_profile_names'],\n",
    "                                scenario['actuator_names'],\n",
    "                                scenario['target_profile_names'],\n",
    "                                scenario['scalar_input_names'],\n",
    "                                scenario['lookbacks'],\n",
    "                                scenario['lookahead'],\n",
    "                                scenario['predict_deltas'],\n",
    "                                scenario['profile_downsample'],\n",
    "                                shuffle=False,\n",
    "                                sample_weights = None) #scenario['shuffle_generators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'target_density'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e83aa43b2f0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_generator_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscenario\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target_profile_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/plasma-profile-predictor/helpers/data_generator.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactuator_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             inp['input_past_' + sig] = self.data[sig][idx * self.batch_size:\n\u001b[0m\u001b[1;32m     95\u001b[0m                                                       \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                                                       0:self.lookbacks[sig]+1]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'target_density'"
     ]
    }
   ],
   "source": [
    "baseline = {sig:[] for sig in scenario['target_profile_names']}\n",
    "predictions = {sig:[] for sig in scenario['target_profile_names']}\n",
    "for i in range(len(train_generator_batch)):\n",
    "    print(i,end=',')\n",
    "    sample = train_generator_batch[i]\n",
    "    out = model.predict(sample[0])\n",
    "    for i, sig in enumerate(scenario['target_profile_names']):\n",
    "        if scenario['predict_deltas']:\n",
    "            baseline[sig].append(sample[1]['target_'+sig])\n",
    "            predictions[sig].append(out[i])\n",
    "            \n",
    "        else:\n",
    "            baseline[sig].append(sample[1]['target_'+sig].squeeze() - sample[0]['input_'+sig].squeeze())\n",
    "            predictions[sig].append(out[i].squeeze() - sample[0]['input_'+sig].squeeze())\n",
    "        \n",
    "baseline = {sig:np.concatenate(baseline[sig],axis=0) for sig in scenario['target_profile_names']}\n",
    "predictions = {sig:np.concatenate(predictions[sig],axis=0) for sig in scenario['target_profile_names']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components=10\n",
    "full_pca_fitters = {}\n",
    "delta_pca_fitters = {}\n",
    "\n",
    "for profile in profiles:\n",
    "    full = np.array([train_generator[i][0]['input_' + profile] for i in range(len(train_generator))]).squeeze()\n",
    "    delta = np.array([train_generator[i][1]['target_' + profile] for i in range(len(train_generator))]).squeeze()\n",
    "    print(profile, ' made arrays')\n",
    "    full_pca_fitters[profile] = decomposition.IncrementalPCA(n_components=num_components).fit(full)\n",
    "    print(profile, ' done full')\n",
    "    delta_pca_fitters[profile] = decomposition.IncrementalPCA(n_components=num_components).fit(delta)\n",
    "    print(profile, ' done deltas')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
