{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "\n",
    "import pickle\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from helpers.data_generator import process_data, AutoEncoderDataGenerator\n",
    "from helpers.custom_losses import denorm_loss, hinge_mse_loss\n",
    "from helpers.custom_losses import percent_correct_sign, baseline_MAE\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from time import strftime, localtime\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import copy\n",
    "from tqdm import tqdm_notebook\n",
    "import seaborn as sns\n",
    "from helpers.normalization import normalize, denormalize, renormalize\n",
    "import scipy\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import Image, display\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from helpers.custom_init import downsample\n",
    "from helpers.custom_reg import groupLasso\n",
    "import helpers\n",
    "# from sklearn import decomposition\n",
    "# from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CPU Only. \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"  # Set this\n",
    "num_cores = 1\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=4*num_cores,\n",
    "                        inter_op_parallelism_threads=4*num_cores, \n",
    "                        allow_soft_placement=True,\n",
    "                        device_count = {'CPU' : 1,\n",
    "                                        'GPU' : 0})\n",
    "                        \n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "font={'family': 'DejaVu Serif',\n",
    "      'size': 18}\n",
    "plt.rc('font', **font)\n",
    "matplotlib.rcParams['figure.facecolor'] = (1,1,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib_colors = [(0.1215, 0.4667, 0.7058), # blue\n",
    "                     (1.0000, 0.4980, 0.0549), # orange\n",
    "                     (0.1725, 0.6275, 0.1725), # green\n",
    "                     (0.8392, 0.1529, 0.1568), # red\n",
    "                     (0.5804, 0.4039, 0.7412), # violet\n",
    "                     (0.4980, 0.4980, 0.4980), # grey\n",
    "                     (0.0902, 0.7450, 0.8117)] # cyan\n",
    "\n",
    "matlab_colors=[(0.0000, 0.4470, 0.7410), # blue\n",
    "               (0.8500, 0.3250, 0.0980), # reddish orange\n",
    "               (0.9290, 0.6940, 0.1250), # yellow\n",
    "               (0.4940, 0.1840, 0.5560), # purple\n",
    "               (0.4660, 0.6740, 0.1880), # light green\n",
    "               (0.3010, 0.7450, 0.9330), # cyan\n",
    "               (0.6350, 0.0780, 0.1840)] # dark red\n",
    "\n",
    "colorblind_colors = [(0.0000, 0.4500, 0.7000), # blue\n",
    "                     (0.8359, 0.3682, 0.0000), # vermillion\n",
    "                     (0.0000, 0.6000, 0.5000), # bluish green\n",
    "                     (0.9500, 0.9000, 0.2500), # yellow\n",
    "                     (0.3500, 0.7000, 0.9000), # sky blue\n",
    "                     (0.8000, 0.6000, 0.7000), # reddish purple\n",
    "                     (0.9000, 0.6000, 0.0000)] # orange\n",
    "\n",
    "dashes = [(1.0, 0.0, 0.0, 0.0, 0.0, 0.0), # solid\n",
    "          (3.7, 1.6, 0.0, 0.0, 0.0, 0.0), # dashed\n",
    "          (1.0, 1.6, 0.0, 0.0, 0.0, 0.0), # dotted\n",
    "          (6.4, 1.6, 1.0, 1.6, 0.0, 0.0), # dot dash\n",
    "          (3.0, 1.6, 1.0, 1.6, 1.0, 1.6), # dot dot dash\n",
    "          (6.0, 4.0, 0.0, 0.0, 0.0, 0.0), # long dash\n",
    "          (1.0, 1.6, 3.0, 1.6, 3.0, 1.6)] # dash dash dot\n",
    "\n",
    "from matplotlib import rcParams, cycler\n",
    "matplotlib.rcdefaults()\n",
    "rcParams['font.family'] = 'DejaVu Serif'\n",
    "rcParams['mathtext.fontset'] = 'cm'\n",
    "rcParams['font.size'] = 12\n",
    "rcParams['figure.facecolor'] = (1,1,1,1)\n",
    "rcParams['figure.figsize'] = (16,8)\n",
    "rcParams['figure.dpi'] = 141\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.right'] = False\n",
    "rcParams['axes.labelsize'] =  'large'\n",
    "rcParams['axes.titlesize'] = 'x-large'\n",
    "rcParams['lines.linewidth'] = 2.5\n",
    "rcParams['lines.solid_capstyle'] = 'round'\n",
    "rcParams['lines.dash_capstyle'] = 'round'\n",
    "rcParams['lines.dash_joinstyle'] = 'round'\n",
    "rcParams['xtick.labelsize'] = 'large'\n",
    "rcParams['ytick.labelsize'] = 'large'\n",
    "# rcParams['text.usetex']=True\n",
    "color_cycle = cycler(color=colorblind_colors)\n",
    "dash_cycle = cycler(dashes=dashes)\n",
    "rcParams['axes.prop_cycle'] =  color_cycle\n",
    "\n",
    "labelsize=10\n",
    "ticksize=8\n",
    "# for i,c in enumerate(colorblind_colors):\n",
    "#     plt.plot((i)*np.ones(5),c=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_sigs = {'temp':'etemp',\n",
    "         'thomson_temp_EFITRT1':'etemp',\n",
    "         'thomson_temp_EFITRT2':'etemp',\n",
    "         'dens':'edens',\n",
    "         'thomson_dens_EFITRT1':'edens',\n",
    "         'thomson_dens_EFITRT2':'edens',\n",
    "         'itemp':'itemp',\n",
    "         'cerquick_temp_EFITRT1':'itemp',\n",
    "         'cerquick_temp_EFITRT2':'itemp',\n",
    "         'rotation':'rotation',\n",
    "         'cerquick_rotation_EFITRT1':'rotation',\n",
    "         'cerquick_rotation_EFITRT2':'rotation',\n",
    "         'press_EFITRT1':'press',\n",
    "         'press_EFITRT2':'press',\n",
    "         'press_EFIT01':'press',\n",
    "         'press_EFIT02':'press',\n",
    "         'ffprime_EFITRT1':'ffprime',\n",
    "         'ffprime_EFITRT2':'ffprime',\n",
    "         'ffprime_EFIT01':'ffprime',\n",
    "         'ffprime_EFIT02':'ffprime',\n",
    "         'q':'q',\n",
    "         'q_EFITRT1':'q',\n",
    "         'q_EFITRT2':'q',\n",
    "         'q_EFIT01':'q',\n",
    "         'q_EFIT02':'q'}\n",
    "\n",
    "labels = {'edens': '$n_e$ ($10^{19}/m^3$)',\n",
    "          'etemp': '$T_e$ (keV)',\n",
    "          'itemp': '$T_i$ (keV)',\n",
    "          'rotation':'$\\Omega$ (kHz)',\n",
    "          'q':'$\\iota$',\n",
    "          'press':'$P$ (Pa)',\n",
    "         'ffprime':\"$FF'$\"}\n",
    "\n",
    "labels = {key:labels[val] for key, val in eq_sigs.items()}\n",
    "\n",
    "scatter_titles = {'mean':'Mean',\n",
    "                  'std':'Std Dev.',\n",
    "                  'pca_1':'PCA Mode 1',\n",
    "                  'pca_2':'PCA Mode 2',\n",
    "                  'pca_3':'PCA Mode 3',\n",
    "                  'pca_4':'PCA Mode 4',\n",
    "                  'pca_5':'PCA Mode 5',\n",
    "                  'pca_6':'PCA Mode 6',\n",
    "                  'pca_2':'PCA Mode 2'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with baseline across shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run_results_06_09',\n",
       " 'run_results_06_11',\n",
       " 'run_results_06_12',\n",
       " 'run_results_06_18',\n",
       " 'run_results_06_19',\n",
       " 'run_results_06_21',\n",
       " 'run_results_06_23_dim',\n",
       " 'run_results_06_27',\n",
       " 'run_results_06_20',\n",
       " 'run_results_06_22_v1',\n",
       " 'run_results_06_25',\n",
       " 'run_results_06_30',\n",
       " 'run_results_06_24_dim',\n",
       " 'run_results_06_27_rec',\n",
       " 'run_results_06_22_v3',\n",
       " 'run_results_06_26',\n",
       " 'run_results_repro']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs = [foo for foo in os.listdir('/home/aaronwu') if 'run_results' in foo]\n",
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 10\n",
      "4 10\n",
      "5 2\n",
      "6 24\n",
      "7 6\n",
      "8 1\n",
      "9 3\n",
      "10 5\n",
      "11 143\n",
      "12 15\n",
      "13 3\n",
      "14 4\n",
      "15 3\n",
      "16 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dirs)):\n",
    "    base_dir = '/home/aaronwu/' + dirs[i] + '/' #run_results_06_22_v3/'\n",
    "    models = [foo for foo in os.listdir(base_dir) if foo.endswith('.h5')]\n",
    "    print(i, len(models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/aaronwu/' + dirs[-6] + '/' #run_results_06_22_v3/'\n",
    "models = [foo for foo in os.listdir(base_dir) if foo.endswith('.h5')]\n",
    "\n",
    "scenarios = [base_dir + model[:-3] + '_params.pkl' for model in models]\n",
    "scenario_path = scenarios[0]\n",
    "ratio = []\n",
    "valmse = []\n",
    "trainmse = []\n",
    "for scenario_path in scenarios:\n",
    "    with open(scenario_path,'rb') as f:\n",
    "        scenario = pickle.load(f,encoding='latin1')\n",
    "    valmse.append(scenario['history']['val_loss'][-1])\n",
    "    trainmse.append(scenario['history']['loss'][-1])\n",
    "    ratio.append(abs(1-(scenario['history']['val_loss'][-1]/scenario['history']['loss'][-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.288753269894772,\n",
       " 3.988937324940438,\n",
       " 4.307391849763547,\n",
       " 4.292760537881757,\n",
       " 4.288205047320168,\n",
       " 4.294377952944999,\n",
       " 4.294469179740031,\n",
       " 4.2905607137932,\n",
       " 4.29247331438334,\n",
       " 4.291481364394154,\n",
       " 4.2934862930726485,\n",
       " 4.304666446508548,\n",
       " 4.264739380698963,\n",
       " 4.29237188360843,\n",
       " 4.291865903633992,\n",
       " 4.28821883615532,\n",
       " 4.253076425462928,\n",
       " 4.288186045085514,\n",
       " 4.2765603197694935,\n",
       " 4.28741401296417,\n",
       " 4.2787756850156375,\n",
       " 4.2768382807988665,\n",
       " 4.276919379497198,\n",
       " 4.298590487511043,\n",
       " 4.274354956956758,\n",
       " 4.29273392912418,\n",
       " 4.288185512161287,\n",
       " 4.2976603652477365,\n",
       " 4.28608963922813,\n",
       " 4.296954562855401,\n",
       " 4.30134273990772,\n",
       " 4.28853413602833,\n",
       " 4.293504384567252,\n",
       " 4.292895553466303,\n",
       " 4.2902101665254735,\n",
       " 4.2974428182831215,\n",
       " 4.288478777779599,\n",
       " 4.347750387590978,\n",
       " 4.28155015033674,\n",
       " 4.278940146607305,\n",
       " 4.280936967264231,\n",
       " 4.269586022699858,\n",
       " 4.407187705275635,\n",
       " 4.29341388347379,\n",
       " 4.300761067557251,\n",
       " 4.307230281576241,\n",
       " 4.257983462803284,\n",
       " 4.298278627277071,\n",
       " 4.298359138625888,\n",
       " 4.297302474787921,\n",
       " 4.303774867330958,\n",
       " 4.290552223258603,\n",
       " 4.310924810978782,\n",
       " 4.286659911239794,\n",
       " 4.295209435328253,\n",
       " 4.2961897193734675,\n",
       " 4.305949034937113,\n",
       " 4.467928427603089,\n",
       " 4.282593543083869,\n",
       " 4.297227967504236,\n",
       " 4.26972432755449,\n",
       " 4.293298325850813,\n",
       " 4.564384901361861,\n",
       " 4.296238270790462,\n",
       " 4.29761256178159,\n",
       " 4.312877130829527,\n",
       " 4.295810634342162,\n",
       " 4.308172892929846,\n",
       " 4.294919606840342,\n",
       " 4.293701452420297,\n",
       " 4.2939602708084585,\n",
       " 4.292945846978354,\n",
       " 4.485161271040806,\n",
       " 4.3076415837830195,\n",
       " 4.300251254269067,\n",
       " 4.310468645278307,\n",
       " 4.438134219439947,\n",
       " 4.307633906091561,\n",
       " 4.292968517822135,\n",
       " 4.2882390688901975,\n",
       " 4.307274478697219,\n",
       " 4.278227768391486,\n",
       " 4.281253392000815,\n",
       " 4.309017760638158,\n",
       " 4.317187810714382,\n",
       " 4.315900959936759,\n",
       " 4.326009076056214,\n",
       " 4.293923288261819,\n",
       " 4.594912353212033,\n",
       " 4.296114094342693,\n",
       " 4.297204983309573,\n",
       " 4.282473880929087,\n",
       " 4.379698048027705,\n",
       " 4.298347363044411,\n",
       " 4.296073598321709,\n",
       " 4.298844505523549,\n",
       " 4.2897150285456505,\n",
       " 4.2884074065971625,\n",
       " 4.3937414210503976,\n",
       " 4.290080642809628,\n",
       " 4.252372197511328,\n",
       " 4.2917697327095174,\n",
       " 4.303632577847274,\n",
       " 4.308196207936145,\n",
       " 4.306645455481092,\n",
       " 4.312710258544405,\n",
       " 4.340865820815027,\n",
       " 4.304559123538351,\n",
       " 4.310102432194179,\n",
       " 4.3108401105053336,\n",
       " 4.297015051964345,\n",
       " 4.388880324142477,\n",
       " 4.255231164800921,\n",
       " 4.325038620804679,\n",
       " 4.3126667996601835,\n",
       " 4.307442892748982,\n",
       " 4.311288912238784,\n",
       " 4.286262405046426,\n",
       " 4.308259111338493,\n",
       " 4.310730651069078,\n",
       " 4.613214790726374,\n",
       " 4.2632479068041915,\n",
       " 4.286678455539184,\n",
       " 4.304240301067158,\n",
       " 4.315755419109099,\n",
       " 4.316041766593264,\n",
       " 4.263165671500346,\n",
       " 4.315677994449478,\n",
       " 4.31450091717115,\n",
       " 4.542057301886897,\n",
       " 4.3162467854757365,\n",
       " 4.301953294551154,\n",
       " 4.278609553434345,\n",
       " 4.312419654089003,\n",
       " 4.315951008585086,\n",
       " 4.296342452883467,\n",
       " 4.290955982882806,\n",
       " 4.262109955263628,\n",
       " 4.515202133634586,\n",
       " 4.322895782230633,\n",
       " 4.300177010613683,\n",
       " 4.287412937625205,\n",
       " 4.25379974812883]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aaronwu/run_results_06_30/model-autoencoder_SET-dense_SDT-dense_CET-dense_CDT-dense_profiles-temp-dens-ffprime_EFIT02-press_EFIT02-q_EFIT02_act-pinj-curr-tinj-gasA_LB-0_LA-3_30Jun20-17-10.h5\n"
     ]
    }
   ],
   "source": [
    "scenario_path = scenarios[1]\n",
    "model_path = base_dir + models[1]\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0414 23:11:18.725466 35184372369072 module_wrapper.py:139] From /scratch/gpfs/aiqtidar/.conda/envs/tfgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0414 23:11:18.752466 35184372369072 module_wrapper.py:139] From /scratch/gpfs/aiqtidar/.conda/envs/tfgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0414 23:11:19.150787 35184372369072 module_wrapper.py:139] From /scratch/gpfs/aiqtidar/.conda/envs/tfgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0414 23:11:19.624019 35184372369072 module_wrapper.py:139] From /scratch/gpfs/aiqtidar/.conda/envs/tfgpu/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0414 23:11:19.989151 35184372369072 module_wrapper.py:139] From /scratch/gpfs/aiqtidar/.conda/envs/tfgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0414 23:11:20.644184 35184372369072 module_wrapper.py:139] From /scratch/gpfs/aiqtidar/.conda/envs/tfgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "W0414 23:11:21.500658 35184372369072 module_wrapper.py:139] From /scratch/gpfs/aiqtidar/.conda/envs/tfgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0414 23:11:21.503036 35184372369072 module_wrapper.py:139] From /scratch/gpfs/aiqtidar/.conda/envs/tfgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0414 23:11:21.505850 35184372369072 module_wrapper.py:139] From /scratch/gpfs/aiqtidar/.conda/envs/tfgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0414 23:11:21.997478 35184372369072 module_wrapper.py:139] From /scratch/gpfs/aiqtidar/.conda/envs/tfgpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: compile flag must be set to False if you wish to experiment with differing forecast horizons\n",
    "with open(scenario_path,'rb') as f:\n",
    "        scenario = pickle.load(f,encoding='latin1')\n",
    "model = keras.models.load_model(model_path, compile=True)\n",
    "\n",
    "#scenario['lookahead'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0585d7217a34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mSVG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_layer_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrankdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'TB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'svg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/gpfs/aiqtidar/.conda/envs/tfgpu/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir, expand_nested, dpi, subgraph)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msubgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dashed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gpfs/aiqtidar/.conda/envs/tfgpu/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         raise ImportError(\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;34m'Failed to import `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;34m'Please install `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model,show_shapes=True,show_layer_names=True,rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario['state_latent_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signals: curr, dens, ffprime_EFIT02, gasA, pinj, press_EFIT02, q_EFIT02, temp, tinj\n",
      "Number of useable shots:  5531\n",
      "Number of shots used:  5531\n",
      "Total number of timesteps:  586968\n",
      "Shots with Complete NaN: \n",
      "125691 samples total\n",
      "Removing weird I-coils\n",
      "Removed 24703 samples\n",
      "100988 samples remaining\n",
      "Removing NaN\n",
      "Removed 0 samples\n",
      "100988 samples remaining\n",
      "Removing dudtrip\n",
      "Removed 7074 samples\n",
      "93914 samples remaining\n",
      "93914 samples remaining after pruning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Denormalizing: 100%|##########| 15/15 [00:00<00:00, 234.26it/s]\n",
      "Normalizing: 100%|##########| 15/15 [00:00<00:00, 117.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples:  93914\n",
      "Number of training samples:  84213\n",
      "Number of validation samples:  9701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "datapath = '/scratch/gpfs/jabbate/full_data_with_error/train_data.pkl'\n",
    "with open(datapath,'rb') as f:\n",
    "    rawdata = pickle.load(f,encoding='latin1')\n",
    "    \n",
    "traindata, valdata, normalization_dict = process_data(rawdata,\n",
    "                                                              scenario['sig_names'],\n",
    "                                                              scenario['normalization_method'],\n",
    "                                                              scenario['window_length'],\n",
    "                                                              scenario['window_overlap'],\n",
    "                                                              scenario['lookback'],\n",
    "                                                              scenario['lookahead'],\n",
    "                                                              scenario['sample_step'],\n",
    "                                                              scenario['uniform_normalization'],\n",
    "                                                              1,\n",
    "                                                              0,\n",
    "                                                              scenario['nshots'],\n",
    "                                                              2,\n",
    "                                                              scenario['flattop_only'],\n",
    "                                                              pruning_functions=scenario['pruning_functions'],\n",
    "                                                              invert_q = scenario['invert_q'],\n",
    "                                                              val_idx = 0,\n",
    "                                                              excluded_shots=scenario['excluded_shots'],\n",
    "                                                            randomize=False)\n",
    "valdata = denormalize(valdata, normalization_dict)\n",
    "valdata = renormalize(valdata, scenario['normalization_dict'])\n",
    "generator = AutoEncoderDataGenerator(valdata,\n",
    "                                               1,  \n",
    "                                               scenario['profile_names'],\n",
    "                                               scenario['actuator_names'],\n",
    "                                               scenario['scalar_names'],\n",
    "                                               scenario['lookback'],\n",
    "                                               scenario['lookahead'],\n",
    "                                               scenario['profile_downsample'],\n",
    "                                               scenario['state_latent_dim'],\n",
    "                                               scenario['discount_factor'],\n",
    "                                               scenario['x_weight'],\n",
    "                                               scenario['u_weight'],                                            \n",
    "                                               shuffle_generators=False)#scenario['shuffle_generators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9701/9701 [==============================] - 19s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions_arr = model.predict_generator(generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9701/9701\r"
     ]
    }
   ],
   "source": [
    "# This block is for manual prediction generation\n",
    "# Use when you want to experiment with different forecast horizons\n",
    "# SKIP if you are using model.predict_generator()\n",
    "\n",
    "def get_AB(model):\n",
    "    A = model.get_layer('AB_matrices').get_weights()[1].T\n",
    "    B = model.get_layer('AB_matrices').get_weights()[0].T\n",
    "    return A,B\n",
    "\n",
    "def get_submodels(model):\n",
    "    from keras.models import Model\n",
    "    state_encoder = model.get_layer('state_encoder_time_dist').layer.layers[-1]\n",
    "    control_encoder = model.get_layer('ctrl_encoder_time_dist').layer.layers[-1]\n",
    "    state_decoder = model.get_layer('state_decoder_time_dist').layer\n",
    "    '''\n",
    "    state_decoder = Model(model.get_layer('state_decoder_time_dist').layer.layers[0].input,\n",
    "                             model.get_layer('state_decoder_time_dist').layer.layers[-2].get_output_at(1),\n",
    "                             name='state_decoder')    \n",
    "    control_decoder = Model(model.get_layer('ctrl_decoder_time_dist').layer.layers[0].input,\n",
    "                                model.get_layer('ctrl_decoder_time_dist').layer.layers[-2].get_output_at(1),\n",
    "                                name='control_decoder')\n",
    "    '''\n",
    "    return state_encoder, state_decoder, control_encoder\n",
    "\n",
    "\n",
    "def get_autoencoder_predictions(state_encoder,state_decoder,control_encoder,A,B,scenario,inputs,**kwargs):\n",
    "    import numpy as np\n",
    "    state_inputs = {}\n",
    "    x0 = {}\n",
    "    for sig in scenario['profile_names']+scenario['scalar_names']:\n",
    "        state_inputs[sig] = np.squeeze(inputs[0]['input_'+sig])\n",
    "        if sig in scenario['profile_names']:\n",
    "            x0['input_'+sig] = inputs[0]['input_'+sig][0][0].reshape((1,1,scenario['profile_length']))\n",
    "        else:\n",
    "            x0['input_'+sig] = inputs[0]['input_'+sig][0][0].reshape((1,1,1))\n",
    "    control_inputs = {}\n",
    "    for sig in scenario['actuator_names']:\n",
    "        control_inputs['input_'+sig] = inputs[0]['input_'+sig]\n",
    "    # encode control    \n",
    "    T = scenario['lookback'] + scenario['lookahead'] \n",
    "    u = []\n",
    "    for i in range(T):\n",
    "        temp_input = {k:v[:,i].reshape((1,1,1)) for k,v in control_inputs.items()}\n",
    "        u.append(np.squeeze(control_encoder.predict(temp_input)))\n",
    "    # encode state and propogate\n",
    "    x0 = np.squeeze(state_encoder.predict(x0))\n",
    "    x = [x0]\n",
    "    for i in range(scenario['lookahead']):\n",
    "        x.append(A.dot(x[i])+B.dot(u[i]))\n",
    "       \n",
    "    # decode state and organize\n",
    "    x_decoded = []\n",
    "    for elem in x:\n",
    "        x_decoded.append(state_decoder.predict(elem[np.newaxis,:]))\n",
    "    state_predictions = {}\n",
    "    residuals = {}\n",
    "    for i, sig in enumerate(scenario['profile_names']):\n",
    "        state_predictions[sig] = np.squeeze(np.dsplit((np.array([x_decoded[j] for j in range(len(x_decoded))])),5)[i])\n",
    "        residuals[sig] = state_inputs[sig] - state_predictions[sig]\n",
    "\n",
    "    return state_inputs, state_predictions, residuals\n",
    "\n",
    "\n",
    "A, B = get_AB(model)\n",
    "state_encoder, state_decoder, control_encoder = get_submodels(model)\n",
    "model_err = {sig:[] for sig in scenario['profile_names']}\n",
    "for i in range(len(generator)):\n",
    "    print(\"{}/{}\".format(i+1,len(generator)),end='\\r')\n",
    "    __,__,resids = get_autoencoder_predictions(state_encoder,state_decoder,control_encoder,A,B,scenario,generator[i])\n",
    "    for sig in scenario['profile_names']:\n",
    "        model_err[sig].append(np.abs(resids[sig][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute model and baseline error\n",
    "\n",
    "full_profiles = {sig:[] for sig in scenario['profile_names']}\n",
    "baseline_err = {sig:[] for sig in scenario['profile_names']}\n",
    "\n",
    "\n",
    "#For each batch\n",
    "for i in range(len(generator)):\n",
    "    print(\"{}/{}\".format(i+1,len(generator)),end='\\r')\n",
    "    sample = generator[i]\n",
    "    #Append each sample\n",
    "    for sig in scenario['profile_names']:\n",
    "        full_profiles[sig].append(sample[0]['input_'+sig])\n",
    "        \n",
    "for sig in scenario['profile_names']:\n",
    "    ##for each batch\n",
    "    for i in range(len(full_profiles[sig])):\n",
    "        print(\"{}/{}\".format(i+1,len(full_profiles[sig])),end='\\r')\n",
    "        #For each sample\n",
    "        for j in range(len(full_profiles[sig][i])):\n",
    "            baseline_err[sig].append(np.abs(full_profiles[sig][i][j][-1]-full_profiles[sig][i][j][0]))\n",
    "            \n",
    "# Use this block ONLY when you are using keras' inbuilt predict_generator().\n",
    "\n",
    "model_err = {sig:[] for sig in scenario['profile_names']}\n",
    "for i in range(len(predictions_arr[0])-1):\n",
    "    for j, sig in enumerate(scenario['profile_names']):\n",
    "        model_err[sig].append(np.abs(predictions_arr[0][i][-1][j*33:((j+1)*33)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bas_tot = {sig:[] for sig in scenario['profile_names']}\n",
    "mod_tot = {sig:[] for sig in scenario['profile_names']}\n",
    "\n",
    "for sig in scenario['profile_names']:\n",
    "    \n",
    "    bas_tot[sig] = np.median(np.array(baseline_err[sig])*scenario['normalization_dict'][sig]['iqr'],axis=0)\n",
    "    mod_tot[sig] = np.median(np.array(model_err[sig])*scenario['normalization_dict'][sig]['iqr'],axis=0)\n",
    "    \n",
    "    # Print out 2-norm of median error for each profile\n",
    "    print('model ' + sig + ' error: ' + str(np.linalg.norm(mod_tot[sig], ord=2)))\n",
    "    print('baseline ' + sig + ' error: ' + str(np.linalg.norm(bas_tot[sig], ord=2)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"get median and 25/75 percentile errors, denormalized, for model and baseline\"\"\"\n",
    "\n",
    "hi_per = 75\n",
    "low_per = 25\n",
    "\n",
    "hiper_model_err = {sig:np.percentile(np.array(model_err[sig])*scenario['normalization_dict'][sig]['iqr'],hi_per,axis=0) for sig in scenario['profile_names']}\n",
    "hiper_baseline_err = {sig:np.percentile(np.array(baseline_err[sig])*scenario['normalization_dict'][sig]['iqr'],hi_per,axis=0) for sig in scenario['profile_names']}\n",
    "\n",
    "lowper_model_err = {sig:np.percentile(np.array(model_err[sig])*scenario['normalization_dict'][sig]['iqr'],low_per,axis=0) for sig in scenario['profile_names']}\n",
    "lowper_baseline_err = {sig:np.percentile(np.array(baseline_err[sig])*scenario['normalization_dict'][sig]['iqr'],low_per,axis=0) for sig in scenario['profile_names']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model vs baseline\n",
    "\n",
    "fig, ax = plt.subplots(len(scenario['profile_names']),1,figsize=(10,15))\n",
    "psi = np.linspace(0,33,33)\n",
    "for i,sig in enumerate(scenario['profile_names']):\n",
    "    \n",
    "    ax[i].plot(bas_tot[sig], label='baseline',color=colorblind_colors[1],lw=1.5)\n",
    "    ax[i].fill_between(psi,lowper_baseline_err[sig],hiper_baseline_err[sig],color=colorblind_colors[1],alpha=0.2)  \n",
    "    ax[i].plot(mod_tot[sig], label='model', color=colorblind_colors[0], lw=1.5)\n",
    "    ax[i].fill_between(psi,lowper_model_err[sig],hiper_model_err[sig],color=colorblind_colors[0],alpha=0.2)    \n",
    "\n",
    "    ax[i].set_ylim(0,None)\n",
    "    ax[i].set_ylabel(sig,size=16)\n",
    "plt.subplots_adjust(hspace=0.9, bottom=0.12)\n",
    "fig.legend(handles=ax[0].lines,     \n",
    "           labels=[line._label for line in ax[0].lines],  \n",
    "           loc=\"upper center\",\n",
    "           bbox_to_anchor=(0.6, 0.06),\n",
    "           frameon=False,\n",
    "           mode=None,\n",
    "           ncol=2,\n",
    "           fontsize=16);\n",
    "fig.suptitle(\"200ms\",y=0.9)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Prediction vs True\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filtfilt(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "cutoff = 1500\n",
    "fs = 50000\n",
    "predictions_smooth = butter_lowpass_filtfilt(predictions[0][-1][j*33:(j+1)*33].squeeze(), cutoff, fs)\n",
    "\n",
    "# good_indices = [5085,8351,1749,10746,7299,479,9980,5878,1297,12951]\n",
    "good_indices = [4851,6074,7645,765,2267,4715]\n",
    "psi = np.linspace(0,1,scenario['profile_length'])\n",
    "# index = good_indices[0]\n",
    "index = np.random.randint(len(generator))\n",
    "print(index)\n",
    "inputs, targets,_ = generator[index]\n",
    "profiles = scenario['profile_names']#['dens','temp','rotation','q_EFIT02','press_EFIT02']\n",
    "\n",
    "shotnum = generator.cur_shotnum[0,0]\n",
    "shottime = generator.cur_times[0,-scenario['lookahead']-1]\n",
    "\n",
    "predictions=model.predict_on_batch(inputs)\n",
    "\n",
    "fig, axes = plt.subplots(len(profiles),1,sharex=True, figsize=(3.5,5))\n",
    "for j, profile in enumerate(profiles):\n",
    "    ax = axes[j]\n",
    "    predictions_smooth = butter_lowpass_filtfilt(predictions[0][-1][j*33:(j+1)*33].squeeze(), cutoff, fs)\n",
    "    inp = inputs['input_{}'.format(profile)][0][0].squeeze()\n",
    "    true = inputs['input_{}'.format(profile)][0][-1].squeeze()\n",
    "#     pred = inputs['input_{}'.format(profile)][0][-1].squeeze() +  predictions[0][-1][j*33:(j+1)*33].squeeze()\n",
    "    pred = inputs['input_{}'.format(profile)][0][-1].squeeze() + predictions_smooth\n",
    "    \n",
    "    inp = helpers.normalization.denormalize_arr(inp,scenario['normalization_dict'][profile])\n",
    "    true = helpers.normalization.denormalize_arr(true,scenario['normalization_dict'][profile])\n",
    "    pred = helpers.normalization.denormalize_arr(pred,scenario['normalization_dict'][profile])\n",
    "\n",
    "    ax.plot(psi,inp,lw=1.5,label='True, time $t$',c=colorblind_colors[1])\n",
    "    ax.plot(psi,true,lw=1.5,label='True, time $t+300$ms',c='k',ls=':')\n",
    "    ax.plot(psi,pred,lw=1.5,label='Prediction, time $t+300$ms',c=colorblind_colors[0])\n",
    "\n",
    "    ax.set_ylabel(labels[profile],size=8)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "    #ax.set_xticks([0,.5,1])\n",
    "    #ax.set_xlim(0,1)\n",
    "    if j in [0,1,2]:\n",
    "        ax.set_xlabel('$\\\\rho$',size=10)\n",
    "    if j in [3,4]:\n",
    "        ax.set_xlabel('$\\psi$',size=10)\n",
    "fig.align_ylabels(axes)\n",
    "plt.subplots_adjust(top=.92, bottom=.18, hspace=0.5)\n",
    "fig.suptitle(\"Shot# {}, $t=${} ms\".format(int(shotnum), int(shottime)),fontsize=10)\n",
    "fig.legend(handles=axes[0].lines,     \n",
    "           labels=[line._label for line in axes[0].lines],  \n",
    "           loc=\"upper center\",\n",
    "           bbox_to_anchor=(0.6, 0.12),\n",
    "           frameon=False,\n",
    "           mode=None,\n",
    "           ncol=1,\n",
    "          fontsize=8)\n",
    "#fig.savefig('images/timeslice_{}.png'.format(index),bbox_inches='tight',pad_inches=0.1)\n",
    "#fig.savefig('images/timeslice_{}.pdf'.format(index),bbox_inches='tight',pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def scalarize_mean(arr, **kwargs):\n",
    "    return np.mean(arr)\n",
    "\n",
    "def scalarize_std(arr, **kwargs):\n",
    "    return np.std(arr)\n",
    "\n",
    "def scalarize_pca_1(arr, **kwargs):\n",
    "    fitter = kwargs.get('fitter')\n",
    "    ret = fitter.transform(arr).squeeze()[0]\n",
    "    return ret\n",
    "\n",
    "def scalarize_pca_2(arr, **kwargs):\n",
    "    fitter = kwargs.get('fitter')\n",
    "    ret = fitter.transform(arr).squeeze()[1]\n",
    "    return ret\n",
    "\n",
    "def scalarize_pca_3(arr, **kwargs):\n",
    "    fitter = kwargs.get('fitter')\n",
    "    ret = fitter.transform(arr).squeeze()[2]\n",
    "    return ret\n",
    "\n",
    "def scalarize_pca_4(arr, **kwargs):\n",
    "    fitter = kwargs.get('fitter')\n",
    "    ret = fitter.transform(arr).squeeze()[3]\n",
    "    return ret\n",
    "\n",
    "def scalarize_pca_5(arr, **kwargs):\n",
    "    fitter = kwargs.get('fitter')\n",
    "    ret = fitter.transform(arr).squeeze()[4]\n",
    "    return ret\n",
    "\n",
    "def scalarize_pca_6(arr, **kwargs):\n",
    "    fitter = kwargs.get('fitter')\n",
    "    ret = fitter.transform(arr).squeeze()[5]\n",
    "    return ret\n",
    "\n",
    "def find_bounds(true,pred,percentile=90):\n",
    "    arr = np.concatenate([true,pred]).flatten()\n",
    "  \n",
    "    true_bounds=(np.percentile(true, 50-percentile/2),np.percentile(true, 50+percentile/2))\n",
    "    pred_bounds=(np.percentile(pred, 50-percentile/2),np.percentile(pred, 50+percentile/2))\n",
    "    return (np.maximum(true_bounds[0],pred_bounds[0]),np.minimum(true_bounds[1],pred_bounds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_generator = AutoEncoderDataGenerator(valdata,\n",
    "                                               1,\n",
    "                                               scenario['profile_names'],\n",
    "                                               [],\n",
    "                                               [],\n",
    "                                               scenario['lookback'],\n",
    "                                               scenario['lookahead'],\n",
    "                                               scenario['profile_downsample'],\n",
    "                                               scenario['state_latent_dim'],\n",
    "                                               scenario['discount_factor'],\n",
    "                                               scenario['x_weight'],\n",
    "                                               scenario['u_weight'],                                            \n",
    "                                               False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate PCA eigenvectors\n",
    "\n",
    "num_components=10\n",
    "pca_fit = {}\n",
    "baseline = {}\n",
    "for j,profile in enumerate(scenario['profile_names']):\n",
    "    #If PCA plot of deltas are desired use:\n",
    "    #full = np.array([pca_generator[i][0]['input_' + profile][0][-1] - pca_generator[i][0]['input_' + profile][0][0] for i in range(len(pca_generator))]).squeeze()\n",
    "    full = np.array([pca_generator[i][0]['input_' + profile][0][-1] for i in range(len(pca_generator))]).squeeze()\n",
    "    baseline[profile] = full\n",
    "    print(full.shape)\n",
    "    print(profile, ' made arrays')\n",
    "    pca_fit[profile] = decomposition.IncrementalPCA(n_components=num_components).fit(full)\n",
    "    print(profile, ' done full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find PCA coefficients\n",
    "profiles = scenario['profile_names']\n",
    "\n",
    "num_samples = baseline[profiles[0]].shape[0]\n",
    "scalarize_functions = [scalarize_mean, scalarize_pca_1,scalarize_pca_2]\n",
    "scalarize_function_names = [fun.__name__[10:] for fun in scalarize_functions]\n",
    "\n",
    "all_true_prof = {sig:{metric:np.zeros(num_samples) for metric in scalarize_function_names} for sig in profiles}\n",
    "all_predicted_prof = {sig:{metric:np.zeros(num_samples) for metric in scalarize_function_names} for sig in profiles}\n",
    "\n",
    "for j,profile in enumerate(profiles):\n",
    "    for k in range(num_samples):\n",
    "        target = baseline[profile][k][np.newaxis,:]\n",
    "        pred = (predictions_arr[0][k][-1][j*33:((j+1)*33)]+baseline[profile][k])[np.newaxis,:]\n",
    "        for i,scalarize in enumerate(scalarize_functions):\n",
    "            all_true_prof[profile][scalarize_function_names[i]][k] = scalarize(target, fitter=pca_fit[profile])\n",
    "            all_predicted_prof[profile][scalarize_function_names[i]][k] = scalarize(pred, fitter=pca_fit[profile])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Scatter plot of PCA Coeffs\"\"\"\n",
    "\n",
    "\n",
    "def my_formatter(x, pos):\n",
    "    if abs(x - int(x))< np.finfo(np.float32).eps:\n",
    "        return str(int(x))\n",
    "    else:\n",
    "        return str(np.around(x,3))\n",
    "#         if x<0:\n",
    "#             return str(x)[:6]\n",
    "#         else:\n",
    "#             return str(x)[:5]\n",
    "\n",
    "formatter = matplotlib.ticker.FuncFormatter(my_formatter)\n",
    "\n",
    "profiles = ['dens','temp','rotation','q_EFIT02','press_EFIT02']\n",
    "\n",
    "fig, axes = plt.subplots(len(profiles),len(scalarize_function_names), figsize=(3.5,7))\n",
    "\n",
    "for i,profile in enumerate(profiles):\n",
    "    for j,name in enumerate(scalarize_function_names):\n",
    "\n",
    "        pred=copy.deepcopy(all_predicted_prof[profile][name])\n",
    "        true=copy.deepcopy(all_true_prof[profile][name])\n",
    "\n",
    "        ax = axes[i,j]\n",
    "#         if name in ['Mean','Std. Dev.']:\n",
    "        pred *= scenario['normalization_dict'][profile]['iqr']\n",
    "        true *= scenario['normalization_dict'][profile]['iqr']\n",
    "        \n",
    "        bounds = find_bounds(true,pred,95)\n",
    "        ax.set_xlim(bounds)\n",
    "        ax.set_ylim(bounds)\n",
    "\n",
    "        r2_model = r2_score(true,pred)\n",
    "        r2_baseline = r2_score(true, np.zeros(true.shape))\n",
    "\n",
    "        hb = ax.hexbin(true,pred,gridsize=100,bins='log',mincnt=0,cmap='Blues')\n",
    "        ax.text(0.95, 0.1, '$R^2=${:.2f}'.format(r2_model), fontsize=6, horizontalalignment='right',verticalalignment='bottom', \n",
    "                transform=ax.transAxes,bbox=dict(facecolor=colorblind_colors[-1], alpha=0.5))\n",
    "#         ax.text(0.95, 0.1, '$R^2=${:.2f}'.format(r2_baseline), fontsize=8, horizontalalignment='right',verticalalignment='bottom', \n",
    "#                 transform=ax.transAxes,bbox=dict(facecolor=colorblind_colors[1], alpha=0.5)) \n",
    "\n",
    "        x_45 = np.linspace(*ax.get_xlim())\n",
    "        ax.plot(x_45, x_45,color='k', linestyle=':',lw=1.5,label='Prediction = True')\n",
    "#         ax.axhline(0,color=colorblind_colors[1],lw=1.5,label='Baseline')\n",
    "        if i==0:\n",
    "            ax.set_title(scatter_titles[name], size=8)\n",
    "#         if i==len(profiles)-1:\n",
    "#             ax.set_xlabel('True',size=10)\n",
    "        if j in [0]:\n",
    "            ax.set_ylabel(labels[profile], size=8)\n",
    "#         if j in [1,2,3]:\n",
    "#             if ylabels[i] == '$q$':\n",
    "#                 ax.set_ylabel(labels[i] + ' \\n Prediction', size=10)\n",
    "#             else:\n",
    "#                 ax.set_ylabel(labels[i] + ' (unitless) \\n Prediction', size=10)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "#         ax.ticklabel_format(style='sci',scilimits=(-3,4),axis='both')\n",
    "        ax.xaxis.set_major_formatter(formatter)\n",
    "        ax.yaxis.set_major_formatter(formatter)\n",
    "        ax.xaxis.set_minor_formatter(formatter)\n",
    "        ax.yaxis.set_minor_formatter(formatter)\n",
    "\n",
    "# axes[3,1].set_xticks([-.25,0,.25])\n",
    "# axes[3,1].set_xticklabels(['-0.25','0','0.25'])\n",
    "# axes[3,2].set_xticks([-.25,0,.25])\n",
    "# axes[3,2].set_xticklabels(['-0.25','0','0.25'])\n",
    "\n",
    "axes[4,0].set_yticks([0,20000])\n",
    "axes[4,0].set_yticklabels(['0',r'2$\\times 10^4$'])\n",
    "axes[4,0].set_xticks([0,20000])\n",
    "axes[4,0].set_xticklabels(['0',r'2$\\times 10^4$'])\n",
    "\n",
    "axes[4,1].set_yticks([0,100000])\n",
    "axes[4,1].set_yticklabels(['0',r'$10^5$'])\n",
    "axes[4,1].set_xticks([0,100000])\n",
    "axes[4,1].set_xticklabels(['0',r'$10^5$'])\n",
    "\n",
    "axes[4,2].set_yticks([0,-10000])\n",
    "axes[4,2].set_yticklabels(['0',r'-$10^4$'])\n",
    "axes[4,2].set_xticks([0,-10000])\n",
    "axes[4,2].set_xticklabels(['0',r'-$10^4$'])\n",
    "\n",
    "plt.subplots_adjust(hspace=0.4,wspace=0.65,bottom=0.15, left=0.2)\n",
    "\n",
    "fig.text(0.5, 0.1, 'True', va='center', ha='center', fontsize=10)\n",
    "fig.text(0.02, 0.5, 'Prediction', va='center', ha='center', rotation='vertical', fontsize=10)\n",
    "\n",
    "fig.align_ylabels(axes[:,0])\n",
    "\n",
    "\n",
    "\n",
    "cbar_ax = fig.add_axes([0.15, 0.01, 0.7, 0.01])\n",
    "cb = fig.colorbar(hb, cax=cbar_ax, orientation='horizontal', fraction=.3)\n",
    "cb.set_label('Count', size=8)\n",
    "cb.ax.tick_params(labelsize=8)\n",
    "\n",
    "fig.legend(handles=axes[0,0].lines,     \n",
    "           labels=[line._label for line in axes[0,0].lines],  \n",
    "           loc=\"upper center\",\n",
    "           bbox_to_anchor=(0.5, 0.15),\n",
    "           frameon=False,\n",
    "           mode=None,\n",
    "           ncol=2,\n",
    "           fontsize=8);\n",
    "#fig.savefig('images/all_pca_acc.pdf',bbox_inches='tight',pad_inches=0.1)\n",
    "#fig.savefig('images/all_pca_acc.png',bbox_inches='tight',pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"plot full pca modes\"\"\"\n",
    "psi = np.linspace(0,1,scenario['profile_length'])\n",
    "labels = ['$n_e$','$T_e$','$\\Omega$','$\\iota$','$P$']\n",
    "profiles = ['dens','temp','rotation','q_EFIT02','press_EFIT02']\n",
    "\n",
    "ncols = 2\n",
    "nrows = 5\n",
    "ylabels = profiles\n",
    "fig, axes = plt.subplots(len(profiles),ncols, sharex=True, sharey=True,figsize=(3.5,5))\n",
    "# fig, axes = plt.subplots(nrows,ncols, sharex=True, sharey=False,figsize=(6,5))\n",
    "\n",
    "\n",
    "for i,profile in enumerate(profiles):\n",
    "    pca=pca_fit[profile]\n",
    "    for j in range(ncols):\n",
    "        ax= axes[i,j]\n",
    "        ax.plot(psi,pca.components_[j],lw=1)\n",
    "        ax.text(0.3,0.25,'$\\hat{\\sigma}^2$' + ' = {:.3f}'.format(pca.explained_variance_ratio_[j]),fontsize=8)\n",
    "        if j==0:\n",
    "            ax.set_ylabel(labels[i], size=8)\n",
    "        if i==0:\n",
    "            ax.set_title('PCA Mode {}'.format(j+1), size=8)\n",
    "        if i in [0,1,2]:\n",
    "            ax.set_xlabel('$\\\\rho$',size=10)\n",
    "        if i in [3,4]:\n",
    "            ax.set_xlabel('$\\psi$',size=10)\n",
    "        ax.set_ylim(-.4,.4)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "        ax.set_xticks([0,.5,1])\n",
    "        ax.set_xlim(0,1)\n",
    "\n",
    "        ax.axhline(0,c='k',lw=1)\n",
    "plt.subplots_adjust(hspace=0.8)\n",
    "plt.tight_layout()\n",
    "# fig.savefig('images/pca_modes.pdf')\n",
    "# fig.savefig('images/pca_modes.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\"\"\"plot explained variance\"\"\"\n",
    "\n",
    "ylabels = ['$n_e$','$T_e$','$\\Omega$','$\\iota$','$P$']\n",
    "\n",
    "fig, axes = plt.subplots(1,len(profiles),sharey=False,figsize=(10,2))\n",
    "for j,profile in enumerate(profiles):\n",
    "    pca=pca_fit[profile]\n",
    "    ax = axes[j]\n",
    "    \n",
    "    ax.plot(np.arange(1,pca.n_components+1),np.cumsum(pca.explained_variance_ratio_),)\n",
    "    ax.set_ylabel(ylabels[j],size=10)\n",
    "    ax.set_xlabel('N modes', size=8)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.grid()\n",
    "    ax.tick_params(axis='both', which='both', labelsize=6)\n",
    "    \n",
    "fig.suptitle('Explained Variance (full profiles)', size=10)\n",
    "plt.subplots_adjust(wspace=0.7)\n",
    "# fig.savefig('explained_variance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All old stuff below. Use autoencoder_plot.py instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"A\" (Koopman Approximation) and \"B\" (recurrent actuator weights) Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_AB(model):\n",
    "    A = model.get_layer('AB_matrices').get_weights()[1].T\n",
    "    B = model.get_layer('AB_matrices').get_weights()[0].T\n",
    "    return A,B\n",
    "\n",
    "def get_submodels(model):\n",
    "    from keras.models import Model\n",
    "    state_encoder = model.get_layer('state_encoder_time_dist').layer.layers[-1]\n",
    "    control_encoder = model.get_layer('ctrl_encoder_time_dist').layer.layers[-1]\n",
    "    state_decoder = model.get_layer('state_decoder_time_dist').layer\n",
    "    #state_decoder = Model(model.get_layer('state_decoder_time_dist').layer.layers[0].input,\n",
    "    #                      model.get_layer('state_decoder_time_dist').layer.layers[-2].get_output_at(1),\n",
    "    #                     name='state_decoder')    \n",
    "    #control_decoder = Model(model.get_layer('ctrl_decoder_time_dist').layer.layers[0].input,\n",
    "    #                        model.get_layer('ctrl_decoder_time_dist').layer.layers[-2].get_output_at(1),\n",
    "    #                        name='control_decoder')\n",
    " \n",
    "    return state_encoder, state_decoder, control_encoder #, control_decoder\n",
    "\n",
    "def plot_autoencoder_AB(model,scenario, filename=None, **kwargs):\n",
    "    \n",
    "    A,B = get_AB(model)\n",
    "    f, axes = plt.subplots(1, 2, figsize=(28, 14),\n",
    "                           gridspec_kw={'width_ratios': [scenario['state_latent_dim'], \n",
    "                                                         scenario['control_latent_dim']]})\n",
    "    sns.heatmap(A, \n",
    "                cmap=kwargs.get('cmap','Spectral'),\n",
    "                annot=kwargs.get('annot',False), \n",
    "                square=kwargs.get('square',True), \n",
    "                robust=kwargs.get('robust',False), \n",
    "                ax=axes[0]).set_title('A')\n",
    "    sns.heatmap(B,\n",
    "                cmap=kwargs.get('cmap','Spectral'), \n",
    "                annot=kwargs.get('annot',False), \n",
    "                square=kwargs.get('square',True), \n",
    "                robust=kwargs.get('robust',False), \n",
    "                ax=axes[1]).set_title('B')\n",
    "\n",
    "    if filename:\n",
    "        f.savefig(filename,bbox_inches='tight')\n",
    "        html = \"\"\"<img src=\\\"\"\"\" + filename + \"\"\"\\\"><p>\"\"\"\n",
    "        return f, html\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalue plots of A matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_autoencoder_spectrum(model,scenario, filename=None, **kwargs):\n",
    "\n",
    "    font={'family': 'DejaVu Serif',\n",
    "          'size': 18}\n",
    "    plt.rc('font', **font)\n",
    "    matplotlib.rcParams['figure.facecolor'] = (1,1,1,1)\n",
    "    \n",
    "    dt = scenario['dt']\n",
    "    A,B = get_AB(model)\n",
    "    eigvals, eigvecs = np.linalg.eig(A)\n",
    "    logeigvals = np.log(eigvals)\n",
    "    for i, elem in enumerate(logeigvals):\n",
    "        if abs(np.imag(elem)-np.pi)<np.finfo(np.float32).resolution:\n",
    "            logeigvals[i] = np.real(elem) + 0j\n",
    "    logeigvals = logeigvals/dt\n",
    "\n",
    "    f, axes = plt.subplots(1, 2, figsize=(28, 14))\n",
    "    axes[0].scatter(np.real(eigvals),np.imag(eigvals))\n",
    "    t = np.linspace(0,2*np.pi,1000)\n",
    "    axes[0].plot(np.cos(t),np.sin(t))\n",
    "\n",
    "    axes[0].set_title('Eigenvalues of A')\n",
    "    axes[0].grid(color='gray')\n",
    "    axes[0].set_xlabel('Re($\\lambda$)')\n",
    "    axes[0].set_ylabel('Im($\\lambda$)')\n",
    "\n",
    "\n",
    "    axes[1].scatter(np.real(logeigvals),np.imag(logeigvals))\n",
    "    axes[1].set_title('Eigenvalues of A')\n",
    "    axes[1].grid(color='gray')\n",
    "    axes[1].set_xlabel('Growth Rate (1/s)')\n",
    "    axes[1].set_ylabel('$\\omega$ (rad/s)')\n",
    "    axes[1].set_xlim((1.1*np.min(np.real(logeigvals)),np.maximum(1.1*np.max(np.real(logeigvals)),0)))\n",
    "    \n",
    "    if filename:\n",
    "        f.savefig(filename,bbox_inches='tight')\n",
    "        html = \"\"\"<img src=\\\"\"\"\" + filename + \"\"\"\\\"><p>\"\"\"\n",
    "        return f, html\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Training Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_autoencoder_training(model,scenario,filename=None,**kwargs):\n",
    "\n",
    "    f, axes = plt.subplots(2, 2, figsize=(28, 28))\n",
    "    axes[0,0].semilogy(scenario['history']['loss'],label='train')\n",
    "    axes[0,0].semilogy(scenario['history']['val_loss'],label='val')\n",
    "    axes[0,0].set_title('Loss')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    axes[0,1].semilogy(scenario['history']['x_residual_mean_squared_error'],label='train')\n",
    "    axes[0,1].semilogy(scenario['history']['val_x_residual_mean_squared_error'],label='val')\n",
    "    axes[0,1].set_title('X residual MSE')\n",
    "    axes[0,1].legend()\n",
    "    '''\n",
    "    axes[1,0].semilogy(scenario['history']['u_residual_mean_squared_error'],label='train')\n",
    "    axes[1,0].semilogy(scenario['history']['val_u_residual_mean_squared_error'],label='val')\n",
    "    axes[1,0].set_title('U residual MSE')\n",
    "    axes[1,0].legend()\n",
    "    '''\n",
    "    axes[1,1].semilogy(scenario['history']['linear_system_residual_mean_squared_error'],label='train')\n",
    "    axes[1,1].semilogy(scenario['history']['val_linear_system_residual_mean_squared_error'],label='val')\n",
    "    axes[1,1].set_title('Linear Model MSE')\n",
    "    axes[1,1].legend()\n",
    "    \n",
    "    \n",
    "    if filename:\n",
    "        f.savefig(filename,bbox_inches='tight')\n",
    "        html = \"\"\"<img src=\\\"\"\"\" + filename + \"\"\"\\\"><p>\"\"\"\n",
    "        return f, html\n",
    "    return f\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observability and Controlability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stabAnalysis(model, scenario, filename = None):\n",
    "    A,B = get_AB(model)\n",
    "    dt = .05\n",
    "    # Ac = logm(Ad)/dt\n",
    "    Ac = scipy.linalg.logm(A)/dt\n",
    "    # Bc = inv(Ad-I)*Ac*Bd\n",
    "    Bc = np.matmul(np.linalg.inv(A-np.eye(scenario['state_latent_dim'])),np.matmul(Ac,B))\n",
    "    C = np.eye(scenario['state_latent_dim'])\n",
    "    D = np.zeros((scenario['state_latent_dim'],scenario['control_latent_dim']))\n",
    "\n",
    "    Wcc = scipy.linalg.solve_continuous_lyapunov(Ac,-Bc.dot(Bc.T))\n",
    "    Wdc = scipy.linalg.solve_discrete_lyapunov(A,B.dot(B.T))\n",
    "\n",
    "    Wco = scipy.linalg.solve_continuous_lyapunov(Ac.T,-C.dot(C.T)).T\n",
    "    Wdo = scipy.linalg.solve_discrete_lyapunov(A.T,C.dot(C.T)).T\n",
    "\n",
    "    fig = plt.figure(figsize=(28,14))\n",
    "    plt.subplot(1,2,1,title='Wcc, Wdc')\n",
    "    plt.semilogy(np.linalg.svd(Wcc,compute_uv=False), label ='wcc')\n",
    "    plt.semilogy(np.linalg.svd(Wdc,compute_uv=False), label = 'wdc')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.subplot(1,2,2, title = 'Wco, Wdo')\n",
    "    plt.semilogy(np.linalg.svd(Wco,compute_uv=False),label = 'wco')\n",
    "    plt.semilogy(np.linalg.svd(Wdo,compute_uv=False), label = 'wdo')\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    if filename:\n",
    "        fig.savefig(filename,bbox_inches='tight')\n",
    "        html = \"\"\"<img src=\\\"\"\"\" + filename + \"\"\"\\\"><p>\"\"\"\n",
    "        return fig, html\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot State and Control Residuals for Specified Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autoencoder_predictions(state_encoder,state_decoder,control_encoder,A,B,scenario,inputs,shot,**kwargs):\n",
    "    import numpy as np\n",
    "    state_inputs = {}\n",
    "    x0 = {}\n",
    "    for sig in scenario['profile_names']+scenario['scalar_names']:\n",
    "        state_inputs[sig] = np.squeeze(inputs['input_'+sig])\n",
    "        if sig in scenario['profile_names']:\n",
    "            x0['input_'+sig] = inputs['input_'+sig][:,0,:].reshape((1,1,scenario['profile_length']))\n",
    "        else:\n",
    "            x0['input_'+sig] = inputs['input_'+sig][:,0].reshape((1,1,1))\n",
    "    control_inputs = {}\n",
    "    for sig in scenario['actuator_names']:\n",
    "        control_inputs['input_'+sig] = inputs['input_'+sig]\n",
    "    # encode control    \n",
    "    T = scenario['lookback'] + scenario['lookahead'] + 1\n",
    "    u = []\n",
    "    for i in range(T):\n",
    "        temp_input = {k:v[:,i].reshape((1,1,1)) for k,v in control_inputs.items()}\n",
    "        u.append(np.squeeze(control_encoder.predict(temp_input)))\n",
    "    # encode state and propogate\n",
    "    x0 = np.squeeze(state_encoder.predict(x0))\n",
    "    x = [x0]\n",
    "    for i in range(scenario['lookahead']):\n",
    "        x.append(A.dot(x[i])+B.dot(u[i]))\n",
    "        '''\n",
    "        print(x[i])\n",
    "        print('Control')\n",
    "        print(B.dot(u[i]))\n",
    "        '''\n",
    "    # decode state and organize\n",
    "    x_decoded = []\n",
    "    for elem in x:\n",
    "        x_decoded.append(state_decoder.predict(elem[np.newaxis,:]))\n",
    "    state_predictions = {}\n",
    "    residuals = {}\n",
    "    for i, sig in enumerate(scenario['profile_names']):\n",
    "        state_predictions[sig] = np.squeeze(np.dsplit((np.array([x_decoded[j] for j in range(len(x_decoded))])),5)[i])\n",
    "        residuals[sig] = state_inputs[sig] - state_predictions[sig]\n",
    "\n",
    "    return state_inputs, state_predictions, residuals\n",
    "\n",
    "def plot_autoencoder_residuals(model,scenario,generator,shots,times, filename = None,**kwargs):\n",
    "    A, B = get_AB(model)\n",
    "    state_encoder, state_decoder, control_encoder = get_submodels(model)\n",
    "    inputs, targets, actual = generator.get_data_by_shot_time(shots,times)\n",
    "    psi = np.linspace(0,1,scenario['profile_length'])\n",
    "    nsteps = scenario['lookahead']\n",
    "    \n",
    "    fig = plt.figure(figsize=(40, 60))\n",
    "    outer_grid = fig.add_gridspec(len(times),1)\n",
    "    \n",
    "    for k, (shot,time) in enumerate(zip(actual['shots'],actual['times'])):\n",
    "        inp = {sig:arr[np.newaxis,k] for sig, arr in inputs.items()}\n",
    "        state_inputs, state_predictions, residuals = get_autoencoder_predictions(\n",
    "            state_encoder,state_decoder,control_encoder,A,B,scenario,inp,shot)\n",
    "        \n",
    "        outerax = fig.add_subplot(outer_grid[k])\n",
    "        outerax.set_title(label='Shot ' + str(int(shot)) + '   Time ' + str(int(time)),pad = 30)\n",
    "        outerax.axis('off')\n",
    "        \n",
    "        inner_grid = outer_grid[k].subgridspec(1, nsteps)\n",
    "        for j in range(nsteps):\n",
    "            ax = fig.add_subplot(inner_grid[j])\n",
    "            for i, sig in enumerate(scenario['profile_names']):\n",
    "                ax.plot(psi,residuals[sig][j].reshape((scenario['profile_length'],)),label=sig)\n",
    "                ax.hlines(0,0,1)\n",
    "                ax.legend()\n",
    "                ax.tick_params(reset=True)\n",
    "                ax.set_title(label = 'State Residuals t+' + str(int(j*scenario['dt']*1000)))\n",
    "            fig.add_subplot(ax)\n",
    "      \n",
    "    if filename:\n",
    "        fig.savefig(filename, bbox='tight')\n",
    "        html = \"\"\"<img src=\\\"\"\"\" + filename + \"\"\"\\\"><p>\"\"\"\n",
    "        return fig, html\n",
    "    return fig \n",
    "    \n",
    "def plot_autoencoder_predictions_timestep(model,scenario,generator,shots,times, filename = None,**kwargs):\n",
    "    A,B = get_AB(model)\n",
    "    state_encoder, state_decoder, control_encoder = get_submodels(model)\n",
    "    inputs, targets, actual = generator.get_data_by_shot_time(shots,times)\n",
    "    \n",
    "    fig = plt.figure(figsize=(40,90))\n",
    "    outer_grid = fig.add_gridspec(len(times),1)\n",
    "    \n",
    "    for i, (shot,time) in enumerate(zip(actual['shots'],actual['times'])):\n",
    "        inp = {sig:arr[np.newaxis,i] for sig, arr in inputs.items()}\n",
    "        state_inputs, state_predictions, residuals = get_autoencoder_predictions(\n",
    "            state_encoder,state_decoder,control_encoder,A,B,scenario,inp,shot)\n",
    "        baseline = {k:v[0].reshape((scenario['profile_length'],)) for k,v in state_inputs.items() if k in scenario['profile_names']}\n",
    "        true = {k:v[-1].reshape((scenario['profile_length'],)) for k,v in state_inputs.items() if k in scenario['profile_names']}\n",
    "        pred = {k:v[-1].reshape((scenario['profile_length'],)) for k,v in state_predictions.items() if k in scenario['profile_names']}\n",
    "        \n",
    "        outerax = fig.add_subplot(outer_grid[i])\n",
    "        outerax.set_title(label='Shot ' + str(int(shot)) + '   Time ' + str(int(time)) + '   Prediction Window ' \n",
    "                     + str(int(scenario['lookahead']*scenario['dt']*1000)),pad = 70)\n",
    "        outerax.axis('off')\n",
    "        \n",
    "        ncols = len(scenario['profile_names'])\n",
    "        nrows = 2\n",
    "        psi = np.linspace(0,1,scenario['profile_length'])\n",
    "        inner_grid = outer_grid[i].subgridspec(nrows, ncols)\n",
    "        \n",
    "        for j, sig in enumerate(scenario['profile_names']):\n",
    "            ax = fig.add_subplot(inner_grid[j]) \n",
    "            ax.plot(psi,pred[sig]-baseline[sig],psi,true[sig]-baseline[sig])\n",
    "            ax.set_title(label = sig + ' (deltas)')\n",
    "            ax.hlines(0,0,1)\n",
    "            ax.legend(['predicted delta','true delta'])\n",
    "            \n",
    "            ax1 = fig.add_subplot(inner_grid[j+ncols]) \n",
    "            ax1.plot(psi,pred[sig],psi,true[sig],psi, baseline[sig])\n",
    "            ax1.set_title(label = sig + ' (full)')\n",
    "            ax1.legend(['predicted','true','baseline'])\n",
    "            \n",
    "            fig.add_subplot(ax)\n",
    "            fig.add_subplot(ax1)\n",
    "            \n",
    "    if filename:\n",
    "        fig.savefig(filename, bbox='tight')\n",
    "        html = \"\"\"<img src=\\\"\"\"\" + filename + \"\"\"\\\"><p>\"\"\"\n",
    "        return fig, html\n",
    "    \n",
    "    return fig        \n",
    "\n",
    "def plot_autoencoder_control_encoding(model,scenario,generator,shots,times,filename=None,**kwargs):\n",
    "    state_encoder, state_decoder, control_encoder = get_submodels(model)\n",
    "    inputs, targets, actual = generator.get_data_by_shot_time(shots,times)\n",
    "    control_inputs = {}\n",
    "    for sig in scenario['actuator_names']:\n",
    "        control_inputs['input_'+sig] = inputs['input_'+sig]\n",
    "    figsize = (20,10)\n",
    "    nrows = int(np.ceil(len(shots)/3))\n",
    "    ncols = min(len(shots),3)\n",
    "    fig, ax = plt.subplots(nrows=nrows,ncols=ncols, figsize=figsize,squeeze=False,sharey=True)\n",
    "    fig.suptitle('Control Residuals',y=.95)\n",
    "    \n",
    "    for j, (shot,time) in enumerate(zip(shots,times)):\n",
    "        inp = {sig:arr[j] for sig, arr in control_inputs.items() }\n",
    "        residuals = []\n",
    "        T = scenario['lookback'] + scenario['lookahead'] +1\n",
    "        for i in range(T):\n",
    "            temp_input = {k:v[i].reshape((1,1,1)) for k,v in inp.items()}\n",
    "            encoded_control = control_encoder.predict(temp_input)\n",
    "            residuals.append(np.squeeze(control_decoder.predict(encoded_control)))\n",
    "        residuals = {sig:np.squeeze(inp['input_'+sig]-np.array(residuals)[:,i]) for i, sig in enumerate(scenario['actuator_names'])}\n",
    "        \n",
    "        t = np.arange(time,time+(T)*scenario['dt']*1000,scenario['dt']*1000)\n",
    "        for i, sig in enumerate(scenario['actuator_names']):\n",
    "            ax[np.unravel_index(j,(nrows,ncols))].plot(t,residuals[sig], label=sig)\n",
    "            ax[np.unravel_index(j,(nrows,ncols))].hlines(0,min(t),max(t))\n",
    "            ax[np.unravel_index(j,(nrows,ncols))].tick_params(reset=True)\n",
    "            ax[np.unravel_index(j,(nrows,ncols))].legend()\n",
    "            ax[np.unravel_index(j,(nrows,ncols))].title.set_text('Shot ' + str(int(shot)) + '   Time ' + str(int(time)))\n",
    "      \n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.88)     \n",
    "\n",
    "    if filename:\n",
    "        fig.savefig(filename,bbox_inches='tight')\n",
    "        html = \"\"\"<img src=\\\"\"\"\" + filename + \"\"\"\\\"><p>\"\"\"\n",
    "        return fig, html    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# state_inputs, state_predictions, residuals = get_autoencoder_predictions(state_encoder,state_decoder,control_encoder,A,B,scenario,inputs,175702,1400)\n",
    "# f = plot_autoencoder_residuals(residuals,scenario,175702,1400)\n",
    "# f = plot_autoencoder_predictions_timestep(state_inputs, state_predictions, scenario,175702,1400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_path = '/scratch/gpfs/aaronwu/run_results_07_21_2'\n",
    "scenarios = []\n",
    "models = []\n",
    "for file in os.listdir(run_path):\n",
    "    if file.endswith(\".h5\"):\n",
    "        model_path = run_path+'/'+file\n",
    "        #model = keras.models.load_model(model_path, compile=False)\n",
    "        #models.append(model)\n",
    "        #print('loaded model: ' + model_path.split('/')[-1])\n",
    "        params_path = model_path[:-3]+'_params.pkl'\n",
    "        with open(params_path, 'rb') as f:\n",
    "            scenario = pickle.load(f, encoding='latin1')\n",
    "            scenario['dt'] = 0.05\n",
    "            scenarios.append(scenario)\n",
    "        print('loaded dict: ' + params_path.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Include in for loop if lookback/lookahead vary\n",
    "\n",
    "datapath = '/scratch/gpfs/jabbate/full_data_with_error/train_data.pkl'\n",
    "with open(datapath,'rb') as f:\n",
    "    rawdata = pickle.load(f,encoding='latin1')\n",
    "data = {163303: rawdata[163303]}\n",
    "times = [2000, 2480, 3080, 4040, 4820, 5840]\n",
    "shots = [163303]*len(times)\n",
    "traindata, valdata, normalization_dict = process_data(data,\n",
    "                                                      scenario['sig_names'],\n",
    "                                                      scenario['normalization_method'],\n",
    "                                                      scenario['window_length'],\n",
    "                                                      scenario['window_overlap'],\n",
    "                                                      scenario['lookback'],\n",
    "                                                      scenario['lookahead'],\n",
    "                                                      scenario['sample_step'],\n",
    "                                                      scenario['uniform_normalization'],\n",
    "                                                      1,\n",
    "                                                      0,\n",
    "                                                      scenario['nshots'],\n",
    "                                                      1,\n",
    "                                                      scenario['flattop_only'],\n",
    "                                                      invert_q = scenario['invert_q'],\n",
    "                                                      randomize=False)\n",
    "traindata = denormalize(traindata, normalization_dict)\n",
    "traindata = renormalize(traindata, scenario['normalization_dict'])\n",
    "generator = AutoEncoderDataGenerator(traindata,\n",
    "                                               scenario['batch_size'],\n",
    "                                               scenario['profile_names'],\n",
    "                                               scenario['actuator_names'],\n",
    "                                               scenario['scalar_names'],\n",
    "                                               scenario['lookback'],\n",
    "                                               scenario['lookahead'],\n",
    "                                               scenario['profile_downsample'],\n",
    "                                               scenario['state_latent_dim'],\n",
    "                                               scenario['discount_factor'],\n",
    "                                               scenario['x_weight'],\n",
    "                                               scenario['u_weight'],                                            \n",
    "                                               scenario['shuffle_generators'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Scenario "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(model, scenario, worksheet):\n",
    "    \n",
    "    if 'image_path' not in scenario.keys():\n",
    "        scenario['image_path'] = 'https://jabbate7.github.io/plasma-profile-predictor/results/' + scenario['runname']\n",
    "    '''\n",
    "    base_sheet_path = \"https://docs.google.com/spreadsheets/d/1h2jm3PWuck-7t_WcHi3Zm0OT35fRfLr4RrrUjHrm1dA/edit#gid=0\"\n",
    "    scope = ['https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(os.path.expanduser('~/plasma-profile-predictor/drive-credentials.json'), scope)\n",
    "    client = gspread.authorize(creds)\n",
    "    sheet = client.open_by_url(base_sheet_path).get_worksheet(worksheet)\n",
    "    \n",
    "    write_scenario_to_sheets(scenario,sheet)\n",
    "    rowid = sheet.find(scenario['runname']).row\n",
    "    scenario['sheet_path'] = base_sheet_path + \"&range={}:{}\".format(rowid,rowid)\n",
    "    '''\n",
    "    results_dir = os.path.expanduser('~/results/'+scenario['runname'])  \n",
    "    if not os.path.exists(results_dir):\n",
    "        os.mkdir(results_dir)\n",
    "    os.chdir(results_dir)\n",
    "    f = open('index.html','w+')\n",
    "    f.write('<html><head></head><body>')\n",
    "    html = scenario_to_html(scenario)\n",
    "    f.write(html + '<p>\\n')\n",
    "    _, html = plot_autoencoder_training(model,scenario, filename='training.png')\n",
    "    f.write(html + '<p>\\n')\n",
    "    \n",
    "    _, html = plot_autoencoder_AB(model,scenario, filename='AB.png')\n",
    "    f.write(html + '<p>\\n')\n",
    "    _, html = plot_autoencoder_spectrum(model,scenario, filename='spectrum.png')\n",
    "    f.write(html + '<p>\\n')\n",
    "    \n",
    "    _, html = plot_autoencoder_residuals(model,scenario,generator,shots,times, filename = 'state_residuals.png')\n",
    "    f.write(html + '<p>\\n')\n",
    "    \n",
    "    _, html = plot_autoencoder_predictions_timestep(model,scenario,generator,shots,times, filename = 'predictions.png')\n",
    "    f.write(html + '<p>\\n')\n",
    "    '''\n",
    "    _, html =  plot_autoencoder_control_encoding(model,scenario,generator,shots,times,filename='control_residuals.png')\n",
    "    f.write(html + '<p>\\n')\n",
    "    '''\n",
    "    f.write('</body></html>')\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "def write_scenario_to_sheets(scenario,sheet):\n",
    "    sheet_keys = sheet.row_values(1)\n",
    "    row = [None]*len(sheet_keys)\n",
    "    for i,key in enumerate(sheet_keys):\n",
    "        if key in scenario.keys():\n",
    "            row[i] = str(scenario[key])\n",
    "        elif key in scenario.get('history',{}):\n",
    "            row[i] = str(scenario['history'][key][-1])\n",
    "    sheet.append_row(row)\n",
    "\n",
    "def scenario_to_html(scenario):\n",
    "    foo = {k:v for k,v in scenario.items() if k not in ['history','normalization_dict','history_params']}\n",
    "    def printitems(dictObj, indent=0):\n",
    "        p=[]\n",
    "        p.append('<ul>\\n')\n",
    "        for k,v in dictObj.items():\n",
    "            if isinstance(v, dict):\n",
    "                p.append('<li><b>'+ str(k)+ '</b>: ')\n",
    "                p.append(printitems(v))\n",
    "                p.append('</li>\\n')\n",
    "            elif k in ['image_path','sheet_path']:\n",
    "                p.append(\"<a href=\\\"\" + str(v) + \"\\\">\" + str(k) + \"</a>\\n\")          \n",
    "            else:\n",
    "                p.append('<li><b>'+ str(k)+ '</b>: '+ str(v)+ '</li>\\n')\n",
    "        p.append('</ul>\\n')\n",
    "        return ''.join(p)\n",
    "    return printitems(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Worksheet 0 for vanilla, 1 for LRAN, 2 for LRAN2\n",
    "for model, scenario in zip(models, scenarios):\n",
    "    write_results(model,scenario,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_sheet_path = \"https://docs.google.com/spreadsheets/d/1h2jm3PWuck-7t_WcHi3Zm0OT35fRfLr4RrrUjHrm1dA/edit#gid=0\"\n",
    "scope = ['https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(os.path.expanduser('~/plasma-profile-predictor/drive-credentials.json'), scope)\n",
    "client = gspread.authorize(creds)\n",
    "sheet = client.open_by_url(base_sheet_path).get_worksheet(1)\n",
    "\n",
    "\n",
    "for scenario in scenarios:\n",
    "    if 'image_path' not in scenario.keys():\n",
    "        scenario['image_path'] = 'https://jabbate7.github.io/plasma-profile-predictor/results/' + scenario['runname']\n",
    "    write_scenario_to_sheets(scenario, sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Architecture Visualization\n",
    "for model,scenario in zip(models, scenarios):\n",
    "    dot = model_to_dot(model,show_shapes=True,show_layer_names=True,rankdir='TB')\n",
    "    dot.write_png('/home/aaronwu/results/'+scenario['runname']+'/architecture.png')\n",
    "    #display(dot.create_png())\n",
    "    #print('\\n'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
