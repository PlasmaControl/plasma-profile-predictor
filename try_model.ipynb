{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "\n",
    "from keras.models import Model\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import sys\n",
    "import time\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import helpers\n",
    "from helpers.data_generator import process_data, AutoEncoderDataGenerator, DataGenerator\n",
    "from helpers.custom_losses import normed_mse, mean_diff_sum_2, max_diff_sum_2, mean_diff2_sum2, max_diff2_sum2, denorm_loss, hinge_mse_loss, percent_correct_sign, baseline_MAE\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from time import strftime, localtime\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import copy\n",
    "from tqdm import tqdm_notebook\n",
    "from helpers.normalization import normalize, denormalize, renormalize\n",
    "import scipy\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import Image, display\n",
    "from helpers.custom_init import downsample\n",
    "from helpers.custom_reg import groupLasso\n",
    "import helpers\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching files at /home/aiqtidar/run_results_04_10 are : final_model-autoencoder_SET-dense_SDT-dense_CET-dense_CDT-dense_profiles-dens-temp-q_EFIT01-rotation-press_EFIT01_act-target_density-pinj-tinj-curr_target_LB-0_LA-6_10Apr21-17-25_Scenario-0_params.pkl\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.expanduser('/home/aiqtidar/run_results_04_10/'))\n",
    "files = [foo for foo in os.listdir() if '0_params.pkl' in foo]\n",
    "print(\"Matching files at {} are : {}\".format(os.getcwd(),files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios_dict = {}\n",
    "\n",
    "base_path = '/home/aiqtidar/run_results_04_10/'\n",
    "\n",
    "# model_name = 'final_model-autoencoder_SET-dense_SDT-dense_CET-dense_CDT-dense_profiles-temp-dens-ffprime_EFIT02-press_EFIT02-q_EFIT02_act-pinj-curr-tinj-gasA_LB-0_LA-6_09Apr21-01-53_Scenario-'\n",
    "# model_extension = '.h5'\n",
    "\n",
    "scenario_name = 'final_model-autoencoder_SET-dense_SDT-dense_CET-dense_CDT-dense_profiles-temp-dens-ffprime_EFIT02-press_EFIT02-q_EFIT02_act-pinj-curr-tinj-gasA_LB-0_LA-6_09Apr21-02-50_Scenario-'\n",
    "scenario_extension = '_params.pkl'\n",
    "# scenario_path = base_path + scenario_name + str(number) + scenario_extension\n",
    "\n",
    "for number in range(0,8):\n",
    "    files = [foo for foo in os.listdir() if (str(number) + scenario_extension) in foo]\n",
    "    scenario_path = base_path + files[0]\n",
    "    with open(scenario_path, 'rb') as f:\n",
    "        scenario = pickle.load(f, encoding='latin1')\n",
    "    scenarios_dict[number] = scenario\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios_dict[0]['state_encoder_kwargs']['layer_scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_u_residual_loss', 'val_x_residual_loss', 'val_linear_system_residual_loss', 'val_u_residual_mean_squared_error', 'val_u_residual_mean_absolute_error', 'val_x_residual_mean_squared_error', 'val_x_residual_mean_absolute_error', 'val_linear_system_residual_mean_squared_error', 'val_linear_system_residual_mean_absolute_error', 'loss', 'u_residual_loss', 'x_residual_loss', 'linear_system_residual_loss', 'u_residual_mean_squared_error', 'u_residual_mean_absolute_error', 'x_residual_mean_squared_error', 'x_residual_mean_absolute_error', 'linear_system_residual_mean_squared_error', 'linear_system_residual_mean_absolute_error', 'lr', 'end_times', 'epoch_times'])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios_dict[0]['history'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00025391113712171394"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios_dict[0]['history']['loss'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "## Run Results 04_08\n",
    "\n",
    "### Summary :\n",
    "- Model 0 for x_weight, u_weight = 1,1\n",
    "- Models 16 and 20 for x_weight, u_weight = 1, 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "Group models together based on u-weight and x-weight. There are two possibilities:\n",
    "\n",
    "[{'x_weight':1, 'u_weight':1}, {'x_weight':1, 'u_weight':0.1}]\n",
    "\n",
    "Errors are directly comparable within the two, since the loss functions are identical.\n",
    "\n",
    "Metrics that are evaluated are\n",
    "\n",
    "- 1: abs(train/val) -> The smaller this number the better\n",
    "- 2: min(loss) -> Minimum training loss\n",
    "- 3: min(val_loss) -> Minimum val loss\n",
    "- 4: loss[-1] -> Last training loss\n",
    "- 5: val_loss[-1] -> Last val loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 'h': \n",
      "\n",
      "min_val_loss_h = 0 \n",
      " min_train_loss_h = 0 \n",
      " fin_val_loss_h = 0 \n",
      " fin_train_loss_h = 0 \n",
      " min_train/val = [0.00277504 0.00098954 0.01542861 0.01880641 0.00082989 0.00095373\n",
      " 0.04452105 0.0137315  0.00087665 0.00097745 0.02293515 0.00313365\n",
      " 0.00090233 0.00098387 0.02083868 0.00244364] \n",
      "\n",
      "Best model is thus: 0 \n",
      "\n",
      "Results for 'l': \n",
      "\n",
      "min_val_loss_l = 12 \n",
      " min_train_loss_l = 12 \n",
      " fin_val_loss_l = 12 \n",
      " fin_train_loss_l = 12 \n",
      " min_train/val = [  0.15495161   8.41870202   0.20352213   0.73625215 135.57701437\n",
      " 139.08616135   2.28790087   5.41955359 101.17040954   8.54937671\n",
      "   0.75244605   0.17104445 148.29704355   8.45137169   0.53474846\n",
      "   2.6811059 ] \n",
      "\n",
      "Best models for l are thus: 16 and 20\n"
     ]
    }
   ],
   "source": [
    "u_weight_l = {}\n",
    "u_weight_h = {}\n",
    "\n",
    "indices_l = []\n",
    "min_val_loss_l = []\n",
    "min_train_loss_l = []\n",
    "fin_val_loss_l = []\n",
    "fin_train_loss_l = []\n",
    "\n",
    "\n",
    "indices_h = []\n",
    "min_val_loss_h = []\n",
    "min_train_loss_h = []\n",
    "fin_val_loss_h = []\n",
    "fin_train_loss_h = []\n",
    "\n",
    "\n",
    "for i in range(0,len(scenarios_dict)):\n",
    "    if scenarios_dict[i]['u_weight'] < 1:\n",
    "        indices_l.append(i)\n",
    "        min_train_loss_l.append(np.min(scenarios_dict[i]['history']['loss']))\n",
    "        min_val_loss_l.append(np.min(scenarios_dict[i]['history']['val_loss']))\n",
    "        fin_val_loss_l.append(scenarios_dict[i]['history']['val_loss'][-1])\n",
    "        fin_train_loss_l.append(scenarios_dict[i]['history']['loss'][-1])\n",
    "    else:\n",
    "        indices_h.append(i)\n",
    "        min_train_loss_h.append(np.min(scenarios_dict[i]['history']['loss']))\n",
    "        min_val_loss_h.append(np.min(scenarios_dict[i]['history']['val_loss']))\n",
    "        fin_val_loss_h.append(scenarios_dict[i]['history']['val_loss'][-1])\n",
    "        fin_train_loss_h.append(scenarios_dict[i]['history']['loss'][-1])\n",
    "\n",
    "\n",
    "min_val_loss_l = np.array(min_val_loss_l)\n",
    "min_train_loss_l = np.array(min_train_loss_l)\n",
    "fin_val_loss_l = np.array(fin_val_loss_l)\n",
    "fin_train_loss_l = np.array(fin_train_loss_l)\n",
    "\n",
    "min_val_loss_h = np.array(min_val_loss_h)\n",
    "min_train_loss_h = np.array(min_train_loss_h)\n",
    "fin_val_loss_h = np.array(fin_val_loss_h)\n",
    "fin_train_loss_h = np.array(fin_train_loss_h)\n",
    "        \n",
    "print(\"Results for 'h': \\n\")\n",
    "print(\"min_val_loss_h = {} \\n min_train_loss_h = {} \\n fin_val_loss_h = {} \\n fin_train_loss_h = {} \\n min_train/val = {} \\n\".format(np.argmin(min_train_loss_h),np.argmin(min_val_loss_h),np.argmin(fin_val_loss_h),np.argmin(fin_train_loss_h), (np.abs(1 - fin_train_loss_h/fin_val_loss_h))))\n",
    "print(\"Best model is thus: {} \\n\".format(indices_h[0]))\n",
    "\n",
    "print(\"Results for 'l': \\n\")\n",
    "print(\"min_val_loss_l = {} \\n min_train_loss_l = {} \\n fin_val_loss_l = {} \\n fin_train_loss_l = {} \\n min_train/val = {} \\n\".format(np.argmin(min_train_loss_l),np.argmin(min_val_loss_l),np.argmin(fin_val_loss_l),np.argmin(fin_train_loss_l), (np.abs(1 - fin_train_loss_h/fin_val_loss_l))))\n",
    "print(\"Best models for l are thus: {} and {}\".format(indices_l[0], indices_l[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for h, Model 0 seems to be optimal\n",
    "\n",
    "for l, Models 16 and 20 are interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00025321, 0.02429986, 0.00513845, 0.00348803, 0.02422663,\n",
       "       0.02443801, 0.00521306, 0.00494815, 0.02437353, 0.02430904,\n",
       "       0.00892593, 0.00968264, 0.02436592, 0.02423983, 0.00625088,\n",
       "       0.01731732])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_val_loss_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios_dict[20];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Results 04_10\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- For encoder layer scale = 2, the best model is 7\n",
    "- For encoder layer scale = 1, the best model is 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "Sort by layer scale\n",
    "\n",
    "Metrics that are evaluated are\n",
    "\n",
    "- 1: abs(train/val) -> The smaller this number the better\n",
    "- 2: min(loss) -> Minimum training loss\n",
    "- 3: min(val_loss) -> Minimum val loss\n",
    "- 4: loss[-1] -> Last training loss\n",
    "- 5: val_loss[-1] -> Last val loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 'h': \n",
      "\n",
      "min_val_loss_h = 3 \n",
      " min_train_loss_h = 3 \n",
      " fin_val_loss_h = 3 \n",
      " fin_train_loss_h = 3 \n",
      " min_train/val = [0.00566873 0.00410943 0.00908795 0.00797236] \n",
      "\n",
      "Best model is thus: 7 \n",
      "\n",
      "Results for 'l': \n",
      "\n",
      "min_val_loss_l = 1 \n",
      " min_train_loss_l = 1 \n",
      " fin_val_loss_l = 1 \n",
      " fin_train_loss_l = 1 \n",
      " min_train/val = [0.47358025 0.52318257 0.063628   0.13391871] \n",
      "\n",
      "Best models for l are thus: 2\n"
     ]
    }
   ],
   "source": [
    "u_weight_l = {}\n",
    "u_weight_h = {}\n",
    "\n",
    "indices_l = []\n",
    "min_val_loss_l = []\n",
    "min_train_loss_l = []\n",
    "fin_val_loss_l = []\n",
    "fin_train_loss_l = []\n",
    "\n",
    "\n",
    "indices_h = []\n",
    "min_val_loss_h = []\n",
    "min_train_loss_h = []\n",
    "fin_val_loss_h = []\n",
    "fin_train_loss_h = []\n",
    "\n",
    "\n",
    "for i in range(0,len(scenarios_dict)):\n",
    "    if scenarios_dict[i]['state_encoder_kwargs']['layer_scale'] < 2:\n",
    "        indices_l.append(i)\n",
    "        min_train_loss_l.append(np.min(scenarios_dict[i]['history']['loss']))\n",
    "        min_val_loss_l.append(np.min(scenarios_dict[i]['history']['val_loss']))\n",
    "        fin_val_loss_l.append(scenarios_dict[i]['history']['val_loss'][-1])\n",
    "        fin_train_loss_l.append(scenarios_dict[i]['history']['loss'][-1])\n",
    "    else:\n",
    "        indices_h.append(i)\n",
    "        min_train_loss_h.append(np.min(scenarios_dict[i]['history']['loss']))\n",
    "        min_val_loss_h.append(np.min(scenarios_dict[i]['history']['val_loss']))\n",
    "        fin_val_loss_h.append(scenarios_dict[i]['history']['val_loss'][-1])\n",
    "        fin_train_loss_h.append(scenarios_dict[i]['history']['loss'][-1])\n",
    "\n",
    "\n",
    "min_val_loss_l = np.array(min_val_loss_l)\n",
    "min_train_loss_l = np.array(min_train_loss_l)\n",
    "fin_val_loss_l = np.array(fin_val_loss_l)\n",
    "fin_train_loss_l = np.array(fin_train_loss_l)\n",
    "\n",
    "min_val_loss_h = np.array(min_val_loss_h)\n",
    "min_train_loss_h = np.array(min_train_loss_h)\n",
    "fin_val_loss_h = np.array(fin_val_loss_h)\n",
    "fin_train_loss_h = np.array(fin_train_loss_h)\n",
    "        \n",
    "print(\"Results for 'h': \\n\")\n",
    "print(\"min_val_loss_h = {} \\n min_train_loss_h = {} \\n fin_val_loss_h = {} \\n fin_train_loss_h = {} \\n min_train/val = {} \\n\".format(np.argmin(min_train_loss_h),np.argmin(min_val_loss_h),np.argmin(fin_val_loss_h),np.argmin(fin_train_loss_h), (np.abs(1 - fin_train_loss_h/fin_val_loss_h))))\n",
    "print(\"Best model is thus: {} \\n\".format(indices_h[3]))\n",
    "\n",
    "print(\"Results for 'l': \\n\")\n",
    "print(\"min_val_loss_l = {} \\n min_train_loss_l = {} \\n fin_val_loss_l = {} \\n fin_train_loss_l = {} \\n min_train/val = {} \\n\".format(np.argmin(min_train_loss_l),np.argmin(min_val_loss_l),np.argmin(fin_val_loss_l),np.argmin(fin_train_loss_l), (np.abs(1 - fin_train_loss_h/fin_val_loss_l))))\n",
    "print(\"Best models for l are thus: {}\".format(indices_l[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
