{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### imports\n",
    "import numpy as np\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# keras2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "code_folding": [
     0,
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### array2c\n",
    "def array2c(array,name):\n",
    "    temp = array.flatten(order='C')\n",
    "    size = array.size\n",
    "    count = 0\n",
    "    s = 'float ' + name + '[' + str(size) + '] = \\n{'\n",
    "    for i in range(size):\n",
    "        if temp[i] == np.inf:\n",
    "            s += \"HUGE_VAL,\"\n",
    "        elif temp[i] == -np.inf:\n",
    "            s += \"-HUGE_VAL,\"\n",
    "        else:\n",
    "            s += \"{:.16e}\".format(temp[i]) + ','\n",
    "        count += 1\n",
    "        if (count)%3 is 0:\n",
    "            s += '\\n'\n",
    "    s += '}; \\n'\n",
    "    return s    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "code_folding": [
     0,
     60,
     99,
     166,
     218,
     266,
     333,
     379,
     388,
     416
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### weights2c\n",
    "\n",
    "def write_weights_LSTM(layer,file, model_io):\n",
    "    weights = layer.get_weights()\n",
    "    units = layer.get_config()['units'] \n",
    "    \n",
    "    inputs, outputs = get_layer_io_names(layer)\n",
    "    for i,(inp,outp) in enumerate(zip(inputs,outputs)):\n",
    "        inshp = layer.get_input_at(i).shape[1:]\n",
    "        outshp = layer.get_output_at(i).shape[1:]\n",
    "        insize = np.prod(inshp)\n",
    "        outsize = np.prod(outshp)\n",
    "        if len(outshp)>1:\n",
    "            outrows = outshp[0]\n",
    "            outcols = outshp[1]\n",
    "        else:\n",
    "            outrows=1\n",
    "            outcols = outshp[0]\n",
    "        if len(inshp)>1:\n",
    "            inrows = inshp[0]\n",
    "            incols = inshp[1]\n",
    "        else:\n",
    "            inrows=1\n",
    "            incols = inshp[0]\n",
    "\n",
    "        s = 'size_t ' + layer.name + '_in' + str(i) + '_height = ' + str(inrows) + ';\\n'\n",
    "        s += 'size_t ' + layer.name + '_in' + str(i) + '_width = ' + str(incols) + ';\\n'\n",
    "        if outp not in model_io[1]:\n",
    "            s += 'float ' + outp + '_output[' + str(units) + '] = {0}; \\n \\n'\n",
    "        file.write(s)\n",
    "\n",
    "    s = 'size_t ' + layer.name + '_units = ' + str(units) + ';\\n'\n",
    "    s += 'float ' + layer.name + '_fwork[' + str(8*units) + '] = {0}; \\n'\n",
    "    s += 'float ' + layer.name + '_state[' + str(2*units) + '] = {0}; \\n'\n",
    "    file.write(s)\n",
    "\n",
    "    kernel = weights[0]\n",
    "    recurrent_kernel = weights[1]\n",
    "    if layer.get_config()['use_bias']:\n",
    "        bias = weights[2]\n",
    "    else:\n",
    "        bias = np.zeros(4*units)\n",
    "    \n",
    "    Wi = kernel[:,:units]\n",
    "    Wf = kernel[:,units:2*units]\n",
    "    Wc = kernel[:,2*units:3*units]\n",
    "    Wo = kernel[:,3*units:]\n",
    "    ckernel = np.concatenate([Wi,Wf,Wc,Wo],axis=0)\n",
    "    Ui = recurrent_kernel[:,:units]\n",
    "    Uf = recurrent_kernel[:,units:2*units]\n",
    "    Uc = recurrent_kernel[:,2*units:3*units]\n",
    "    Uo = recurrent_kernel[:,3*units:]\n",
    "    crecurrent_kernel = np.concatenate([Ui,Uf,Uc,Uo],axis=0)\n",
    "\n",
    "  \n",
    "    file.write(array2c(ckernel,layer.name + '_kernel'))\n",
    "    file.write(array2c(crecurrent_kernel,layer.name + '_recurrent_kernel'))\n",
    "    file.write(array2c(bias,layer.name + '_bias'))\n",
    "    file.write('\\n \\n')\n",
    "        \n",
    "def write_weights_Dense(layer,file, model_io):\n",
    "    weights = layer.get_weights()\n",
    "    \n",
    "    inputs, outputs = get_layer_io_names(layer)\n",
    "    for i,(inp,outp) in enumerate(zip(inputs,outputs)):\n",
    "        inshp = layer.get_input_at(i).shape[1:]\n",
    "        outshp = layer.get_output_at(i).shape[1:]\n",
    "        insize = np.prod(inshp)\n",
    "        outsize = np.prod(outshp)\n",
    "        if len(outshp)>1:\n",
    "            outrows = outshp[0]\n",
    "            outcols = outshp[1]\n",
    "        else:\n",
    "            outrows=1\n",
    "            outcols = outshp[0]\n",
    "        if len(inshp)>1:\n",
    "            inrows = inshp[0]\n",
    "            incols = inshp[1]\n",
    "        else:\n",
    "            inrows=1\n",
    "            incols = inshp[0]\n",
    "\n",
    "        s = 'size_t ' + layer.name + '_out' + str(i) + 'rows =' + str(outrows) + ';\\n'\n",
    "        s += 'size_t ' + layer.name + '_out' + str(i) + 'cols =' + str(outcols) + ';\\n'\n",
    "        s += 'size_t ' + layer.name + '_inner' + str(i) + 'dim =' + str(incols) + ';\\n'\n",
    "        if outp not in model_io[1]:\n",
    "            s += 'float ' + outp + '_output[' + str(outsize) + '] = {0}; \\n \\n'\n",
    "        file.write(s)\n",
    "    \n",
    "    A = weights[0]\n",
    "    if layer.get_config()['use_bias']:\n",
    "        b = weights[1]\n",
    "    else:\n",
    "        b = np.zeros(A.shape[1])\n",
    "\n",
    "    file.write(array2c(A,layer.name + '_kernel'))\n",
    "    file.write(array2c(b,layer.name + '_bias'))\n",
    "    file.write('\\n \\n')\n",
    "\n",
    "def write_weights_Conv1D(layer,file, model_io):\n",
    "    weights = layer.get_weights()\n",
    "    filters = weights[0]\n",
    "    if layer.get_config()['use_bias']:\n",
    "        bias = weights[1]\n",
    "    else:\n",
    "        bias = np.zeros(filters.shape[2])\n",
    "    pad = layer.get_config()['padding']\n",
    "    stride = layer.get_config()['strides'][0]\n",
    "    dilation = layer.get_config()['dilation_rate'][0]\n",
    "    kernel_size = layer.get_config()['kernel_size'][0]\n",
    "    num_filters = layer.get_config()['filters']\n",
    "    s = 'size_t ' + layer.name + '_stride = ' + str(stride) + '; \\n'\n",
    "    s += 'size_t ' + layer.name + '_dilation = ' + str(dilation) + '; \\n'\n",
    "    s += 'size_t ' + layer.name + '_kernel_size = ' + str(kernel_size) + '; \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "    \n",
    "    inputs, outputs = get_layer_io_names(layer)\n",
    "    for i,(inp,outp) in enumerate(zip(inputs,outputs)):\n",
    "        inshp = layer.get_input_at(i).shape[1:]\n",
    "        outshp = layer.get_output_at(i).shape[1:]\n",
    "        insize = np.prod(inshp)\n",
    "        outsize = np.prod(outshp)\n",
    "        if len(outshp)>1:\n",
    "            outrows = outshp[0]\n",
    "            outcols = outshp[1]\n",
    "        else:\n",
    "            outrows=1\n",
    "            outcols = outshp[0]\n",
    "        if len(inshp)>1:\n",
    "            inrows = inshp[0]\n",
    "            incols = inshp[1]\n",
    "        else:\n",
    "            inrows=1\n",
    "            incols = inshp[0]\n",
    "        if pad == 'causal':\n",
    "            pad_along_height = dilation*(kernel_size-1)\n",
    "            pad_top = pad_along_height\n",
    "            pad_bottom = 0\n",
    "        elif pad == 'same':\n",
    "            pad_along_height = max((outshp[0] - 1) * stride*dilation +\n",
    "                    kernel_size - inshp[0], 0)\n",
    "            pad_top = int(pad_along_height // 2)\n",
    "            pad_bottom = int(pad_along_height - pad_top)\n",
    "        elif pad == 'valid':\n",
    "            pad_top=0\n",
    "            pad_bottom=0\n",
    "\n",
    "        s = 'size_t ' + layer.name + '_pad' + str(i) + '_top = ' + str(pad_top) + '; \\n'\n",
    "        s += 'size_t ' + layer.name + '_pad' + str(i) + '_bottom = ' + str(pad_bottom) + '; \\n'\n",
    "        s += 'size_t ' + layer.name + '_in' + str(i) + '_height = ' + str(inrows) + '; \\n'\n",
    "        s += 'size_t ' + layer.name + '_padded_in' + str(i) + '_height = ' + str(inrows + pad_top + pad_bottom) + '; \\n'\n",
    "        s += 'size_t ' + layer.name + '_in' + str(i) + '_width = ' + str(incols) + '; \\n'\n",
    "        s += 'size_t ' + layer.name + '_out' + str(i) + '_height = ' + str(outrows) + '; \\n'\n",
    "        s += 'size_t ' + layer.name + '_out' + str(i) + '_width = ' + str(outcols) + '; \\n'\n",
    "        s += 'float ' + layer.name + '_padded' + str(i) + '_input[' + str((inrows + pad_top + pad_bottom)*incols) + \\\n",
    "            '] = {0}; \\n'\n",
    "        s += 'float ' + layer.name + '_fill' + str(i) + '[' + str(incols) + '] = {0}; \\n'\n",
    "        if outp not in model_io[1]:\n",
    "            s += 'float ' + outp + '_output[' + str(outsize) + '] = {0}; \\n \\n'\n",
    "        file.write(s)\n",
    " \n",
    "    file.write(array2c(filters, layer.name + '_kernel'))\n",
    "    file.write(array2c(bias, layer.name + '_bias'))\n",
    "    file.write('\\n \\n')\n",
    "\n",
    "def write_weights_Pooling1D(layer,file, model_io):\n",
    "    pad = layer.get_config()['padding']\n",
    "    stride = layer.get_config()['strides'][0]\n",
    "    pool_size = layer.get_config()['pool_size'][0]\n",
    "    s = 'size_t ' + layer.name + '_stride = ' + str(stride) + '; \\n'\n",
    "    s += 'size_t ' + layer.name + '_pool_size = ' + str(pool_size) + '; \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "    inputs, outputs = get_layer_io_names(layer)\n",
    "    for i,(inp,outp) in enumerate(zip(inputs,outputs)):\n",
    "        inshp = layer.get_input_at(i).shape[1:]\n",
    "        outshp = layer.get_output_at(i).shape[1:]\n",
    "\n",
    "        insize = np.prod(inshp)\n",
    "        outsize = np.prod(outshp)\n",
    "        if len(outshp)>1:\n",
    "            outrows = outshp[0]\n",
    "            outcols = outshp[1]\n",
    "        else:\n",
    "            outrows=1\n",
    "            outcols = outshp[0]\n",
    "        if len(inshp)>1:\n",
    "            inrows = inshp[0]\n",
    "            incols = inshp[1]\n",
    "        else:\n",
    "            inrows=1\n",
    "            incols = inshp[0]\n",
    "\n",
    "        if pad == 'same':\n",
    "            pad_along_height = max((outshp[0] - 1) * stride +\n",
    "                    pool_size - inshp[0], 0)\n",
    "            pad_top = int(pad_along_height // 2)\n",
    "            pad_bottom = int(pad_along_height - pad_top)\n",
    "        elif pad == 'valid':\n",
    "            pad_top=0\n",
    "            pad_bottom=0 \n",
    "\n",
    "        s = 'size_t ' + layer.name + '_pad' + str(i) + '_top = ' + str(pad_top) + '; \\n'\n",
    "        s += 'size_t ' + layer.name + '_pad' + str(i) + '_bottom = ' + str(pad_bottom) + '; \\n'\n",
    "        s += 'size_t ' + layer.name + '_in' + str(i) + '_height = ' + str(inrows) + '; \\n'\n",
    "        s += 'size_t ' + layer.name + '_padded_in' + str(i) + '_height = ' + str(inrows + pad_top + pad_bottom) + '; \\n'\n",
    "        s += 'size_t ' + layer.name + '_in' + str(i) + '_width = ' + str(incols) + '; \\n'\n",
    "        s += 'size_t ' + layer.name + '_out' + str(i) + '_height = ' + str(outrows) + '; \\n'\n",
    "        s += 'size_t ' + layer.name + '_out' + str(i) + '_width = ' + str(outcols) + '; \\n'\n",
    "        s += 'float ' + layer.name + '_padded' + str(i) + '_input[' + str((inrows + pad_top + pad_bottom)*incols) + \\\n",
    "            '] = {0}; \\n'\n",
    "        s += array2c(-1*np.ones(incols)*np.inf, layer.name + '_fill' + str(i) + '')\n",
    "        if outp not in model_io[1]:\n",
    "            s += 'float ' + outp + '_output[' + str(outsize) + '] = {0}; \\n \\n'\n",
    "        file.write(s)\n",
    "    file.write('\\n \\n')\n",
    "    \n",
    "def write_weights_GlobalPooling1D(layer,file, model_io):\n",
    "    inputs, outputs = get_layer_io_names(layer)\n",
    "    for i,(inp,outp) in enumerate(zip(inputs,outputs)):\n",
    "        inshp = layer.get_input_at(i).shape[1:]\n",
    "        outshp = layer.get_output_at(i).shape[1:]\n",
    "        insize = np.prod(inshp)\n",
    "        outsize = np.prod(outshp)\n",
    "        if len(outshp)>1:\n",
    "            outrows = outshp[0]\n",
    "            outcols = outshp[1]\n",
    "        else:\n",
    "            outrows=1\n",
    "            outcols = outshp[0]\n",
    "        if len(inshp)>1:\n",
    "            inrows = inshp[0]\n",
    "            incols = inshp[1]\n",
    "        else:\n",
    "            inrows=1\n",
    "            incols = inshp[0]\n",
    "   \n",
    "        s = 'size_t ' + layer.name + '_in' + str(i) + '_height = ' + str(inrows) + '; \\n'\n",
    "        s += 'size_t ' + layer.name + '_in' + str(i) + '_width = ' + str(incols) + '; \\n'\n",
    "        if outp not in model_io[1]:\n",
    "            s += 'float ' + outp + '_output[' + str(outsize) + '] = {0}; \\n \\n'\n",
    "        file.write(s)\n",
    "\n",
    "    file.write('\\n\\n')   \n",
    "\n",
    "def write_weights_Merge(layer,file, model_io):\n",
    "    inputs, outputs = get_layer_io_names(layer)\n",
    "    for i,(inp,outp) in enumerate(zip(inputs,outputs)):\n",
    "        outshp = layer.get_output_at(i).shape[1:]\n",
    "        if len(outshp)>1:\n",
    "            outrows = outshp[0]\n",
    "            outcols = outshp[1]\n",
    "        else:\n",
    "            outrows=1\n",
    "            outcols = outshp[0]\n",
    "        numels = np.prod(outshp)\n",
    "        num_tensors = len(inp)\n",
    "    \n",
    "        s = 'size_t ' + layer.name + '_numels' + str(i) + ' = ' + str(numels) + '; \\n'\n",
    "        s += 'size_t ' + layer.name + '_num_tensors' + str(i) + ' = ' + str(num_tensors) + '; \\n'\n",
    "        if outp not in model_io[1]:\n",
    "            s += 'float ' + outp + '_output[' + str(numels) + '] = {0}; \\n \\n'\n",
    "        file.write(s)\n",
    "    file.write('\\n\\n')\n",
    "    \n",
    "def write_weights_GRU(layer,file, model_io):\n",
    "    weights = layer.get_weights()\n",
    "    units = layer.get_config()['units']\n",
    "    if layer.get_config()['reset_after']:\n",
    "        reset_after = 1\n",
    "    else:\n",
    "        reset_after = 0    \n",
    "\n",
    "    s = 'size_t ' + layer.name + '_units = ' + str(units) + ';\\n'\n",
    "    s += 'float ' + layer.name + '_fwork[' + str(6*units) + '] = {0}; \\n'\n",
    "    s += 'int ' + layer.name + '_reset_after = ' + str(reset_after) + ';\\n'\n",
    "    s += 'float ' + layer.name + '_state[' + str(units) + '] = {0}; \\n'\n",
    "    file.write(s)\n",
    "       \n",
    "    inputs, outputs = get_layer_io_names(layer)\n",
    "    for i,(inp,outp) in enumerate(zip(inputs,outputs)):\n",
    "        inshp = layer.get_input_at(i).shape[1:]\n",
    "        outshp = layer.get_output_at(i).shape[1:]\n",
    "        insize = np.prod(inshp)\n",
    "        outsize = np.prod(outshp)\n",
    "        if len(outshp)>1:\n",
    "            outrows = outshp[0]\n",
    "            outcols = outshp[1]\n",
    "        else:\n",
    "            outrows=1\n",
    "            outcols = outshp[0]\n",
    "        if len(inshp)>1:\n",
    "            inrows = inshp[0]\n",
    "            incols = inshp[1]\n",
    "        else:\n",
    "            inrows=1\n",
    "            incols = inshp[0] \n",
    "        \n",
    "        s = 'size_t ' + layer.name + '_in' + str(i) + '_height = ' + str(inrows) + ';\\n'\n",
    "        s += 'size_t ' + layer.name + '_in' + str(i) + '_width = ' + str(incols) + ';\\n'\n",
    "        if outp not in model_io[1]:\n",
    "            s += 'float ' + outp + '_output[' + str(units) + '] = {0}; \\n \\n'\n",
    "        file.write(s)\n",
    "    \n",
    "    kernel = weights[0]\n",
    "    recurrent_kernel = weights[1]\n",
    "    if layer.get_config()['use_bias']:\n",
    "        bias = weights[2]\n",
    "        if layer.get_config()['reset_after']:\n",
    "            bias = b[0]\n",
    "            rbias = b[1]\n",
    "        else:\n",
    "            bias = bias\n",
    "            rbias = np.zeros(3*units)\n",
    "    else:\n",
    "        bias = np.zeros(3*units)\n",
    "        rbias = np.zeros(3*units)\n",
    "    bias = np.concatenate([bias,rbias],axis=0)\n",
    "    Wz = kernel[:,:units]\n",
    "    Wr = kernel[:,units:2*units]\n",
    "    Wh = kernel[:,2*units:]\n",
    "    ckernel = np.concatenate([Wz,Wr,Wh],axis=0)\n",
    "    Uz = recurrent_kernel[:,:units]\n",
    "    Ur = recurrent_kernel[:,units:2*units]\n",
    "    Uh = recurrent_kernel[:,2*units:3*units]\n",
    "    crecurrent_kernel = np.concatenate([Uz,Ur,Uh],axis=0)\n",
    "\n",
    "    file.write(array2c(ckernel,layer.name + '_kernel'))\n",
    "    file.write(array2c(crecurrent_kernel,layer.name + '_recurrent_kernel'))\n",
    "    file.write(array2c(bias,layer.name + '_bias'))\n",
    "    file.write('\\n \\n')    \n",
    "\n",
    "def write_weights_SimpleRNN(layer,file, model_io):\n",
    "    weights = layer.get_weights()\n",
    "    units = layer.get_config()['units']\n",
    "\n",
    "    s = 'size_t ' + layer.name + '_units = ' + str(units) + ';\\n'\n",
    "    s += 'float ' + layer.name + '_fwork[' + str(2*units) + '] = {0}; \\n'\n",
    "    s += 'float ' + layer.name + '_state[' + str(units) + '] = {0}; \\n'\n",
    "    file.write(s)\n",
    "\n",
    "    inputs, outputs = get_layer_io_names(layer)\n",
    "    for i,(inp,outp) in enumerate(zip(inputs,outputs)):\n",
    "        inshp = layer.get_input_at(i).shape[1:]\n",
    "        outshp = layer.get_output_at(i).shape[1:]\n",
    "        insize = np.prod(inshp)\n",
    "        outsize = np.prod(outshp)\n",
    "        if len(outshp)>1:\n",
    "            outrows = outshp[0]\n",
    "            outcols = outshp[1]\n",
    "        else:\n",
    "            outrows=1\n",
    "            outcols = outshp[0]\n",
    "        if len(inshp)>1:\n",
    "            inrows = inshp[0]\n",
    "            incols = inshp[1]\n",
    "        else:\n",
    "            inrows=1\n",
    "            incols = inshp[0]   \n",
    "            \n",
    "        s = 'size_t ' + layer.name + '_in' + str(i) + '_height = ' + str(inrows) + ';\\n'\n",
    "        s += 'size_t ' + layer.name + '_in' + str(i) + '_width = ' + str(incols) + ';\\n'\n",
    "        if outp not in model_io[1]:\n",
    "            s += 'float ' + outp + '_output[' + str(units) + '] = {0}; \\n \\n'\n",
    "        file.write(s)    \n",
    "   \n",
    "    kernel = weights[0]\n",
    "    recurrent_kernel = weights[1]\n",
    "    if layer.get_config()['use_bias']:\n",
    "        bias = weights[2]\n",
    "    else:\n",
    "        bias = np.zeros(units)\n",
    "\n",
    "    file.write(array2c(kernel,layer.name + '_kernel'))\n",
    "    file.write(array2c(recurrent_kernel,layer.name + '_recurrent_kernel'))\n",
    "    file.write(array2c(bias,layer.name + '_bias'))\n",
    "    file.write('\\n \\n')    \n",
    "            \n",
    "def write_weights_Activation(layer,file, model_io):\n",
    "    inputs, outputs = get_layer_io_names(layer)\n",
    "    for i,(inp,outp) in enumerate(zip(inputs,outputs)):\n",
    "        outshp = layer.get_output_at(i).shape[1:]\n",
    "        numels = np.prod(outshp)\n",
    "        s = 'size_t ' + layer.name + '_size' + str(i) + ' = ' + str(numels) + '; \\n'\n",
    "        file.write(s)\n",
    "    file.write('\\n\\n')\n",
    "\n",
    "def write_weights_AdvancedActivation(layer,file, model_io):\n",
    "    inputs, outputs = get_layer_io_names(layer)\n",
    "    s = ''\n",
    "    for i,(inp,outp) in enumerate(zip(inputs,outputs)):\n",
    "        outshp = layer.get_output_at(i).shape[1:]\n",
    "        numels = np.prod(outshp)\n",
    "        s += 'size_t ' + layer.name + '_size' + str(i) + ' = ' + str(numels) + '; \\n'\n",
    "        \n",
    "    if layer_type(layer) in ['LeakyReLU','ELU']:\n",
    "        alpha = layer.get_config()['alpha']\n",
    "        s += 'float ' + layer.name + '_alpha = ' + str(alpha) + '; \\n'\n",
    "    if layer_type(layer) == 'ThresholdedReLU':\n",
    "        theta = layer.get_config()['theta']\n",
    "        s += 'float ' + layer.name + '_theta = ' + str(theta) + '; \\n'\n",
    "    if layer_type(layer) == 'ReLU':\n",
    "        max_value = layer.get_config()['max_value']\n",
    "        negative_slope = layer.get_config()['negative_slope']\n",
    "        threshold = layer.get_config()['threshold']\n",
    "        if max_value is None:\n",
    "            max_value = 'HUGE_VAL'\n",
    "        s += 'float ' + layer.name + '_max_value = ' + str(max_value) + '; \\n'\n",
    "        s += 'float ' + layer.name + '_negative_slope = ' + str(negative_slope) + '; \\n'\n",
    "        s += 'float ' + layer.name + '_threshold = ' + str(threashold) + '; \\n'\n",
    "    if layer_type(layer) == 'PReLU':\n",
    "        s += array2c(layer.get_weights()[0],layer.name + '_alpha')\n",
    "    file.write(s)\n",
    "    file.write('\\n\\n')\n",
    "\n",
    "def weights2c(layer,file, model_io):\n",
    "    if layer_type(layer) == 'Dense':\n",
    "        write_weights_Dense(layer,file, model_io)\n",
    "     \n",
    "    if layer_type(layer) == 'LSTM':\n",
    "        write_weights_LSTM(layer,file, model_io)\n",
    "        \n",
    "    if layer_type(layer) == 'GRU':\n",
    "        write_weights_GRU(layer,file, model_io)\n",
    "        \n",
    "    if layer_type(layer) == 'SimpleRNN':\n",
    "        write_weights_SimpleRNN(layer,file, model_io)\n",
    "\n",
    "    if layer_type(layer) == 'Conv1D':\n",
    "        write_weights_Conv1D(layer,file, model_io)\n",
    "    \n",
    "    if layer_type(layer) in ['Add','Subtract','Multiply','Maximum','Minimum','Average']:\n",
    "        write_weights_Merge(layer,file, model_io)\n",
    "\n",
    "    if layer_type(layer) in ['MaxPooling1D','AveragePooling1D']:\n",
    "        write_weights_Pooling1D(layer,file, model_io)\n",
    "\n",
    "    if layer_type(layer) in ['GlobalMaxPooling1D','GlobalAveragePooling']:\n",
    "        write_weights_GlobalPooling1D(layer,file, model_io)\n",
    "\n",
    "    if layer_type(layer) == 'Activation':\n",
    "        write_weights_Activation(layer,file, model_io)\n",
    "    \n",
    "    if layer_type(layer) in ['LeakyReLU','PReLU','ELU','ThresholdedReLU','ReLU']:\n",
    "        write_weights_AdvancedActivation(layer,file, model_io)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "code_folding": [
     0,
     2,
     13,
     51,
     60,
     80,
     92,
     102,
     109,
     129
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### layer2c\n",
    "\n",
    "def write_layer_LSTM(layer,file,inputs,outputs,i):\n",
    "    nm = layer.name\n",
    "    output_activation = 'keras2c_' + layer.get_config()['activation']\n",
    "    recurrent_activation = 'keras2c_' + layer.get_config()['recurrent_activation']\n",
    "\n",
    "    s = 'keras2c_lstm(' + inputs + ',' + nm + '_state,' + nm + '_kernel, \\n\\t' + \\\n",
    "           nm + '_recurrent_kernel,' + nm + '_bias,' + nm + '_units, \\n\\t' + \\\n",
    "           nm + '_in' + str(i) + '_height,' + nm + '_in' + str(i) + '_width,' + nm + '_fwork, \\n\\t' + \\\n",
    "           recurrent_activation + ',' + output_activation + ',' + outputs + '); \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "def write_layer_Dense(layer,file,inputs,outputs,i):\n",
    "    nm = layer.name\n",
    "    activation = 'keras2c_' + layer.get_config()['activation']\n",
    "\n",
    "    s = 'keras2c_dense(' + outputs + ',' + inputs + ',' + nm + '_kernel, \\n\\t' + \\\n",
    "           nm + '_bias,' + nm + '_out' + str(i) + 'rows,' + nm + '_out' + str(i) + 'cols, \\n\\t' + \\\n",
    "           nm + '_inner' + str(i) + 'dim,' + activation + '); \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "def write_layer_Conv1D(layer,file,inputs,outputs,i):\n",
    "    nm = layer.name\n",
    "    activation = 'keras2c_' + layer.get_config()['activation']\n",
    "\n",
    "    s = 'keras2c_pad1d(' + inputs + ',' + nm + '_padded' + str(i) + '_input,' + nm + \\\n",
    "        '_fill' + str(i) + ', \\n\\t' + nm + '_in' + str(i) + '_height,' + nm + '_in' + str(i) + \\\n",
    "        '_width,' + nm + '_pad' + str(i) + '_top,' + nm + '_pad' + str(i) + '_bottom); \\n'\n",
    "    file.write(s)\n",
    "    s = 'keras2c_conv1d(' + nm + '_padded' + str(i) + '_input,' + outputs + ',' + nm + '_kernel, \\n\\t' + \\\n",
    "        nm + '_bias,' + nm + '_out' + str(i) + '_height,' + nm + '_out' + str(i) + '_width, \\n\\t' + \\\n",
    "        nm + '_kernel_size,' + nm + '_padded_in' + str(i) + '_height,' + nm + '_in' + str(i) + '_width, \\n\\t' + \\\n",
    "        nm + '_stride,' + nm + '_dilation,' + activation + '); \\n'\n",
    "    file.write(s)\n",
    "\n",
    "def write_layer_Pooling1D(layer,file,inputs,outputs,i):\n",
    "    nm = layer.name\n",
    "    s = 'keras2c_pad1d(' + inputs + ',' + nm + '_padded' + str(i) + '_input,' + \\\n",
    "        nm + '_fill' + str(i) + ', \\n\\t' + nm + '_in' + str(i) + '_height,' + \\\n",
    "        nm + '_in' + str(i) + '_width,' + nm + '_pad' + str(i) + '_top, \\n\\t' + \\\n",
    "        nm + '_pad' + str(i) + '_bottom); \\n'\n",
    "    file.write(s)\n",
    "    if 'Max' in layer_type(layer):\n",
    "        s = 'keras2c_maxpool1d('\n",
    "    else:\n",
    "        s = 'keras2c_avgpool1d('\n",
    "    s += outputs + ',' + nm + '_padded' + str(i) + '_input,' + nm + '_pool_size, \\n\\t' + \\\n",
    "        nm + '_stride,' + nm + '_in' + str(i) + '_width,' + nm + '_out' + str(i) + '_height); \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "def write_layer_GlobalPooling1D(layer,file,inputs,outputs,i):\n",
    "    nm = layer.name\n",
    "    if 'Max' in layer_type(layer):\n",
    "        s = 'keras2c_global_max_pooling_1d('\n",
    "    else:\n",
    "        s = 'keras2c_global_avg_pooling_1d('\n",
    "    s += outputs + ',' + inputs + ',' + nm + '_in' + str(i) + '_height,' + nm + '_in' + str(i) + '_width); \\n'\n",
    "    file.write(s)\n",
    "\n",
    "def write_layer_Merge(layer,file,inputs,outputs,i):\n",
    "    nm = layer.name\n",
    "    \n",
    "    if 'Subtract' == layer_type(layer):\n",
    "        s = 'keras2c_subtract('\n",
    "    elif 'Add' == layer_type(layer):\n",
    "        s = 'keras2c_add('\n",
    "    elif 'Multiply' == layer_type(layer):\n",
    "        s = 'keras2c_multiply('\n",
    "    elif 'Average' == layer_type(layer):\n",
    "        s = 'keras2c_average('\n",
    "    elif 'Maximum' == layer_type(layer):\n",
    "        s = 'keras2c_max'\n",
    "    elif 'Minimum' == layer_type(layer):\n",
    "        s = 'keras2c_min('\n",
    "    s += outputs + ',' + nm + '_numels' + str(i) + ',' + nm + '_num_tensors' + str(i) + ','\n",
    "    c = ','.join(inputs)\n",
    "    s += c + '); \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "def write_layer_GRU(layer,file,inputs,outputs,i):\n",
    "    nm = layer.name\n",
    "    output_activation = 'keras2c_' + layer.get_config()['activation']\n",
    "    recurrent_activation = 'keras2c_' + layer.get_config()['recurrent_activation']\n",
    "\n",
    "    s = 'keras2c_gru(' + inputs + ',' + nm + '_state,' + nm + '_kernel, \\n\\t' + \\\n",
    "        nm + '_recurrent_kernel,' + nm + '_bias,' + nm + '_units, \\n\\t' + \\\n",
    "        nm + '_in' + str(i) + '_height,' + nm + '_in' + str(i) + '_width,' +\\\n",
    "        nm + '_fwork, \\n\\t' + recurrent_activation + ',' + output_activation +\\\n",
    "        ',' + nm + '_reset_after, \\n\\t' + outputs + '); \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "def write_layer_SimpleRNN(layer,file,inputs,outputs,i):\n",
    "    nm = layer.name\n",
    "    activation = 'keras2c_' + layer.get_config()['activation']\n",
    "\n",
    "    s = 'keras2c_simpleRNN(' + inputs + ',' + nm + '_state,' + nm + '_kernel, \\n\\t' + \\\n",
    "        nm + '_recurrent_kernel,' + nm + '_bias,' + nm + '_units, \\n\\t' + \\\n",
    "        nm + '_in' + str(i) + '_height,' + nm + '_in' + str(i) + '_width,' +\\\n",
    "        nm + '_fwork, \\n\\t' + activation + ',' + outputs + '); \\n'\n",
    "    file.write(s)    \n",
    "    \n",
    "def write_layer_Activation(layer,file,inputs,outputs,i):\n",
    "    activation = 'keras2c_' + layer.get_config()['activation']\n",
    "    nm = layer.name\n",
    "    s = activation + '(' + inputs + ',' + nm + '_size' + str(i) + '); \\n'\n",
    "    s += 'float *' + outputs + ' = &' + inputs + '[0]; // rename for clarity \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "def write_layer_AdvancedActivation(layer,file,inputs,outputs,i):\n",
    "    nm = layer.name\n",
    "    if layer_type(layer) == 'LeakyReLU':\n",
    "        s = 'keras2c_LeakyReLU(' + inputs + ',' + nm + '_size' + str(i) + ',' + nm + '_alpha); \\n'\n",
    "    if layer_type(layer) == 'PReLU':\n",
    "        s += 'keras2c_PReLU(' + inputs + ',' + nm + '_size' + str(i) + ',' + nm + '_alpha); \\n'\n",
    "    if layer_type(layer) == 'ELU':\n",
    "        s = 'keras2c_ELU(' + inputs + ',' + nm + '+size' + str(i) + ',' + nm + '_alpha); \\n'\n",
    "    if layer_type(layer) == 'ThresholdedReLU':\n",
    "        s = 'keras2c_ThresholdedReLU(' + inputs + ',' + nm + '_size' + str(i) + ',' + nm + '_theta); \\n'\n",
    "    if layer_type(layer) == 'ReLU':\n",
    "        s = 'keras2c_ReLU(' + inputs + ',' + nm + '_size' + str(i) + ',' + nm + '_max_value, \\n\\t' + \\\n",
    "            nm + '_negative_slope,' + nm + '_threshold); \\n'\n",
    "  \n",
    "    s += 'float *' + outputs + ' = &' + inputs + '[0]; // rename for clarity \\n'\n",
    "    file.write(s)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "def layer2c(layer,file,inputs,outputs,i):\n",
    "\n",
    "    if layer_type(layer) == 'Dense':\n",
    "        write_layer_Dense(layer,file,inputs,outputs,i)\n",
    "\n",
    "    elif layer_type(layer) == 'LSTM':\n",
    "        write_layer_LSTM(layer,file,inputs,outputs,i)\n",
    "        \n",
    "    elif layer_type(layer) == 'GRU':\n",
    "        write_layer_GRU(layer,file,inputs,outputs,i)\n",
    "        \n",
    "    elif layer_type(layer) == 'SimpleRNN':\n",
    "        write_layer_SimpleRNN(layer,file,inputs,outputs,i)\n",
    "\n",
    "    elif layer_type(layer) == 'Conv1D':\n",
    "        write_layer_Conv1D(layer,file,inputs,outputs,i)\n",
    "    \n",
    "    elif layer_type(layer) in ['MaxPooling1D', 'AveragePooling1D']:\n",
    "        write_layer_Pooling1D(layer,file,inputs,outputs,i)\n",
    "    \n",
    "    elif layer_type(layer) in ['GlobalMaxPooling1D', 'GlobalAveragePooling1D']:\n",
    "        write_layer_GlobalPooling1D(layer,file,inputs,outputs,i)\n",
    "    \n",
    "    elif layer_type(layer) in ['Add','Subtract','Multiply','Average','Maximum','Minimum']:\n",
    "        write_layer_Merge(layer,file,inputs,outputs,i)\n",
    "    \n",
    "    elif layer_type(layer) == 'Activation':\n",
    "        write_layer_Activation(layer,file,inputs,outputs,i)\n",
    "    \n",
    "    elif layer_type(layer) in ['LeakyReLU','PReLU','ELU','ThresholdedReLU','ReLU']:\n",
    "        write_layer_AdvancedActivation(layer,file,inputs,outputs,i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "code_folding": [
     0,
     2,
     5,
     9,
     29,
     62
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### types, names, io\n",
    "\n",
    "def layer_type(layer):\n",
    "    return str(layer.__class__).split('.')[-1][0:-2]\n",
    "\n",
    "def get_all_io_names(model):\n",
    "    a = [get_layer_io_names(layer) for layer in model.layers]\n",
    "    return list(set(flatten(a)))\n",
    "\n",
    "def get_layer_num_io(layer):\n",
    "    num_inputs = 0\n",
    "    error = False\n",
    "    while not error:\n",
    "        try:\n",
    "            layer.get_input_at(num_inputs)\n",
    "            num_inputs +=1\n",
    "        except:\n",
    "            error = True\n",
    "    \n",
    "    num_outputs = 0\n",
    "    error = False\n",
    "    while not error:\n",
    "        try:\n",
    "            layer.get_output_at(num_outputs)\n",
    "            num_outputs +=1\n",
    "        except:\n",
    "            error = True\n",
    "    return num_inputs, num_outputs\n",
    "\n",
    "def get_layer_io_names(layer):\n",
    "    num_inputs, num_outputs = get_layer_num_io(layer)\n",
    "    inputs = []\n",
    "    # num_inputs>1 -> shared layer\n",
    "    for i in range(num_inputs):\n",
    "        # is the input a list?\n",
    "        if isinstance(layer.get_input_at(i), list):\n",
    "            temp_list = []\n",
    "            list_length = len(layer.get_input_at(i))\n",
    "            for j in range(list_length):\n",
    "                name = str(layer.get_input_at(i)[j]).split()[0].split('\"')[1].split('/')[0].split(':')[0]\n",
    "                temp_list.append(name)\n",
    "            inputs.insert(i,temp_list)\n",
    "        else:\n",
    "            name = str(layer.get_input_at(i)).split()[0].split('\"')[1].split('/')[0].split(':')[0]\n",
    "            inputs.insert(i,name)\n",
    "    \n",
    "    outputs = []       \n",
    "    for i in range(num_outputs):\n",
    "        # is the output a list?\n",
    "        if isinstance(layer.get_output_at(i), list):\n",
    "            temp_list = []\n",
    "            list_length = len(layer.get_output_at(i))\n",
    "            for j in range(list_length):\n",
    "                name = str(layer.get_output_at(i)[j]).split()[0].split('\"')[1].split('/')[0].split(':')[0]\n",
    "                temp_list.append(name)\n",
    "            outputs.insert(i,temp_list)\n",
    "        else:\n",
    "            name = str(layer.get_output_at(i)).split()[0].split('\"')[1].split('/')[0].split(':')[0]\n",
    "            outputs.insert(i,name)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "def get_model_io_names(model):\n",
    "    num_inputs = len(model.inputs)\n",
    "    num_outputs = len(model.outputs)\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for i in range(num_inputs):\n",
    "        nm = str(model.inputs[i]).split()[0].split('\"')[1].split('/')[0].split(':')[0]\n",
    "        inputs.append(nm)\n",
    "    for i in range(num_outputs):\n",
    "        nm = str(model.outputs[i]).split()[0].split('\"')[1].split('/')[0].split(':')[0]\n",
    "        outputs.append(nm)\n",
    "    return inputs, outputs\n",
    "\n",
    "def flatten(x):\n",
    "        if isinstance(x, list) or isinstance(x, tuple):\n",
    "            return [a for i in x for a in flatten(i)]\n",
    "        else:\n",
    "            return [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "code_folding": [
     0,
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### model2c\n",
    "def model2c(model,file,function_name):\n",
    "    num_inputs = len(model.inputs)\n",
    "    num_outputs = len(model.outputs)\n",
    "    model_inputs, model_outputs = get_model_io_names(model)\n",
    "    \n",
    "    s = '#include <stdio.h> \\n#include <stddef.h> \\n#include <math.h> \\n#include <string.h> \\n'\n",
    "    s += '#include <stdarg.h> \\n#include \"keras2c_include.h\" \\n'\n",
    "    s += '\\n \\n'\n",
    "    s += 'void ' + function_name + '('\n",
    "    s_in = ['float ' + in_nm + '_input[]' for in_nm in model_inputs]\n",
    "    s += ', '.join(s_in) + ', '\n",
    "    s_out = ['float ' + out_nm + '_output[]' for out_nm in model_outputs]\n",
    "    s += ', '.join(s_out) + ') { \\n \\n'\n",
    "    file.write(s)\n",
    "    \n",
    "\n",
    "    print('Writing Weights')\n",
    "    for layer in model.layers:\n",
    "        weights2c(layer,file,[model_inputs,model_outputs])\n",
    "    written_io = set(model_inputs)\n",
    "    unwritten_io = set(get_all_io_names(model)) - written_io\n",
    "    \n",
    "    \n",
    "    while len(unwritten_io)>0:\n",
    "        for layer in model.layers:\n",
    "            layer_inputs, layer_outputs = get_layer_io_names(layer)\n",
    "            for i,(inp,outp) in enumerate(zip(layer_inputs,layer_outputs)):\n",
    "                if (set(flatten(inp)).issubset(written_io) and set(flatten(outp)).issubset(unwritten_io)) \\\n",
    "                    or layer_type(layer) == 'InputLayer':\n",
    "                    print('Writing layer ', outp)\n",
    "                    if set(flatten(inp)).issubset(set(model_inputs)):\n",
    "                        if isinstance(inp,list):\n",
    "                            inp_nm = [nm + '_input' for nm in inp]\n",
    "                        else:\n",
    "                            inp_nm = inp + '_input'\n",
    "                    else:\n",
    "                        if isinstance(inp,list):\n",
    "                            inp_nm = [nm + '_output' for nm in inp]\n",
    "                        else:\n",
    "                            inp_nm = inp + '_output'                    \n",
    "                    layer2c(layer,file,inp_nm,outp + '_output',i)\n",
    "                    written_io |= set(flatten(inp)) \n",
    "                    written_io |= set(flatten(outp))\n",
    "                    unwritten_io -= set(flatten(inp))\n",
    "                    unwritten_io -= set(flatten(outp))\n",
    "    file.write('\\n }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "code_folding": [
     0,
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### keras2c\n",
    "def keras2c(model_filepath,function_name,num_tests=10):\n",
    "\n",
    "    function_name = str(function_name)\n",
    "    filename = function_name + '.c'\n",
    "    model = keras.models.load_model(str(model_filepath))\n",
    "    \n",
    "    # check that the model can be converted\n",
    "    check_model(model, function_name)\n",
    "    print('All checks passed')\n",
    "    \n",
    "    file = open(filename,\"w+\")\n",
    "    model2c(model,file,function_name)\n",
    "    file.close()\n",
    "    make_test_suite(model,function_name,num_tests)\n",
    "    print(\"Done \\n C code is in '\" + function_name + \\\n",
    "          \".c' and tests are in '\" + function_name + \"_test_suite.c'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "code_folding": [
     0,
     11,
     20,
     42,
     60
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### checks\n",
    "\n",
    "def is_valid_c_name(name):\n",
    "    allowed_chars = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_1234567890'\n",
    "    allowed_starting_chars = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_'\n",
    "    if not set(name).issubset(allowed_chars):\n",
    "        return False\n",
    "    if not set(name[0]).issubset(allowed_starting_chars):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def name_check(model):\n",
    "    valid = True\n",
    "    log = ''\n",
    "    for layer in model.layers:\n",
    "        if not is_valid_c_name(layer.name):\n",
    "            valid = False\n",
    "            log += \"layer name '\" + layer.name + \"' is not a valid C name \\n\"\n",
    "    return valid, log\n",
    "\n",
    "def layers_supported_check(model):\n",
    "    core_layers = ['Dense','Activation','InputLayer']\n",
    "    conv_layers = ['Conv1D']\n",
    "    pool_layers = ['MaxPooling1D','AveragePooling1D','GlobalMaxPooling1D','GlobalAveragePooling1D']\n",
    "    local_layers = []\n",
    "    recur_layers = ['LSTM','GRU','SimpleRNN']\n",
    "    embed_layers = []\n",
    "    merge_layers = ['Add','Subtract','Multiply','Average','Maximum','Minimum']\n",
    "    activ_layers = ['LeakyReLU','PReLU','ELU','ThresholdedReLU','ReLU']\n",
    "    norm_layers = []\n",
    "    noise_layers = []\n",
    "    \n",
    "    supported_layers = core_layers + conv_layers + pool_layers + local_layers + \\\n",
    "        recur_layers + embed_layers + merge_layers + activ_layers + norm_layers + noise_layers\n",
    "    valid = True\n",
    "    log = ''\n",
    "    for layer in model.layers:\n",
    "        if not (layer_type(layer) in supported_layers):\n",
    "            valid = False\n",
    "            log += \"layer type '\" + layer_type(layer) + \"' is not supported at this time \\n\"\n",
    "    return valid, log\n",
    "        \n",
    "def activation_supported_check(model):\n",
    "    supported_activations = ['linear', 'relu','softmax','softplus','softsign','relu','tanh',\\\n",
    "                             'sigmoid','hard_sigmoid','exponential' ]\n",
    "    valid = True\n",
    "    log = ''\n",
    "    for layer in model.layers:\n",
    "        if 'activation' in layer.get_config():\n",
    "            if not (layer.get_config()['activation'] in supported_activations):\n",
    "                valid = False\n",
    "                log += \"activation type '\" + layer.get_config()['activation'] + \\\n",
    "                    \"' for layer '\" + layer.name + \"' is not supported at this time \\n\"\n",
    "        if 'recurrent_activation' in layer.get_config():\n",
    "            if not (layer.get_config()['recurrent_activation'] in supported_activations):\n",
    "                valid = False\n",
    "                log += \"recurrent activation type '\" + layer.get_config()['recurrent_activation'] + \\\n",
    "                    \"' for layer '\" + layer.name + \"' is not supported at this time \\n\"\n",
    "    return valid, log\n",
    "\n",
    "def config_supported_check(model):\n",
    "    valid = True\n",
    "    log = ''\n",
    "    for layer in model.layers:\n",
    "        if 'data_format' in layer.get_config():\n",
    "            if layer.get_config()['data_format'] != 'channels_last':\n",
    "                valid = False\n",
    "                log += \"data format '\" + layer.get_config()['data_format'] + \"' for layer '\" + \\\n",
    "                    layer.name + \"' is not supported at this time \\n\"\n",
    "        if 'return_sequences' in layer.get_config():\n",
    "            if layer.get_config()['return_sequences']:\n",
    "                valid = False\n",
    "                log += \"'return_sequences' option for layer '\" + layer.name + \\\n",
    "                    \"' is not supported at this time \\n\"\n",
    "        if 'return_state' in layer.get_config():\n",
    "            if layer.get_config()['return_state']:\n",
    "                valid = False\n",
    "                log += \"'return_state' option for layer '\" + layer.name + \\\n",
    "                    \"' is not supported at this time \\n\"\n",
    "        if 'go_backwards' in layer.get_config():\n",
    "            if layer.get_config()['go_backwards']:\n",
    "                valid = False\n",
    "                log += \"'go_backwards' option for layer '\" + layer.name + \\\n",
    "                    \"' is not supported at this time \\n\"\n",
    "        if 'stateful' in layer.get_config():\n",
    "            if layer.get_config()['stateful']:\n",
    "                valid = False\n",
    "                log += \"'stateful' option for layer '\" + layer.name + \\\n",
    "                    \"' is not supported at this time \\n\"\n",
    "        if 'shared_axes' in layer.get_config():\n",
    "            if layer.get_config()['shared_axes'] is not None:\n",
    "                valid = False\n",
    "                log += \"shared axes option for layer '\" + layer.name + \\\n",
    "                    \"' is not supported at this time\"\n",
    "    return valid, log\n",
    "\n",
    "def check_model(model, function_name):\n",
    "    valid_fname = True\n",
    "    log = 'The following errors were found: \\n'\n",
    "    if not is_valid_c_name(function_name):\n",
    "        valid_fname = False\n",
    "        log += \"function name '\" + function_name + \"' is not a valid C name \\n\"\n",
    "    valid_lname, name_log = name_check(model)\n",
    "    log += name_log\n",
    "    valid_layer, layer_log = layers_supported_check(model)\n",
    "    log += layer_log\n",
    "    valid_activation, activation_log = activation_supported_check(model)\n",
    "    log += activation_log\n",
    "    valid_config, config_log = config_supported_check(model)\n",
    "    log += config_log\n",
    "    assert(valid_fname and valid_lname and valid_layer and valid_activation and valid_config), log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "code_folding": [
     0,
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### make test suite\n",
    "def make_test_suite(model,function_name,num_tests=10):\n",
    "    print('Writing tests')\n",
    "    input_shape = []\n",
    "    output_shape = []\n",
    "    model_inputs, model_outputs = get_model_io_names(model)\n",
    "    num_inputs = len(model_inputs)\n",
    "    num_outputs = len(model_outputs)\n",
    "    for i in range(num_inputs):\n",
    "        input_shape.insert(i,model.inputs[i].shape[1:])\n",
    "    for i in range(num_outputs):\n",
    "        output_shape.insert(i, model.outputs[i].shape[1:])\n",
    "    \n",
    "    \n",
    "    file = open(function_name + '_test_suite.c',\"w+\")\n",
    "    s = '#include <stdio.h> \\n#include <math.h> \\n#include <sys/time.h> \\n \\n'\n",
    "    s += 'float norm2(float array1[], float array2[], size_t numel);\\n'\n",
    "    s += 'struct timeval GetTimeStamp(); \\n \\n'\n",
    "    file.write(s)\n",
    "    fsig = 'void ' + function_name + '('\n",
    "    s_in = ['float ' + in_nm + '_input[]' for in_nm in model_inputs]\n",
    "    fsig += ', '.join(s_in) + ', '\n",
    "    s_out = ['float ' + out_nm + '_output[]' for out_nm in model_outputs]\n",
    "    fsig += ', '.join(s_out) + '); \\n \\n'\n",
    "    file.write(fsig)\n",
    "\n",
    "    s = 'int main(){\\n'\n",
    "    file.write(s)\n",
    "    for i in range(num_tests):\n",
    "        #generate random input and write to file\n",
    "        rand_inputs = []\n",
    "        for j,inpt in enumerate(model_inputs):\n",
    "            rand_input = np.random.random(input_shape[j])\n",
    "            file.write(array2c(rand_input,'test' + str(i+1) + '_' + model_inputs[j] + '_input'))\n",
    "            rand_input = rand_input[np.newaxis,...]\n",
    "            rand_inputs.insert(j,rand_input)\n",
    "        # make predictions\n",
    "        outputs = model.predict(rand_inputs)\n",
    "        # write predictions\n",
    "        if not isinstance(outputs,list):\n",
    "            outputs = [outputs]\n",
    "        for j,outpt in enumerate(model_outputs):\n",
    "            output = outputs[j][0,:]\n",
    "            file.write(array2c(output,'keras_' + model_outputs[j] + '_test' + str(i+1)))\n",
    "            s = 'float c_' + model_outputs[j] + '_test' + str(i+1) + '[' + \\\n",
    "                str(np.prod(output_shape[j])) + '] = {0};\\n'\n",
    "            file.write(s)\n",
    "    s = ' float errors[' + str(num_tests*num_outputs) + '];\\n'\n",
    "    s += ' int num_tests = ' + str(num_tests) + '; \\n'\n",
    "    s += 'int num_outputs = ' + str(num_outputs) + '; \\n'\n",
    "    for i, outpt in enumerate(model_outputs):\n",
    "        s += ' size_t numel_' + outpt + ' = ' + str(np.prod(output_shape[i])) + ';\\n'\n",
    "    s += ' struct timeval t1 = GetTimeStamp(); \\n'\n",
    "    file.write(s)\n",
    "    for i in range(num_tests):\n",
    "        s = function_name + '('\n",
    "        for j, inpt in enumerate(model_inputs): \n",
    "            s +=  'test' + str(i+1) + '_' + model_inputs[j] + '_input,'\n",
    "        s += '\\n\\t'\n",
    "        for j, outpt in enumerate(model_outputs):\n",
    "            s += 'c_' + model_outputs[j] + '_test' + str(i+1) + ','\n",
    "        s = s[:-1] + '); \\n'\n",
    "        file.write(s)\n",
    "    file.write('\\n')\n",
    "    s =  'struct timeval t2 = GetTimeStamp(); \\n'\n",
    "    s += 'typedef unsigned long long u64; \\n'\n",
    "    s += 'u64 t1u = t1.tv_sec*1e6 + t1.tv_usec; \\n'\n",
    "    s += 'u64 t2u = t2.tv_sec*1e6 + t2.tv_usec; \\n'\n",
    "    s += 'printf(\"average time: %llu us \\\\n\", (t2u-t1u)/' + str(num_tests) + '); \\n'\n",
    "    file.write(s)\n",
    "    for i in range(num_tests):\n",
    "        for j, outpt in enumerate(model_outputs):\n",
    "            s = 'errors[' + str(i*num_outputs+j) + '] = norm2(keras_' + model_outputs[j] + '_test' + \\\n",
    "                str(i+1) + ',c_' + model_outputs[j] + '_test' + str(i+1) + ',numel_' + outpt + '); \\n'\n",
    "            file.write(s)\n",
    "    s = 'printf(\"L2 norm of output errors for tests:\\\\n\");\\n'\n",
    "    file.write(s)\n",
    "    s = 'for(size_t i=0; i< num_tests;i++){ \\n'\n",
    "    s += 'for(size_t j=0; j<num_outputs;j++){ \\n'\n",
    "    s += ' printf(\"test %lu, output %lu : %e \\\\t\",i,j, errors[i*num_outputs+j]);}\\n'\n",
    "    s += 'printf(\"\\\\n\");}\\n'\n",
    "    file.write(s)\n",
    "    file.write('return 0;\\n} \\n\\n')\n",
    "    s = \"\"\"float norm2(float array1[], float array2[], size_t numel){ \\n\n",
    "    float sum = 0; \\n\n",
    "    for(size_t i=0; i<numel; i++){\\n\n",
    "    sum += (array1[i]-array2[i])*(array1[i]-array2[i]);}\\n\n",
    "    return sqrt(sum);}\\n\\n\"\"\"\n",
    "    file.write(s)\n",
    "    s = \"\"\"struct timeval GetTimeStamp() {\n",
    "    struct timeval tv;\n",
    "    gettimeofday(&tv,NULL);\n",
    "    return tv;}\"\"\"\n",
    "    file.write(s)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All checks passed\n",
      "Writing Weights\n",
      "Writing layer  input_80_1\n",
      "Writing layer  max_pooling1d_21_1\n",
      "Writing tests\n",
      "Done \n",
      " C code is in 'test1.c' and tests are in 'test1_test_suite.c'\n"
     ]
    }
   ],
   "source": [
    "inshape = (8,23,)\n",
    "a = keras.layers.Input(inshape)\n",
    "j = keras.layers.MaxPooling1D(pool_size=2,padding='same', strides=1)(a)\n",
    "model = keras.models.Model(inputs=[a], outputs=[j])\n",
    "model.save('test1.h5')\n",
    "keras2c('test1.h5','test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### test model\n",
    "inshape = (8,23,)\n",
    "inshape2 = (1,23)\n",
    "stride=1\n",
    "dilation=1\n",
    "filter_height=3\n",
    "num_filters=5\n",
    "pad = 'valid'\n",
    "a = keras.layers.Input(inshape)\n",
    "b = keras.layers.Dense(48)(a)\n",
    "m = keras.layers.Dropout(.4)(b)\n",
    "c = keras.layers.Dense(30)(m)\n",
    "d = keras.layers.Conv1D(filters=num_filters, kernel_size=filter_height, strides=stride, padding=pad, \\\n",
    "                        dilation_rate=dilation)(c)\n",
    "e = keras.layers.Dense(20)(d)\n",
    "f = keras.layers.GRU(20, activation='relu')(e)\n",
    "g = keras.layers.Dense(20)(f)\n",
    "h = keras.layers.Input(inshape2)\n",
    "i = keras.layers.Dense(20)(h)\n",
    "j = keras.layers.MaxPooling1D(pool_size=2,padding='same', strides=1)(i)\n",
    "k = keras.layers.Add()([j,g])\n",
    "l = keras.layers.Dense(30)(k)\n",
    "                       \n",
    "model = keras.models.Model(inputs=[a,h], outputs=[g,l,j])\n",
    "# model.save('test1.h5')\n",
    "# keras2c('test1.h5','test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "layers: dropout, input, activity regularization, spatial dropout, stacked RNN, embedding\n",
    "         concatentate, dot, merge with different sizes, \n",
    "         flatten? permute? do these actually do anything besides change shape?\n",
    " \n",
    " conv2d, pool2d, conv3d, pool3d\n",
    " seperable conv\n",
    " conv transpose\n",
    " depthwise conv\n",
    " spatial dropout 1d,2d,3d\n",
    " dense layer in 3d,4d\n",
    " test all the layers\n",
    " static or const?\n",
    " broadcasting sizes for merge layers\n",
    " pass in array of pointers to arrays and array of sizes?\n",
    " find the common dimension and add them that way?\n",
    " dummy layers - just add pointers:\n",
    "     dropout, regularization, flatten, reshape\n",
    " repeat vector(output, input, width, height) - also use for broadcasting\n",
    " crop1d, crop2d, crop3d\n",
    " pad2d, pad3d\n",
    " upsampling1d,2d,3d\n",
    " \n",
    " array struct with data, ndims, shape\n",
    " array2c writes as struct\n",
    " also have it write blank tensors?\n",
    " padding - change fill to fill value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ktf2cpp] *",
   "language": "python",
   "name": "conda-env-ktf2cpp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
